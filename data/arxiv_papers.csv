title,abstract,url
GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning,"Visual generation models have made remarkable progress in creating realistic
images from text prompts, yet struggle with complex prompts that specify
multiple objects with precise spatial relationships and attributes. Effective
handling of such prompts requires explicit reasoning about the semantic content
and spatial layout. We present GoT-R1, a framework that applies reinforcement
learning to enhance semantic-spatial reasoning in visual generation. Building
upon the Generation Chain-of-Thought approach, GoT-R1 enables models to
autonomously discover effective reasoning strategies beyond predefined
templates through carefully designed reinforcement learning. To achieve this,
we propose a dual-stage multi-dimensional reward framework that leverages MLLMs
to evaluate both the reasoning process and final output, enabling effective
supervision across the entire generation pipeline. The reward system assesses
semantic alignment, spatial accuracy, and visual quality in a unified approach.
Experimental results demonstrate significant improvements on T2I-CompBench
benchmark, particularly in compositional tasks involving precise spatial
relationships and attribute binding. GoT-R1 advances the state-of-the-art in
image generation by successfully transferring sophisticated reasoning
capabilities to the visual generation domain. To facilitate future research, we
make our code and pretrained models publicly available at
https://github.com/gogoduan/GoT-R1.",http://arxiv.org/abs/2505.17022v1
Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO,"Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT",http://arxiv.org/abs/2505.17017v1
Interactive Post-Training for Vision-Language-Action Models,"We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based
interactive post-training paradigm that fine-tunes pretrained
Vision-Language-Action (VLA) models using only sparse binary success rewards.
Existing VLA training pipelines rely heavily on offline expert demonstration
data and supervised imitation, limiting their ability to adapt to new tasks and
environments under low-data regimes. RIPT-VLA addresses this by enabling
interactive post-training with a stable policy optimization algorithm based on
dynamic rollout sampling and leave-one-out advantage estimation.
  RIPT-VLA has the following characteristics. First, it applies to various VLA
models, resulting in an improvement on the lightweight QueST model by 21.2%,
and the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it
is computationally efficient and data-efficient: with only one demonstration,
RIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success
rate within 15 iterations. Furthermore, we demonstrate that the policy learned
by RIPT-VLA generalizes across different tasks and scenarios and is robust to
the initial state context. These results highlight RIPT-VLA as a practical and
effective paradigm for post-training VLA models through minimal supervision.",http://arxiv.org/abs/2505.17016v1
When Are Concepts Erased From Diffusion Models?,"Concept erasure, the ability to selectively prevent a model from generating
specific concepts, has attracted growing interest, with various approaches
emerging to address the challenge. However, it remains unclear how thoroughly
these methods erase the target concept. We begin by proposing two conceptual
models for the erasure mechanism in diffusion models: (i) reducing the
likelihood of generating the target concept, and (ii) interfering with the
model's internal guidance mechanisms. To thoroughly assess whether a concept
has been truly erased from the model, we introduce a suite of independent
evaluations. Our evaluation framework includes adversarial attacks, novel
probing techniques, and analysis of the model's alternative generations in
place of the erased concept. Our results shed light on the tension between
minimizing side effects and maintaining robustness to adversarial prompts.
Broadly, our work underlines the importance of comprehensive evaluation for
erasure in diffusion models.",http://arxiv.org/abs/2505.17013v1
Understanding Prompt Tuning and In-Context Learning via Meta-Learning,"Prompting is one of the main ways to adapt a pretrained model to target
tasks. Besides manually constructing prompts, many prompt optimization methods
have been proposed in the literature. Method development is mainly empirically
driven, with less emphasis on a conceptual understanding of prompting. In this
paper we discuss how optimal prompting can be understood through a Bayesian
view, which also implies some fundamental limitations of prompting that can
only be overcome by tuning weights. The paper explains in detail how
meta-trained neural networks behave as Bayesian predictors over the pretraining
distribution, whose hallmark feature is rapid in-context adaptation. Optimal
prompting can be studied formally as conditioning these Bayesian predictors,
yielding criteria for target tasks where optimal prompting is and is not
possible. We support the theory with educational experiments on LSTMs and
Transformers, where we compare different versions of prefix-tuning and
different weight-tuning methods. We also confirm that soft prefixes, which are
sequences of real-valued vectors outside the token alphabet, can lead to very
effective prompts for trained and even untrained networks by manipulating
activations in ways that are not achievable by hard tokens. This adds an
important mechanistic aspect beyond the conceptual Bayesian theory.",http://arxiv.org/abs/2505.17010v1
Guided Diffusion Sampling on Function Spaces with Applications to PDEs,"We propose a general framework for conditional sampling in PDE-based inverse
problems, targeting the recovery of whole solutions from extremely sparse or
noisy measurements. This is accomplished by a function-space diffusion model
and plug-and-play guidance for conditioning. Our method first trains an
unconditional discretization-agnostic denoising model using neural operator
architectures. At inference, we refine the samples to satisfy sparse
observation data via a gradient-based guidance mechanism. Through rigorous
mathematical analysis, we extend Tweedie's formula to infinite-dimensional
Hilbert spaces, providing the theoretical foundation for our posterior sampling
approach. Our method (FunDPS) accurately captures posterior distributions in
function spaces under minimal supervision and severe data scarcity. Across five
PDE tasks with only 3% observation, our method achieves an average 32% accuracy
improvement over state-of-the-art fixed-resolution diffusion baselines while
reducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning
ensures strong cross-resolution generalizability. To the best of our knowledge,
this is the first diffusion-based framework to operate independently of
discretization, offering a practical and flexible solution for forward and
inverse problems in the context of PDEs. Code is available at
https://github.com/neuraloperator/FunDPS",http://arxiv.org/abs/2505.17004v1
Sufficient conditions for offline reactivation in recurrent neural networks,"During periods of quiescence, such as sleep, neural activity in many brain
circuits resembles that observed during periods of task engagement. However,
the precise conditions under which task-optimized networks can autonomously
reactivate the same network states responsible for online behavior is poorly
understood. In this study, we develop a mathematical framework that outlines
sufficient conditions for the emergence of neural reactivation in circuits that
encode features of smoothly varying stimuli. We demonstrate mathematically that
noisy recurrent networks optimized to track environmental state variables using
change-based sensory information naturally develop denoising dynamics, which,
in the absence of input, cause the network to revisit state configurations
observed during periods of online activity. We validate our findings using
numerical experiments on two canonical neuroscience tasks: spatial position
estimation based on self-motion cues, and head direction estimation based on
angular velocity cues. Overall, our work provides theoretical support for
modeling offline reactivation as an emergent consequence of task optimization
in noisy neural circuits.",http://arxiv.org/abs/2505.17003v1
Critical Points of Random Neural Networks,"This work investigates the expected number of critical points of random
neural networks with different activation functions as the depth increases in
the infinite-width limit. Under suitable regularity conditions, we derive
precise asymptotic formulas for the expected number of critical points of fixed
index and those exceeding a given threshold. Our analysis reveals three
distinct regimes depending on the value of the first derivative of the
covariance evaluated at 1: the expected number of critical points may converge,
grow polynomially, or grow exponentially with depth. The theoretical
predictions are supported by numerical experiments. Moreover, we provide
numerical evidence suggesting that, when the regularity condition is not
satisfied (e.g. for neural networks with ReLU as activation function), the
number of critical points increases as the map resolution increases, indicating
a potential divergence in the number of critical points.",http://arxiv.org/abs/2505.17000v1
A Unified Framework for Simultaneous Parameter and Function Discovery in Differential Equations,"Inverse problems involving differential equations often require identifying
unknown parameters or functions from data. Existing approaches, such as
Physics-Informed Neural Networks (PINNs), Universal Differential Equations
(UDEs) and Universal Physics-Informed Neural Networks (UPINNs), are effective
at isolating either parameters or functions but can face challenges when
applied simultaneously due to solution non-uniqueness. In this work, we
introduce a framework that addresses these limitations by establishing
conditions under which unique solutions can be guaranteed. To illustrate, we
apply it to examples from biological systems and ecological dynamics,
demonstrating accurate and interpretable results. Our approach significantly
enhances the potential of machine learning techniques in modeling complex
systems in science and engineering.",http://arxiv.org/abs/2505.16996v1
Native Segmentation Vision Transformers,"Uniform downsampling remains the de facto standard for reducing spatial
resolution in vision backbones. In this work, we propose an alternative design
built around a content-aware spatial grouping layer, that dynamically assigns
tokens to a reduced set based on image boundaries and their semantic content.
Stacking our grouping layer across consecutive backbone stages results in
hierarchical segmentation that arises natively in the feature extraction
process, resulting in our coined Native Segmentation Vision Transformer. We
show that a careful design of our architecture enables the emergence of strong
segmentation masks solely from grouping layers, that is, without additional
segmentation-specific heads. This sets the foundation for a new paradigm of
native, backbone-level segmentation, which enables strong zero-shot results
without mask supervision, as well as a minimal and efficient standalone model
design for downstream segmentation tasks. Our project page is
https://research.nvidia.com/labs/dvl/projects/native-segmentation.",http://arxiv.org/abs/2505.16993v1
"PICT -- A Differentiable, GPU-Accelerated Multi-Block PISO Solver for Simulation-Coupled Learning Tasks in Fluid Dynamics","Despite decades of advancements, the simulation of fluids remains one of the
most challenging areas of in scientific computing. Supported by the necessity
of gradient information in deep learning, differentiable simulators have
emerged as an effective tool for optimization and learning in physics
simulations. In this work, we present our fluid simulator PICT, a
differentiable pressure-implicit solver coded in PyTorch with
Graphics-processing-unit (GPU) support. We first verify the accuracy of both
the forward simulation and our derived gradients in various established
benchmarks like lid-driven cavities and turbulent channel flows before we show
that the gradients provided by our solver can be used to learn complicated
turbulence models in 2D and 3D. We apply both supervised and unsupervised
training regimes using physical priors to match flow statistics. In particular,
we learn a stable sub-grid scale (SGS) model for a 3D turbulent channel flow
purely based on reference statistics. The low-resolution corrector trained with
our solver runs substantially faster than the highly resolved references, while
keeping or even surpassing their accuracy. Finally, we give additional insights
into the physical interpretation of different solver gradients, and motivate a
physically informed regularization technique. To ensure that the full potential
of PICT can be leveraged, it is published as open source:
https://github.com/tum-pbs/PICT.",http://arxiv.org/abs/2505.16992v1
Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation,"Out-of-distribution (OOD) detection and segmentation are crucial for
deploying machine learning models in safety-critical applications such as
autonomous driving and robot-assisted surgery. While prior research has
primarily focused on unimodal image data, real-world applications are
inherently multimodal, requiring the integration of multiple modalities for
improved OOD detection. A key challenge is the lack of supervision signals from
unknown data, leading to overconfident predictions on OOD samples. To address
this challenge, we propose Feature Mixing, an extremely simple and fast method
for multimodal outlier synthesis with theoretical support, which can be further
optimized to help the model better distinguish between in-distribution (ID) and
OOD data. Feature Mixing is modality-agnostic and applicable to various
modality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal
dataset for OOD segmentation, featuring synthetic OOD objects across diverse
scenes and weather conditions. Extensive experiments on SemanticKITTI,
nuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that
Feature Mixing achieves state-of-the-art performance with a $10 \times$ to $370
\times$ speedup. Our source code and dataset will be available at
https://github.com/mona4399/FeatureMixing.",http://arxiv.org/abs/2505.16985v1
UFT: Unifying Supervised and Reinforcement Fine-Tuning,"Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.",http://arxiv.org/abs/2505.16984v1
"CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark","We introduce \texttt{CASS}, the first large-scale dataset and model suite for
cross-architecture GPU code transpilation, targeting both source-level
(CUDA~$\leftrightarrow$~HIP) and assembly-level (Nvidia
SASS~$\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k
verified code pairs across host and device, addressing a critical gap in
low-level GPU code portability. Leveraging this resource, we train the
\texttt{CASS} family of domain-specific language models, achieving 95\% source
translation accuracy and 37.5\% assembly translation accuracy, substantially
outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our
generated code matches native performance in over 85\% of test cases,
preserving runtime and memory behavior. To support rigorous evaluation, we
introduce \texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with
ground-truth execution. All data, models, and evaluation tools are released as
open source to foster progress in GPU compiler tooling, binary compatibility,
and LLM-guided hardware translation. Dataset and benchmark are on
\href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}},
with code at
\href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.",http://arxiv.org/abs/2505.16968v1
BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation,"Text segmentation based on the semantic meaning of sentences is a fundamental
task with broad utility in many downstream applications. In this paper, we
propose a graphical model-based unsupervised learning approach, named BP-Seg
for efficient text segmentation. Our method not only considers local coherence,
capturing the intuition that adjacent sentences are often more related, but
also effectively groups sentences that are distant in the text yet semantically
similar. This is achieved through belief propagation on the carefully
constructed graphical models. Experimental results on both an illustrative
example and a dataset with long-form documents demonstrate that our method
performs favorably compared to competing approaches.",http://arxiv.org/abs/2505.16965v1
Bigger Isn't Always Memorizing: Early Stopping Overparameterized Diffusion Models,"Diffusion probabilistic models have become a cornerstone of modern generative
AI, yet the mechanisms underlying their generalization remain poorly
understood. In fact, if these models were perfectly minimizing their training
loss, they would just generate data belonging to their training set, i.e.,
memorize, as empirically found in the overparameterized regime. We revisit this
view by showing that, in highly overparameterized diffusion models,
generalization in natural data domains is progressively achieved during
training before the onset of memorization. Our results, ranging from image to
language diffusion models, systematically support the empirical law that
memorization time is proportional to the dataset size. Generalization vs.
memorization is then best understood as a competition between time scales. We
show that this phenomenology is recovered in diffusion models learning a simple
probabilistic context-free grammar with random rules, where generalization
corresponds to the hierarchical acquisition of deeper grammar rules as training
time grows, and the generalization cost of early stopping can be characterized.
We summarize these results in a phase diagram. Overall, our results support
that a principled early-stopping criterion - scaling with dataset size - can
effectively optimize generalization while avoiding memorization, with direct
implications for hyperparameter transfer and privacy-sensitive applications.",http://arxiv.org/abs/2505.16959v1
A Comprehensive Evaluation of Contemporary ML-Based Solvers for Combinatorial Optimization,"Machine learning (ML) has demonstrated considerable potential in supporting
model design and optimization for combinatorial optimization (CO) problems.
However, much of the progress to date has been evaluated on small-scale,
synthetic datasets, raising concerns about the practical effectiveness of
ML-based solvers in real-world, large-scale CO scenarios. Additionally, many
existing CO benchmarks lack sufficient training data, limiting their utility
for evaluating data-driven approaches. To address these limitations, we
introduce FrontierCO, a comprehensive benchmark that covers eight canonical CO
problem types and evaluates 16 representative ML-based solvers--including graph
neural networks and large language model (LLM) agents. FrontierCO features
challenging instances drawn from industrial applications and frontier CO
research, offering both realistic problem difficulty and abundant training
data. Our empirical results provide critical insights into the strengths and
limitations of current ML methods, helping to guide more robust and practically
relevant advances at the intersection of machine learning and combinatorial
optimization. Our data is available at
https://huggingface.co/datasets/CO-Bench/FrontierCO.",http://arxiv.org/abs/2505.16952v1
ICYM2I: The illusion of multimodal informativeness under missingness,"Multimodal learning is of continued interest in artificial intelligence-based
applications, motivated by the potential information gain from combining
different types of data. However, modalities collected and curated during
development may differ from the modalities available at deployment due to
multiple factors including cost, hardware failure, or -- as we argue in this
work -- the perceived informativeness of a given modality. Na{\""i}ve estimation
of the information gain associated with including an additional modality
without accounting for missingness may result in improper estimates of that
modality's value in downstream tasks. Our work formalizes the problem of
missingness in multimodal learning and demonstrates the biases resulting from
ignoring this process. To address this issue, we introduce ICYM2I (In Case You
Multimodal Missed It), a framework for the evaluation of predictive performance
and information gain under missingness through inverse probability
weighting-based correction. We demonstrate the importance of the proposed
adjustment to estimate information gain under missingness on synthetic,
semi-synthetic, and real-world medical datasets.",http://arxiv.org/abs/2505.16953v1
Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised Reasoning,"Despite their impressive capabilities, Large Language Models struggle with
generalisation beyond their training distribution, often exhibiting
sophisticated pattern interpolation rather than true abstract reasoning
(extrapolation). In this work, we approach this limitation through the lens of
Information Bottleneck (IB) theory, which posits that model generalisation
emerges from an optimal balance between input compression and retention of
predictive information in latent representations. We prove using IB theory that
decoder-only Transformers are inherently constrained in their ability to form
task-optimal sequence representations. We then use this result to demonstrate
that periodic global transformation of the internal sequence-level
representations (KV cache) is a necessary computational step for improving
Transformer generalisation in reasoning tasks. Based on these theoretical
insights, we propose a modification to the Transformer architecture, in the
form of an additional module that globally rewrites the KV cache at periodic
intervals, shifting its capacity away from memorising input prefixes and toward
encoding features most useful for predicting future tokens. Our model delivers
substantial gains on mathematical reasoning benchmarks, outperforming both
vanilla Transformers with up to 3.5x more parameters, as well as
heuristic-driven pruning mechanisms for cache compression. Our approach can be
seen as a principled generalisation of existing KV-cache compression methods;
whereas such methods focus solely on compressing input representations, they
often do so at the expense of retaining predictive information, and thus their
capabilities are inherently bounded by those of an unconstrained model. This
establishes a principled framework to manipulate Transformer memory using
information theory, addressing fundamental reasoning limitations that scaling
alone cannot overcome.",http://arxiv.org/abs/2505.16950v1
MixAT: Combining Continuous and Discrete Adversarial Training for LLMs,"Despite recent efforts in Large Language Models (LLMs) safety and alignment,
current adversarial attacks on frontier LLMs are still able to force harmful
generations consistently. Although adversarial training has been widely studied
and shown to significantly improve the robustness of traditional machine
learning models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. As these relaxations do not correspond to discrete input tokens,
such latent training methods often leave models vulnerable to a diverse set of
discrete attacks. In this work, we aim to bridge this gap by introducing MixAT,
a novel method that combines stronger discrete and faster continuous attacks
during training. We rigorously evaluate MixAT across a wide spectrum of
state-of-the-art attacks, proposing the At Least One Attack Success Rate
(ALO-ASR) metric to capture the worst-case vulnerability of models. We show
MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to
prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to
methods based on continuous relaxations. We further analyze MixAT in realistic
deployment settings, exploring how chat templates, quantization, low-rank
adapters, and temperature affect both adversarial training and evaluation,
revealing additional blind spots in current methodologies. Our results
demonstrate that MixAT's discrete-continuous defense offers a principled and
superior robustness-accuracy tradeoff with minimal computational overhead,
highlighting its promise for building safer LLMs. We provide our code and
models at https://github.com/insait-institute/MixAT.",http://arxiv.org/abs/2505.16947v1
NY Real Estate Racial Equity Analysis via Applied Machine Learning,"This study analyzes tract-level real estate ownership patterns in New York
State (NYS) and New York City (NYC) to uncover racial disparities. We use an
advanced race/ethnicity imputation model (LSTM+Geo with XGBoost filtering,
validated at 89.2% accuracy) to compare the predicted racial composition of
property owners to the resident population from census data. We examine both a
Full Model (statewide) and a Name-Only LSTM Model (NYC) to assess how
incorporating geospatial context affects our predictions and disparity
estimates. The results reveal significant inequities: White individuals hold a
disproportionate share of properties and property value relative to their
population, while Black, Hispanic, and Asian communities are underrepresented
as property owners. These disparities are most pronounced in minority-majority
neighborhoods, where ownership is predominantly White despite a predominantly
non-White population. Corporate ownership (LLCs, trusts, etc.) exacerbates
these gaps by reducing owner-occupied opportunities in urban minority
communities. We provide a breakdown of ownership vs. population by race for
majority-White, -Black, -Hispanic, and -Asian tracts, identify those with
extreme ownership disparities, and compare patterns in urban, suburban, and
rural contexts. The findings underscore persistent racial inequity in property
ownership, reflecting broader historical and socio-economic forces, and
highlight the importance of data-driven approaches to address these issues.",http://arxiv.org/abs/2505.16946v1
Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical Flow Estimation,"Recent optical flow estimation methods often employ local cost sampling from
a dense all-pairs correlation volume. This results in quadratic computational
and memory complexity in the number of pixels. Although an alternative
memory-efficient implementation with on-demand cost computation exists, this is
slower in practice and therefore prior methods typically process images at
reduced resolutions, missing fine-grained details.
  To address this, we propose a more efficient implementation of the all-pairs
correlation volume sampling, still matching the exact mathematical operator as
defined by RAFT. Our approach outperforms on-demand sampling by up to 90% while
maintaining low memory usage, and performs on par with the default
implementation with up to 95% lower memory usage. As cost sampling makes up a
significant portion of the overall runtime, this can translate to up to 50%
savings for the total end-to-end model inference in memory-constrained
environments. Our evaluation of existing methods includes an 8K
ultra-high-resolution dataset and an additional inference-time modification of
the recent SEA-RAFT method. With this, we achieve state-of-the-art results at
high resolutions both in accuracy and efficiency.",http://arxiv.org/abs/2505.16942v1
FoMoH: A clinically meaningful foundation model evaluation for structured electronic health records,"Foundation models hold significant promise in healthcare, given their
capacity to extract meaningful representations independent of downstream tasks.
This property has enabled state-of-the-art performance across several clinical
applications trained on structured electronic health record (EHR) data, even in
settings with limited labeled data, a prevalent challenge in healthcare.
However, there is little consensus on these models' potential for clinical
utility due to the lack of desiderata of comprehensive and meaningful tasks and
sufficiently diverse evaluations to characterize the benefit over conventional
supervised learning. To address this gap, we propose a suite of clinically
meaningful tasks spanning patient outcomes, early prediction of acute and
chronic conditions, including desiderata for robust evaluations. We evaluate
state-of-the-art foundation models on EHR data consisting of 5 million patients
from Columbia University Irving Medical Center (CUMC), a large urban academic
medical center in New York City, across 14 clinically relevant tasks. We
measure overall accuracy, calibration, and subpopulation performance to surface
tradeoffs based on the choice of pre-training, tokenization, and data
representation strategies. Our study aims to advance the empirical evaluation
of structured EHR foundation models and guide the development of future
healthcare foundation models.",http://arxiv.org/abs/2505.16941v1
SPAR: Self-supervised Placement-Aware Representation Learning for Multi-Node IoT Systems,"This work develops the underpinnings of self-supervised placement-aware
representation learning given spatially-distributed (multi-view and multimodal)
sensor observations, motivated by the need to represent external environmental
state in multi-sensor IoT systems in a manner that correctly distills spatial
phenomena from the distributed multi-vantage observations. The objective of
sensing in IoT systems is, in general, to collectively represent an externally
observed environment given multiple vantage points from which sensory
observations occur. Pretraining of models that help interpret sensor data must
therefore encode the relation between signals observed by sensors and the
observers' vantage points in order to attain a representation that encodes the
observed spatial phenomena in a manner informed by the specific placement of
the measuring instruments, while allowing arbitrary placement. The work
significantly advances self-supervised model pretraining from IoT signals
beyond current solutions that often overlook the distinctive spatial nature of
IoT data. Our framework explicitly learns the dependencies between measurements
and geometric observer layouts and structural characteristics, guided by a core
design principle: the duality between signals and observer positions. We
further provide theoretical analyses from the perspectives of information
theory and occlusion-invariant representation learning to offer insight into
the rationale behind our design. Experiments on three real-world
datasets--covering vehicle monitoring, human activity recognition, and
earthquake localization--demonstrate the superior generalizability and
robustness of our method across diverse modalities, sensor placements,
application-level inference tasks, and spatial scales.",http://arxiv.org/abs/2505.16936v1
LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning,"In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large
Language Model (MLLM) that integrates visual instruction tuning with masked
diffusion models, representing a departure from the autoregressive paradigms
dominant in current multimodal approaches. Built upon LLaDA, a representative
large language diffusion model, LLaDA-V incorporates a vision encoder and MLP
connector that projects visual features into the language embedding space,
enabling effective multimodal alignment. Our empirical investigation reveals
several intriguing results: First, LLaDA-V demonstrates promising multimodal
performance despite its language model being weaker on purely textual tasks
than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same
instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal
tasks with better data scalability. It also narrows the performance gap to
Qwen2-VL, suggesting the effectiveness of its architecture for multimodal
tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal
understanding compared to existing hybrid autoregressive-diffusion and purely
diffusion-based MLLMs. Our findings suggest that large language diffusion
models show promise in multimodal contexts and warrant further investigation in
future research. Project page and codes:
https://ml-gsai.github.io/LLaDA-V-demo/.",http://arxiv.org/abs/2505.16933v1
The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm,"Computing the polar decomposition and the related matrix sign function, has
been a well-studied problem in numerical analysis for decades. More recently,
it has emerged as an important subroutine in deep learning, particularly within
the Muon optimization framework. However, the requirements in this setting
differ significantly from those of traditional numerical analysis. In deep
learning, methods must be highly efficient and GPU-compatible, but high
accuracy is often unnecessary. As a result, classical algorithms like
Newton-Schulz (which suffers from slow initial convergence) and methods based
on rational functions (which rely on QR decompositions or matrix inverses) are
poorly suited to this context. In this work, we introduce Polar Express, a
GPU-friendly algorithm for computing the polar decomposition. Like classical
polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix
multiplications, making it GPU-compatible. Motivated by earlier work of Chen &
Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule
at each iteration by solving a minimax optimization problem, and we prove that
it enjoys a strong worst-case optimality guarantee. This property ensures both
rapid early convergence and fast asymptotic convergence. We also address
finite-precision issues, making it stable in bfloat16 in practice. We apply
Polar Express within the Muon optimization framework and show consistent
improvements in validation loss on large-scale models such as GPT-2,
outperforming recent alternatives across a range of learning rates.",http://arxiv.org/abs/2505.16932v1
"Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning","We introduce $\infty$-THOR, a new framework for long-horizon embodied tasks
that advances long-context understanding in embodied AI. $\infty$-THOR
provides: (1) a generation framework for synthesizing scalable, reproducible,
and unlimited long-horizon trajectories; (2) a novel embodied QA task,
Needle(s) in the Embodied Haystack, where multiple scattered clues across
extended trajectories test agents' long-context reasoning ability; and (3) a
long-horizon dataset and benchmark suite featuring complex tasks that span
hundreds of environment steps, each paired with ground-truth action sequences.
To enable this capability, we explore architectural adaptations, including
interleaved Goal-State-Action modeling, context extension techniques, and
Context Parallelism, to equip LLM-based agents for extreme long-context
reasoning and interaction. Experimental results and analyses highlight the
challenges posed by our benchmark and provide insights into training strategies
and model behaviors under long-horizon conditions. Our work provides a
foundation for the next generation of embodied AI systems capable of robust,
long-term reasoning and planning.",http://arxiv.org/abs/2505.16928v1
Latent Principle Discovery for Language Model Self-Improvement,"When language model (LM) users aim to improve the quality of its generations,
it is crucial to specify concrete behavioral attributes that the model should
strive to reflect. However, curating such principles across many domains, even
non-exhaustively, requires a labor-intensive annotation process. To automate
this process, we propose eliciting these latent attributes guiding model
reasoning towards human-preferred responses by explicitly modeling them in a
self-correction setting. Our approach mines new principles from the LM itself
and compresses the discovered elements to an interpretable set via clustering.
Specifically, we employ an approximation of posterior-regularized Monte Carlo
Expectation-Maximization to both identify a condensed set of the most effective
latent principles and teach the LM to strategically invoke them in order to
intrinsically refine its responses. We demonstrate that bootstrapping our
algorithm over multiple iterations enables smaller language models (7-8B
parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an
average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on
IFEval. We also show that clustering the principles yields interpretable and
diverse model-generated constitutions while retaining model performance. The
gains our method achieves highlight the potential of automated,
principle-driven post-training recipes toward continual self-improvement.",http://arxiv.org/abs/2505.16927v1
Risk-Averse Reinforcement Learning with Itakura-Saito Loss,"Risk-averse reinforcement learning finds application in various high-stakes
fields. Unlike classical reinforcement learning, which aims to maximize
expected returns, risk-averse agents choose policies that minimize risk,
occasionally sacrificing expected value. These preferences can be framed
through utility theory. We focus on the specific case of the exponential
utility function, where we can derive the Bellman equations and employ various
reinforcement learning algorithms with few modifications. However, these
methods suffer from numerical instability due to the need for exponent
computation throughout the process. To address this, we introduce a numerically
stable and mathematically sound loss function based on the Itakura-Saito
divergence for learning state-value and action-value functions. We evaluate our
proposed loss function against established alternatives, both theoretically and
empirically. In the experimental section, we explore multiple financial
scenarios, some with known analytical solutions, and show that our loss
function outperforms the alternatives.",http://arxiv.org/abs/2505.16925v1
TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation,"A reliable uncertainty estimation method is the foundation of many modern
out-of-distribution (OOD) detectors, which are critical for safe deployments of
deep learning models in the open world. In this work, we propose TULiP, a
theoretically-driven post-hoc uncertainty estimator for OOD detection. Our
approach considers a hypothetical perturbation applied to the network before
convergence. Based on linearized training dynamics, we bound the effect of such
perturbation, resulting in an uncertainty score computable by perturbing model
parameters. Ultimately, our approach computes uncertainty from a set of sampled
predictions. We visualize our bound on synthetic regression and classification
datasets. Furthermore, we demonstrate the effectiveness of TULiP using
large-scale OOD detection benchmarks for image classification. Our method
exhibits state-of-the-art performance, particularly for near-distribution
samples.",http://arxiv.org/abs/2505.16923v1
Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype,"This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)
methods and introduces an experimental framework for scalable, interpretable
offer selection, addressing the challenge of fast-changing offers. The approach
models context at the product category level, allowing offers to span multiple
categories and enabling knowledge transfer across similar offers. This improves
learning efficiency and generalization in dynamic environments. The framework
extends standard CMAB methodology to support multi-category contexts, and
achieves scalability through efficient feature engineering and modular design.
Advanced features such as MPG (Member Purchase Gap) and MF (Matrix
Factorization) capture nuanced user-offer interactions, with implementation in
Python for practical deployment.
  A key contribution is interpretability at scale: logistic regression models
yield transparent weight vectors, accessible via a large language model (LLM)
interface for real-time, user-level tracking and explanation of evolving
preferences. This enables the generation of detailed member profiles and
identification of behavioral patterns, supporting personalized offer
optimization and enhancing trust in automated decisions. By situating our
prototype alongside established paradigms like Generalized Linear Models and
Thompson Sampling, we demonstrate its value for both research and real-world
CMAB applications.",http://arxiv.org/abs/2505.16918v1
Unsupervised Prompting for Graph Neural Networks,"Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to
address the semantic gap between pre-training and fine-tuning steps. However,
existing GNN prompting methods rely on labeled data and involve lightweight
fine-tuning for downstream tasks. Meanwhile, in-context learning methods for
Large Language Models (LLMs) have shown promising performance with no parameter
updating and no or minimal labeled data. Inspired by these approaches, in this
work, we first introduce a challenging problem setup to evaluate GNN prompting
methods. This setup encourages a prompting function to enhance a pre-trained
GNN's generalization to a target dataset under covariate shift without updating
the GNN's parameters and with no labeled data. Next, we propose a fully
unsupervised prompting method based on consistency regularization through
pseudo-labeling. We use two regularization techniques to align the prompted
graphs' distribution with the original data and reduce biased predictions.
Through extensive experiments under our problem setting, we demonstrate that
our unsupervised approach outperforms the state-of-the-art prompting methods
that have access to labels.",http://arxiv.org/abs/2505.16903v1
Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks,"Recent advances in Large Language Models (LLMs) have shown promise in
function-level code generation, yet repository-level software engineering tasks
remain challenging. Current solutions predominantly rely on proprietary LLM
agents, which introduce unpredictability and limit accessibility, raising
concerns about data privacy and model customization. This paper investigates
whether open-source LLMs can effectively address repository-level tasks without
requiring agent-based approaches. We demonstrate this is possible by enabling
LLMs to comprehend functions and files within codebases through their semantic
information and structural dependencies. To this end, we introduce Code Graph
Models (CGMs), which integrate repository code graph structures into the LLM's
attention mechanism and map node attributes to the LLM's input space using a
specialized adapter. When combined with an agentless graph RAG framework, our
approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark
using the open-source Qwen2.5-72B model. This performance ranks first among
open weight models, second among methods with open-source systems, and eighth
overall, surpassing the previous best open-source model-based method by 12.33%.",http://arxiv.org/abs/2505.16901v1
Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality,"During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.",http://arxiv.org/abs/2505.16900v1
Structure-Aligned Protein Language Model,"Protein language models (pLMs) pre-trained on vast protein sequence databases
excel at various downstream tasks but lack the structural knowledge essential
for many biological applications. To address this, we integrate structural
insights from pre-trained protein graph neural networks (pGNNs) into pLMs
through a latent-level contrastive learning task. This task aligns residue
representations from pLMs with those from pGNNs across multiple proteins,
enriching pLMs with inter-protein structural knowledge. Additionally, we
incorporate a physical-level task that infuses intra-protein structural
knowledge by optimizing pLMs to predict structural tokens. The proposed
dual-task framework effectively incorporates both inter-protein and
intra-protein structural knowledge into pLMs. Given the variability in the
quality of protein structures in PDB, we further introduce a residue loss
selection module, which uses a small model trained on high-quality structures
to select reliable yet challenging residue losses for the pLM to learn.
Applying our structure alignment method to the state-of-the-art ESM2 and
AMPLIFY results in notable performance gains across a wide range of tasks,
including a 12.7% increase in ESM2 contact prediction. The data, code, and
resulting SaESM2 and SaAMPLIFY models will be released on Hugging Face.",http://arxiv.org/abs/2505.16896v1
Statistical Test for Saliency Maps of Graph Neural Networks via Selective Inference,"Graph Neural Networks (GNNs) have gained prominence for their ability to
process graph-structured data across various domains. However, interpreting GNN
decisions remains a significant challenge, leading to the adoption of saliency
maps for identifying influential nodes and edges. Despite their utility, the
reliability of GNN saliency maps has been questioned, particularly in terms of
their robustness to noise. In this study, we propose a statistical testing
framework to rigorously evaluate the significance of saliency maps. Our main
contribution lies in addressing the inflation of the Type I error rate caused
by double-dipping of data, leveraging the framework of Selective Inference. Our
method provides statistically valid $p$-values while controlling the Type I
error rate, ensuring that identified salient subgraphs contain meaningful
information rather than random artifacts. To demonstrate the effectiveness of
our method, we conduct experiments on both synthetic and real-world datasets,
showing its effectiveness in assessing the reliability of GNN interpretations.",http://arxiv.org/abs/2505.16893v1
"Don't ""Overthink"" Passage Reranking: Is Reasoning Truly Necessary?","With the growing success of reasoning models across complex natural language
tasks, researchers in the Information Retrieval (IR) community have begun
exploring how similar reasoning capabilities can be integrated into passage
rerankers built on Large Language Models (LLMs). These methods typically employ
an LLM to produce an explicit, step-by-step reasoning process before arriving
at a final relevance prediction. But, does reasoning actually improve reranking
accuracy? In this paper, we dive deeper into this question, studying the impact
of the reasoning process by comparing reasoning-based pointwise rerankers
(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under
identical training conditions, and observe that StandardRR generally
outperforms ReasonRR. Building on this observation, we then study the
importance of reasoning to ReasonRR by disabling its reasoning process
(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more
effective than ReasonRR. Examining the cause of this result, our findings
reveal that reasoning-based rerankers are limited by the LLM's reasoning
process, which pushes it toward polarized relevance scores and thus fails to
consider the partial relevance of passages, a key factor for the accuracy of
pointwise rerankers.",http://arxiv.org/abs/2505.16886v1
How high is `high'? Rethinking the roles of dimensionality in topological data analysis and manifold learning,"We present a generalised Hanson-Wright inequality and use it to establish new
statistical insights into the geometry of data point-clouds. In the setting of
a general random function model of data, we clarify the roles played by three
notions of dimensionality: ambient intrinsic dimension $p_{\mathrm{int}}$,
which measures total variability across orthogonal feature directions;
correlation rank, which measures functional complexity across samples; and
latent intrinsic dimension, which is the dimension of manifold structure hidden
in data. Our analysis shows that in order for persistence diagrams to reveal
latent homology and for manifold structure to emerge it is sufficient that
$p_{\mathrm{int}}\gg \log n$, where $n$ is the sample size. Informed by these
theoretical perspectives, we revisit the ground-breaking neuroscience discovery
of toroidal structure in grid-cell activity made by Gardner et al. (Nature,
2022): our findings reveal, for the first time, evidence that this structure is
in fact isometric to physical space, meaning that grid cell activity conveys a
geometrically faithful representation of the real world.",http://arxiv.org/abs/2505.16879v1
T2I-ConBench: Text-to-Image Benchmark for Continual Post-training,"Continual post-training adapts a single text-to-image diffusion model to
learn new tasks without incurring the cost of separate models, but naive
post-training causes forgetting of pretrained knowledge and undermines
zero-shot compositionality. We observe that the absence of a standardized
evaluation protocol hampers related research for continual post-training. To
address this, we introduce T2I-ConBench, a unified benchmark for continual
post-training of text-to-image models. T2I-ConBench focuses on two practical
scenarios, item customization and domain enhancement, and analyzes four
dimensions: (1) retention of generality, (2) target-task performance, (3)
catastrophic forgetting, and (4) cross-task generalization. It combines
automated metrics, human-preference modeling, and vision-language QA for
comprehensive assessment. We benchmark ten representative methods across three
realistic task sequences and find that no approach excels on all fronts. Even
joint ""oracle"" training does not succeed for every task, and cross-task
generalization remains unsolved. We release all datasets, code, and evaluation
tools to accelerate research in continual post-training for text-to-image
models.",http://arxiv.org/abs/2505.16875v1
A Multi-Step Comparative Framework for Anomaly Detection in IoT Data Streams,"The rapid expansion of Internet of Things (IoT) devices has introduced
critical security challenges, underscoring the need for accurate anomaly
detection. Although numerous studies have proposed machine learning (ML)
methods for this purpose, limited research systematically examines how
different preprocessing steps--normalization, transformation, and feature
selection--interact with distinct model architectures. To address this gap,
this paper presents a multi-step evaluation framework assessing the combined
impact of preprocessing choices on three ML algorithms: RNN-LSTM, autoencoder
neural networks (ANN), and Gradient Boosting (GBoosting). Experiments on the
IoTID20 dataset shows that GBoosting consistently delivers superior accuracy
across preprocessing configurations, while RNN-LSTM shows notable gains with
z-score normalization and autoencoders excel in recall, making them well-suited
for unsupervised scenarios. By offering a structured analysis of preprocessing
decisions and their interplay with various ML techniques, the proposed
framework provides actionable guidance to enhance anomaly detection performance
in IoT environments.",http://arxiv.org/abs/2505.16872v1
GCAL: Adapting Graph Models to Evolving Domain Shifts,"This paper addresses the challenge of graph domain adaptation on evolving,
multiple out-of-distribution (OOD) graphs. Conventional graph domain adaptation
methods are confined to single-step adaptation, making them ineffective in
handling continuous domain shifts and prone to catastrophic forgetting. This
paper introduces the Graph Continual Adaptive Learning (GCAL) method, designed
to enhance model sustainability and adaptability across various graph domains.
GCAL employs a bilevel optimization strategy. The ""adapt"" phase uses an
information maximization approach to fine-tune the model with new graph domains
while re-adapting past memories to mitigate forgetting. Concurrently, the
""generate memory"" phase, guided by a theoretical lower bound derived from
information bottleneck theory, involves a variational memory graph generation
module to condense original graphs into memories. Extensive experimental
evaluations demonstrate that GCAL substantially outperforms existing methods in
terms of adaptability and knowledge retention.",http://arxiv.org/abs/2505.16860v1
Redefining Clustered Federated Learning for System Identification: The Path of ClusterCraft,"This paper addresses the System Identification (SYSID) problem within the
framework of federated learning. We introduce a novel algorithm, Incremental
Clustering-based federated learning method for SYSID (IC-SYSID), designed to
tackle SYSID challenges across multiple data sources without prior knowledge.
IC-SYSID utilizes an incremental clustering method, ClusterCraft (CC), to
eliminate the dependency on the prior knowledge of the dataset. CC starts with
a single cluster model and assigns similar local workers to the same clusters
by dynamically increasing the number of clusters. To reduce the number of
clusters generated by CC, we introduce ClusterMerge, where similar cluster
models are merged. We also introduce enhanced ClusterCraft to reduce the
generation of similar cluster models during the training. Moreover, IC-SYSID
addresses cluster model instability by integrating a regularization term into
the loss function and initializing cluster models with scaled Glorot
initialization. It also utilizes a mini-batch deep learning approach to manage
large SYSID datasets during local training. Through the experiments conducted
on a real-world representing SYSID problem, where a fleet of vehicles
collaboratively learns vehicle dynamics, we show that IC-SYSID achieves a high
SYSID performance while preventing the learning of unstable clusters.",http://arxiv.org/abs/2505.16857v1
Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only,"Improving the performance of pre-trained policies through online
reinforcement learning (RL) is a critical yet challenging topic. Existing
online RL fine-tuning methods require continued training with offline
pretrained Q-functions for stability and performance. However, these offline
pretrained Q-functions commonly underestimate state-action pairs beyond the
offline dataset due to the conservatism in most offline RL methods, which
hinders further exploration when transitioning from the offline to the online
setting. Additionally, this requirement limits their applicability in scenarios
where only pre-trained policies are available but pre-trained Q-functions are
absent, such as in imitation learning (IL) pre-training. To address these
challenges, we propose a method for efficient online RL fine-tuning using
solely the offline pre-trained policy, eliminating reliance on pre-trained
Q-functions. We introduce PORL (Policy-Only Reinforcement Learning
Fine-Tuning), which rapidly initializes the Q-function from scratch during the
online phase to avoid detrimental pessimism. Our method not only achieves
competitive performance with advanced offline-to-online RL algorithms and
online RL approaches that leverage data or policies prior, but also pioneers a
new path for directly fine-tuning behavior cloning (BC) policies.",http://arxiv.org/abs/2505.16856v1
"ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning","Federated Learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data privacy across decentralized participants.
As FL adoption grows, numerous techniques have been proposed to tackle its
practical challenges. However, the lack of standardized evaluation across key
dimensions hampers systematic progress and fair comparison of FL methods. In
this work, we introduce ATR-Bench, a unified framework for analyzing federated
learning through three foundational dimensions: Adaptation, Trust, and
Reasoning. We provide an in-depth examination of the conceptual foundations,
task formulations, and open research challenges associated with each theme. We
have extensively benchmarked representative methods and datasets for adaptation
to heterogeneous clients and trustworthiness in adversarial or unreliable
environments. Due to the lack of reliable metrics and models for reasoning in
FL, we only provide literature-driven insights for this dimension. ATR-Bench
lays the groundwork for a systematic and holistic evaluation of federated
learning with real-world relevance. We will make our complete codebase publicly
accessible and a curated repository that continuously tracks new developments
and research in the FL literature.",http://arxiv.org/abs/2505.16850v1
Strategically Linked Decisions in Long-Term Planning and Reinforcement Learning,"Long-term planning, as in reinforcement learning (RL), involves finding
strategies: actions that collectively work toward a goal rather than
individually optimizing their immediate outcomes. As part of a strategy, some
actions are taken at the expense of short-term benefit to enable future actions
with even greater returns. These actions are only advantageous if followed up
by the actions they facilitate, consequently, they would not have been taken if
those follow-ups were not available. In this paper, we quantify such
dependencies between planned actions with strategic link scores: the drop in
the likelihood of one decision under the constraint that a follow-up decision
is no longer available. We demonstrate the utility of strategic link scores
through three practical applications: (i) explaining black-box RL agents by
identifying strategically linked pairs among decisions they make, (ii)
improving the worst-case performance of decision support systems by
distinguishing whether recommended actions can be adopted as standalone
improvements or whether they are strategically linked hence requiring a
commitment to a broader strategy to be effective, and (iii) characterizing the
planning processes of non-RL agents purely through interventions aimed at
measuring strategic link scores - as an example, we consider a realistic
traffic simulator and analyze through road closures the effective planning
horizon of the emergent routing behavior of many drivers.",http://arxiv.org/abs/2505.16833v1
From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization,"While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.",http://arxiv.org/abs/2505.16832v1
Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs,"Unlearning in large language models (LLMs) is intended to remove the
influence of specific data, yet current evaluations rely heavily on token-level
metrics such as accuracy and perplexity. We show that these metrics can be
misleading: models often appear to forget, but their original behavior can be
rapidly restored with minimal fine-tuning, revealing that unlearning may
obscure information rather than erase it. To diagnose this phenomenon, we
introduce a representation-level evaluation framework using PCA-based
similarity and shift, centered kernel alignment, and Fisher information.
Applying this toolkit across six unlearning methods, three domains (text, code,
math), and two open-source LLMs, we uncover a critical distinction between
reversible and irreversible forgetting. In reversible cases, models suffer
token-level collapse yet retain latent features; in irreversible cases, deeper
representational damage occurs. We further provide a theoretical account
linking shallow weight perturbations near output layers to misleading
unlearning signals, and show that reversibility is modulated by task type and
hyperparameters. Our findings reveal a fundamental gap in current evaluation
practices and establish a new diagnostic foundation for trustworthy unlearning
in LLMs. We provide a unified toolkit for analyzing LLM representation changes
under unlearning and relearning:
https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.",http://arxiv.org/abs/2505.16831v1
Contextual Learning for Stochastic Optimization,"Motivated by stochastic optimization, we introduce the problem of learning
from samples of contextual value distributions. A contextual value distribution
can be understood as a family of real-valued distributions, where each sample
consists of a context $x$ and a random variable drawn from the corresponding
real-valued distribution $D_x$. By minimizing a convex surrogate loss, we learn
an empirical distribution $D'_x$ for each context, ensuring a small L\'evy
distance to $D_x$. We apply this result to obtain the sample complexity bounds
for the learning of an $\epsilon$-optimal policy for stochastic optimization
problems defined on an unknown contextual value distribution. The sample
complexity is shown to be polynomial for the general case of strongly monotone
and stable optimization problems, including Single-item Revenue Maximization,
Pandora's Box and Optimal Stopping.",http://arxiv.org/abs/2505.16829v1
LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols,"Integrating large AI models (LAMs) into 6G mobile networks promises to
redefine protocol design and control-plane intelligence by enabling autonomous,
cognitive network operations. While industry concepts, such as ETSI's
Experiential Networked Intelligence (ENI), envision LAM-driven agents for
adaptive network slicing and intent-based management, practical implementations
still face challenges in protocol literacy and real-world deployment. This
paper presents an end-to-end demonstration of a LAM that generates
standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as
part of control-plane procedures inside a gNB. We treat RRC messaging as a
domain-specific language and fine-tune a decoder-only transformer model (LLaMA
class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages
linearized to retain their ASN.1 syntactic structure before standard byte-pair
encoding tokenization. This enables combinatorial generalization over RRC
protocol states while minimizing training overhead. On 30k field-test
request-response pairs, our 8 B model achieves a median cosine similarity of
0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a
zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural
and semantic RRC fidelity. Overall, our results show that LAMs, when augmented
with Radio Access Network (RAN)-specific reasoning, can directly orchestrate
control-plane procedures, representing a stepping stone toward the AI-native
air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for
future AI-native wireless standards.",http://arxiv.org/abs/2505.16821v1
A modular framework for automated evaluation of procedural content generation in serious games with deep reinforcement learning agents,"Serious Games (SGs) are nowadays shifting focus to include procedural content
generation (PCG) in the development process as a means of offering personalized
and enhanced player experience. However, the development of a framework to
assess the impact of PCG techniques when integrated into SGs remains
particularly challenging. This study proposes a methodology for automated
evaluation of PCG integration in SGs, incorporating deep reinforcement learning
(DRL) game testing agents. To validate the proposed framework, a previously
introduced SG featuring card game mechanics and incorporating three different
versions of PCG for nonplayer character (NPC) creation has been deployed.
Version 1 features random NPC creation, while versions 2 and 3 utilize a
genetic algorithm approach. These versions are used to test the impact of
different dynamic SG environments on the proposed framework's agents. The
obtained results highlight the superiority of the DRL game testing agents
trained on Versions 2 and 3 over those trained on Version 1 in terms of win
rate (i.e. number of wins per played games) and training time. More
specifically, within the execution of a test emulating regular gameplay, both
Versions 2 and 3 peaked at a 97% win rate and achieved statistically
significant higher (p=0009) win rates compared to those achieved in Version 1
that peaked at 94%. Overall, results advocate towards the proposed framework's
capability to produce meaningful data for the evaluation of procedurally
generated content in SGs.",http://arxiv.org/abs/2505.16801v1
Cohort-Based Active Modality Acquisition,"Real-world machine learning applications often involve data from multiple
modalities that must be integrated effectively to make robust predictions.
However, in many practical settings, not all modalities are available for every
sample, and acquiring additional modalities can be costly. This raises the
question: which samples should be prioritized for additional modality
acquisition when resources are limited? While prior work has explored
individual-level acquisition strategies and training-time active learning
paradigms, test-time and cohort-based acquisition remain underexplored despite
their importance in many real-world settings. We introduce Cohort-based Active
Modality Acquisition (CAMA), a novel test-time setting to formalize the
challenge of selecting which samples should receive additional modalities. We
derive acquisition strategies that leverage a combination of generative
imputation and discriminative modeling to estimate the expected benefit of
acquiring missing modalities based on common evaluation metrics. We also
introduce upper-bound heuristics that provide performance ceilings to benchmark
acquisition strategies. Experiments on common multimodal datasets demonstrate
that our proposed imputation-based strategies can more effectively guide the
acquisition of new samples in comparison to those relying solely on unimodal
information, entropy guidance, and random selections. Our work provides an
effective solution for optimizing modality acquisition at the cohort level,
enabling better utilization of resources in constrained settings.",http://arxiv.org/abs/2505.16791v1
Learning Flexible Forward Trajectories for Masked Molecular Diffusion,"Masked diffusion models (MDMs) have achieved notable progress in modeling
discrete data, while their potential in molecular generation remains
underexplored. In this work, we explore their potential and introduce the
surprising result that naively applying standards MDMs severely degrades the
performance. We identify the critical cause of this issue as a state-clashing
problem-where the forward diffusion of distinct molecules collapse into a
common state, resulting in a mixture of reconstruction targets that cannot be
learned using typical reverse diffusion process with unimodal predictions. To
mitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that
orchestrates per-element corruption trajectories to avoid collision between
distinct molecular graphs. This is achieved through a parameterized noise
scheduling network that assigns distinct corruption rates to individual graph
elements, i.e., atoms and bonds. Extensive experiments on diverse molecular
benchmarks reveal that MELD markedly enhances overall generation quality
compared to element-agnostic noise scheduling, increasing the chemical validity
of vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves
state-of-the-art property alignment in conditional generation tasks.",http://arxiv.org/abs/2505.16790v1
Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability,"As large language models gain popularity, their vulnerability to adversarial
attacks remains a primary concern. While fine-tuning models on domain-specific
datasets is often employed to improve model performance, it can introduce
vulnerabilities within the underlying model. In this work, we investigate
Accidental Misalignment, unexpected vulnerabilities arising from
characteristics of fine-tuning data. We begin by identifying potential
correlation factors such as linguistic features, semantic similarity, and
toxicity within our experimental datasets. We then evaluate the adversarial
performance of these fine-tuned models and assess how dataset factors correlate
with attack success rates. Lastly, we explore potential causal links, offering
new insights into adversarial defense strategies and highlighting the crucial
role of dataset design in preserving model alignment. Our code is available at
https://github.com/psyonp/accidental_misalignment.",http://arxiv.org/abs/2505.16789v1
FlowMixer: A Constrained Neural Architecture for Interpretable Spatiotemporal Forecasting,"We introduce FlowMixer, a neural architecture that leverages constrained
matrix operations to model structured spatiotemporal patterns. At its core,
FlowMixer incorporates non-negative matrix mixing layers within a reversible
mapping framework-applying transforms before mixing and their inverses
afterward. This shape-preserving design enables a Kronecker-Koopman eigenmode
framework that bridges statistical learning with dynamical systems theory,
providing interpretable spatiotemporal patterns and facilitating direct
algebraic manipulation of prediction horizons without retraining. Extensive
experiments across diverse domains demonstrate FlowMixer's robust long-horizon
forecasting capabilities while effectively modeling physical phenomena such as
chaotic attractors and turbulent flows. These results suggest that
architectural constraints can simultaneously enhance predictive performance and
mathematical interpretability in neural forecasting systems.",http://arxiv.org/abs/2505.16786v1
Multi-Output Gaussian Processes for Graph-Structured Data,"Graph-structured data is a type of data to be obtained associated with a
graph structure where vertices and edges describe some kind of data
correlation. This paper proposes a regression method on graph-structured data,
which is based on multi-output Gaussian processes (MOGP), to capture both the
correlation between vertices and the correlation between associated data. The
proposed formulation is built on the definition of MOGP. This allows it to be
applied to a wide range of data configurations and scenarios. Moreover, it has
high expressive capability due to its flexibility in kernel design. It includes
existing methods of Gaussian processes for graph-structured data as special
cases and is possible to remove restrictions on data configurations, model
selection, and inference scenarios in the existing methods. The performance of
extensions achievable by the proposed formulation is evaluated through computer
experiments with synthetic and real data.",http://arxiv.org/abs/2505.16755v1
PyTupli: A Scalable Infrastructure for Collaborative Offline Reinforcement Learning Projects,"Offline reinforcement learning (RL) has gained traction as a powerful
paradigm for learning control policies from pre-collected data, eliminating the
need for costly or risky online interactions. While many open-source libraries
offer robust implementations of offline RL algorithms, they all rely on
datasets composed of experience tuples consisting of state, action, next state,
and reward. Managing, curating, and distributing such datasets requires
suitable infrastructure. Although static datasets exist for established
benchmark problems, no standardized or scalable solution supports developing
and sharing datasets for novel or user-defined benchmarks. To address this gap,
we introduce PyTupli, a Python-based tool to streamline the creation, storage,
and dissemination of benchmark environments and their corresponding tuple
datasets. PyTupli includes a lightweight client library with defined interfaces
for uploading and retrieving benchmarks and data. It supports fine-grained
filtering at both the episode and tuple level, allowing researchers to curate
high-quality, task-specific datasets. A containerized server component enables
production-ready deployment with authentication, access control, and automated
certificate provisioning for secure use. By addressing key barriers in dataset
infrastructure, PyTupli facilitates more collaborative, reproducible, and
scalable offline RL research.",http://arxiv.org/abs/2505.16754v1
Revenue Optimization with Price-Sensitive and Interdependent Demand,"As Kalyan T. Talluri and Garrett J. Van Ryzin describe in their work [3],
Revenue Management aims to maximize an organization's revenue by considering
three types of decision categories: structural, pricing, and quantity. In this
document, our primary focus will be on decisions related to pricing and
quantity for the sale of airline tickets on a direct flight over a certain
number of time periods. More specifically, we will only focus on the
optimization aspect of this problem. We will assume the demand data to be
given, since Air France estimates it beforehand using real data. Similarly, we
assume all price options to be predetermined by Air France's algorithms and
verified by their analysts. Our objective will be to maximize the revenue of a
direct flight by choosing the prices for each product from the predefined set
of options.
  --
  Comme d\'ecrit par Kalyan T. Talluri et Garrett J. Van Ryzin dans leur
ouvrage [3], le Revenue Management consiste en la maximisation du revenu d'un
organisme \`a partir de trois types de cat\'egories de d\'ecision :
structurelles, prix et quantit\'e. Dans ce document, nous nous int\'eresserons
principalement aux d\'ecisions de type prix et quantit\'e pour la vente de
billets d'avion sur un vol direct au cours d'un certain nombre de pas de temps.
Plus pr\'ecis\'ement, nous nous situerons dans la partie optimisation du
probl\`eme. Nous prendrons ainsi les donn\'ees de demande comme acquises, car
elles sont estim\'ees au pr\'ealable par Air France \`a partir des donn\'ees
r\'eelles. De m\^eme, pour chaque produit que l'on cherchera \`a vendre, on
nous impose en amont les prix possibles que l'on a droit d'utiliser et qui se
basent sur des algorithmes d'Air France dont les r\'esultats sont v\'erifi\'es
par des analystes. Notre but sera alors de maximiser le revenu d'un vol direct
en choisissant les prix de chaque produit parmi ceux impos\'es.",http://arxiv.org/abs/2505.16748v1
TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning,"Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making pruning essential for their
efficient deployment. Existing one-shot pruning methods often apply uniform
sparsity constraints across layers or within each layer, resulting in
suboptimal performance, especially at high sparsity ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel
approach that applies varying sparsity ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise sparsity allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise pruning
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise sparsity adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM",http://arxiv.org/abs/2505.16743v1
Meta-reinforcement learning with minimum attention,"Minimum attention applies the least action principle in the changes of
control concerning state and time, first proposed by Brockett. The involved
regularization is highly relevant in emulating biological control, such as
motor learning. We apply minimum attention in reinforcement learning (RL) as
part of the rewards and investigate its connection to meta-learning and
stabilization. Specifically, model-based meta-learning with minimum attention
is explored in high-dimensional nonlinear dynamics. Ensemble-based model
learning and gradient-based meta-policy learning are alternately performed.
Empirically, we show that the minimum attention does show outperforming
competence in comparison to the state-of-the-art algorithms in model-free and
model-based RL, i.e., fast adaptation in few shots and variance reduction from
the perturbations of the model and environment. Furthermore, the minimum
attention demonstrates the improvement in energy efficiency.",http://arxiv.org/abs/2505.16741v1
Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization,"The significant progress of large language models (LLMs) has led to
remarkable achievements across numerous applications. However, their ability to
generate harmful content has sparked substantial safety concerns. Despite the
implementation of safety alignment techniques during the pre-training phase,
recent research indicates that fine-tuning LLMs on adversarial or even benign
data can inadvertently compromise their safety. In this paper, we re-examine
the fundamental issue of why fine-tuning on non-harmful data still results in
safety degradation. We introduce a safety-aware probing (SAP) optimization
framework designed to mitigate the safety risks of fine-tuning LLMs.
Specifically, SAP incorporates a safety-aware probe into the gradient
propagation process, mitigating the model's risk of safety degradation by
identifying potential pitfalls in gradient directions, thereby enhancing
task-specific performance while successfully preserving model safety. Our
extensive experimental results demonstrate that SAP effectively reduces
harmfulness below the original fine-tuned model and achieves comparable test
loss to standard fine-tuning methods. Our code is available at
https://github.com/ChengcanWu/SAP.",http://arxiv.org/abs/2505.16737v1
Backward Oversmoothing: why is it hard to train deep Graph Neural Networks?,"Oversmoothing has long been identified as a major limitation of Graph Neural
Networks (GNNs): input node features are smoothed at each layer and converge to
a non-informative representation, if the weights of the GNN are sufficiently
bounded. This assumption is crucial: if, on the contrary, the weights are
sufficiently large, then oversmoothing may not happen. Theoretically, GNN could
thus learn to not oversmooth. However it does not really happen in practice,
which prompts us to examine oversmoothing from an optimization point of view.
In this paper, we analyze backward oversmoothing, that is, the notion that
backpropagated errors used to compute gradients are also subject to
oversmoothing from output to input. With non-linear activation functions, we
outline the key role of the interaction between forward and backward smoothing.
Moreover, we show that, due to backward oversmoothing, GNNs provably exhibit
many spurious stationary points: as soon as the last layer is trained, the
whole GNN is at a stationary point. As a result, we can exhibit regions where
gradients are near-zero while the loss stays high. The proof relies on the fact
that, unlike forward oversmoothing, backward errors are subjected to a linear
oversmoothing even in the presence of non-linear activation function, such that
the average of the output error plays a key role. Additionally, we show that
this phenomenon is specific to deep GNNs, and exhibit counter-example
Multi-Layer Perceptron. This paper is a step toward a more complete
comprehension of the optimization landscape specific to GNNs.",http://arxiv.org/abs/2505.16736v1
Maximum Total Correlation Reinforcement Learning,"Simplicity is a powerful inductive bias. In reinforcement learning,
regularization is used for simpler policies, data augmentation for simpler
representations, and sparse reward functions for simpler objectives, all that,
with the underlying motivation to increase generalizability and robustness by
focusing on the essentials. Supplementary to these techniques, we investigate
how to promote simple behavior throughout the episode. To that end, we
introduce a modification of the reinforcement learning problem that
additionally maximizes the total correlation within the induced trajectories.
We propose a practical algorithm that optimizes all models, including policy
and state representation, based on a lower-bound approximation. In simulated
robot environments, our method naturally generates policies that induce
periodic and compressible trajectories, and that exhibit superior robustness to
noise and changes in dynamics compared to baseline methods, while also
improving performance in the original tasks.",http://arxiv.org/abs/2505.16734v1
Forward-only Diffusion Probabilistic Models,"This work presents a forward-only diffusion (FoD) approach for generative
modelling. In contrast to traditional diffusion models that rely on a coupled
forward-backward diffusion scheme, FoD directly learns data generation through
a single forward diffusion process, yielding a simple yet efficient generative
framework. The core of FoD is a state-dependent linear stochastic differential
equation that involves a mean-reverting term in both the drift and diffusion
functions. This mean-reversion property guarantees the convergence to clean
data, naturally simulating a stochastic interpolation between source and target
distributions. More importantly, FoD is analytically tractable and is trained
using a simple stochastic flow matching objective, enabling a few-step
non-Markov chain sampling during inference. The proposed FoD model, despite its
simplicity, achieves competitive performance on various image-conditioned
(e.g., image restoration) and unconditional generation tasks, demonstrating its
effectiveness in generative modelling. Our code is available at
https://github.com/Algolzw/FoD.",http://arxiv.org/abs/2505.16733v1
Sequential Monte Carlo for Policy Optimization in Continuous POMDPs,"Optimal decision-making under partial observability requires agents to
balance reducing uncertainty (exploration) against pursuing immediate
objectives (exploitation). In this paper, we introduce a novel policy
optimization framework for continuous partially observable Markov decision
processes (POMDPs) that explicitly addresses this challenge. Our method casts
policy learning as probabilistic inference in a non-Markovian Feynman--Kac
model that inherently captures the value of information gathering by
anticipating future observations, without requiring extrinsic exploration
bonuses or handcrafted heuristics. To optimize policies under this model, we
develop a nested sequential Monte Carlo~(SMC) algorithm that efficiently
estimates a history-dependent policy gradient under samples from the optimal
trajectory distribution induced by the POMDP. We demonstrate the effectiveness
of our algorithm across standard continuous POMDP benchmarks, where existing
methods struggle to act under uncertainty.",http://arxiv.org/abs/2505.16732v1
Masked Conditioning for Deep Generative Models,"Datasets in engineering domains are often small, sparsely labeled, and
contain numerical as well as categorical conditions. Additionally.
computational resources are typically limited in practical applications which
hinders the adoption of generative models for engineering tasks. We introduce a
novel masked-conditioning approach, that enables generative models to work with
sparse, mixed-type data. We mask conditions during training to simulate sparse
conditions at inference time. For this purpose, we explore the use of various
sparsity schedules that show different strengths and weaknesses. In addition,
we introduce a flexible embedding that deals with categorical as well as
numerical conditions. We integrate our method into an efficient variational
autoencoder as well as a latent diffusion model and demonstrate the
applicability of our approach on two engineering-related datasets of 2D point
clouds and images. Finally, we show that small models trained on limited data
can be coupled with large pretrained foundation models to improve generation
quality while retaining the controllability induced by our conditioning scheme.",http://arxiv.org/abs/2505.16725v1
Advancing Brainwave Modeling with a Codebook-Based Foundation Model,"Recent advances in large-scale pre-trained Electroencephalogram (EEG) models
have shown great promise, driving progress in Brain-Computer Interfaces (BCIs)
and healthcare applications. However, despite their success, many existing
pre-trained models have struggled to fully capture the rich information content
of neural oscillations, a limitation that fundamentally constrains their
performance and generalizability across diverse BCI tasks. This limitation is
frequently rooted in suboptimal architectural design choices which constrain
their representational capacity. In this work, we introduce LaBraM++, an
enhanced Large Brainwave Foundation Model (LBM) that incorporates principled
improvements grounded in robust signal processing foundations. LaBraM++
demonstrates substantial gains across a variety of tasks, consistently
outperforming its originally-based architecture and achieving competitive
results when compared to other open-source LBMs. Its superior performance and
training efficiency highlight its potential as a strong foundation for future
advancements in LBMs.",http://arxiv.org/abs/2505.16724v1
Robust LLM Fingerprinting via Domain-Specific Watermarks,"As open-source language models (OSMs) grow more capable and are widely shared
and finetuned, ensuring model provenance, i.e., identifying the origin of a
given model instance, has become an increasingly important issue. At the same
time, existing backdoor-based model fingerprinting techniques often fall short
of achieving key requirements of real-world model ownership detection. In this
work, we build on the observation that while current open-source model
watermarks fail to achieve reliable content traceability, they can be
effectively adapted to address the challenge of model provenance. To this end,
we introduce the concept of domain-specific watermarking for model
fingerprinting. Rather than watermarking all generated content, we train the
model to embed watermarks only within specified subdomains (e.g., particular
languages or topics). This targeted approach ensures detection reliability,
while improving watermark durability and quality under a range of real-world
deployment settings. Our evaluations show that domain-specific watermarking
enables model fingerprinting with strong statistical guarantees, controllable
false positive rates, high detection power, and preserved generation quality.
Moreover, we find that our fingerprints are inherently stealthy and naturally
robust to real-world variability across deployment scenarios.",http://arxiv.org/abs/2505.16723v1
The Computational Complexity of Counting Linear Regions in ReLU Neural Networks,"An established measure of the expressive power of a given ReLU neural network
is the number of linear regions into which it partitions the input space. There
exist many different, non-equivalent definitions of what a linear region
actually is. We systematically assess which papers use which definitions and
discuss how they relate to each other. We then analyze the computational
complexity of counting the number of such regions for the various definitions.
Generally, this turns out to be an intractable problem. We prove NP- and
#P-hardness results already for networks with one hidden layer and strong
hardness of approximation results for two or more hidden layers. Finally, on
the algorithmic side, we demonstrate that counting linear regions can at least
be achieved in polynomial space for some common definitions.",http://arxiv.org/abs/2505.16716v1
Experimental robustness benchmark of quantum neural network on a superconducting quantum processor,"Quantum machine learning (QML) models, like their classical counterparts, are
vulnerable to adversarial attacks, hindering their secure deployment. Here, we
report the first systematic experimental robustness benchmark for 20-qubit
quantum neural network (QNN) classifiers executed on a superconducting
processor. Our benchmarking framework features an efficient adversarial attack
algorithm designed for QNNs, enabling quantitative characterization of
adversarial robustness and robustness bounds. From our analysis, we verify that
adversarial training reduces sensitivity to targeted perturbations by
regularizing input gradients, significantly enhancing QNN's robustness.
Additionally, our analysis reveals that QNNs exhibit superior adversarial
robustness compared to classical neural networks, an advantage attributed to
inherent quantum noise. Furthermore, the empirical upper bound extracted from
our attack experiments shows a minimal deviation ($3 \times 10^{-3}$) from the
theoretical lower bound, providing strong experimental confirmation of the
attack's effectiveness and the tightness of fidelity-based robustness bounds.
This work establishes a critical experimental framework for assessing and
improving quantum adversarial robustness, paving the way for secure and
reliable QML applications.",http://arxiv.org/abs/2505.16714v1
Sharp concentration of uniform generalization errors in binary linear classification,"We examine the concentration of uniform generalization errors around their
expectation in binary linear classification problems via an isoperimetric
argument. In particular, we establish Poincar\'{e} and log-Sobolev inequalities
for the joint distribution of the output labels and the label-weighted input
vectors, which we apply to derive concentration bounds. The derived
concentration bounds are sharp up to moderate multiplicative constants by those
under well-balanced labels. In asymptotic analysis, we also show that almost
sure convergence of uniform generalization errors to their expectation occurs
in very broad settings, such as proportionally high-dimensional regimes. Using
this convergence, we establish uniform laws of large numbers under
dimension-free conditions.",http://arxiv.org/abs/2505.16713v1
Training Long-Context LLMs Efficiently via Chunk-wise Optimization,"While long-context large language models (LLMs) exhibit remarkable document
processing capabilities, their prohibitively high training costs often hinder
customized applications. To mitigate this issue, we propose \textit{Sequential
Chunk-wise Optimization} (SeCO), a memory-efficient training paradigm that
partitions lengthy inputs into manageable chunks. Each chunk independently
constructs its computational graph and performs localized backpropagation,
ensuring that only one chunk's forward activations are stored in memory.
Building on SeCO, we further introduce \textit{Sparse Chunk-wise Optimization}
(SpaCO), which reduces computational overhead by selectively propagating
gradients to specific chunks and incorporates a carefully designed compensation
factor to ensure unbiased gradient estimation. SpaCO decouples the
computational cost of backpropagation from the context length, enabling
training time to gradually converge to inference time as sequences become
longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer
substantial practical benefits. For example, when fine-tuning an 8B model with
LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to
16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up
to 3x faster than SeCO under the same experimental setup. These innovations
provide new insights into optimizing long-context models, making them more
accessible for practical applications. We have open-sourced the code at
\href{https://github.com/wenhaoli-xmu/seco}{here}.",http://arxiv.org/abs/2505.16710v1
"An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations","Concept bottleneck models (CBMs) ensure interpretability by decomposing
predictions into human interpretable concepts. Yet the annotations used for
training CBMs that enable this transparency are often noisy, and the impact of
such corruption is not well understood. In this study, we present the first
systematic study of noise in CBMs and show that even moderate corruption
simultaneously impairs prediction performance, interpretability, and the
intervention effectiveness. Our analysis identifies a susceptible subset of
concepts whose accuracy declines far more than the average gap between noisy
and clean supervision and whose corruption accounts for most performance loss.
To mitigate this vulnerability we propose a two-stage framework. During
training, sharpness-aware minimization stabilizes the learning of
noise-sensitive concepts. During inference, where clean labels are unavailable,
we rank concepts by predictive entropy and correct only the most uncertain
ones, using uncertainty as a proxy for susceptibility. Theoretical analysis and
extensive ablations elucidate why sharpness-aware training confers robustness
and why uncertainty reliably identifies susceptible concepts, providing a
principled basis that preserves both interpretability and resilience in the
presence of noise.",http://arxiv.org/abs/2505.16705v1
Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator,"Post-training of large language models is essential for adapting pre-trained
language models (PLMs) to align with human preferences and downstream tasks.
While PLMs typically exhibit well-calibrated confidence, post-trained language
models (PoLMs) often suffer from over-confidence, assigning high confidence to
both correct and incorrect outputs, which can undermine reliability in critical
applications. A major obstacle in calibrating PoLMs is the scarcity of labeled
data for individual downstream tasks. To address this, we propose
Disagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to
optimize the parameters (e.g., temperature $\tau$) in post-hoc confidence
calibration. Our method is motivated by the under-confidence issue caused by
prediction disagreement between the PLM and PoLM while aligning their
confidence via temperature scaling. Theoretically, the PLM's confidence
underestimates PoLM's prediction accuracy on disagreement examples, causing a
larger $\tau$ and producing under-confident predictions. DACA mitigates this by
selectively using only agreement examples for calibration, effectively
decoupling the influence of disagreement. In this manner, our method avoids an
overly large $\tau$ in temperature scaling caused by disagreement examples,
improving calibration performance. Extensive experiments demonstrate the
effectiveness of our method, improving the average ECE of open-sourced and
API-based LLMs (e.g. GPT-4o) by up to 15.08$\%$ on common benchmarks.",http://arxiv.org/abs/2505.16690v1
Learning Genomic Structure from $k$-mers,"Sequencing a genome to determine an individual's DNA produces an enormous
number of short nucleotide subsequences known as reads, which must be
reassembled to reconstruct the full genome. We present a method for analyzing
this type of data using contrastive learning, in which an encoder model is
trained to produce embeddings that cluster together sequences from the same
genomic region. The sequential nature of genomic regions is preserved in the
form of trajectories through this embedding space. Trained solely to reflect
the structure of the genome, the resulting model provides a general
representation of $k$-mer sequences, suitable for a range of downstream tasks
involving read data. We apply our framework to learn the structure of the $E.\
coli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read
mapping and identification of structural variations. Furthermore, we illustrate
the potential of using this type of model for metagenomic species
identification. We show how incorporating a domain-specific noise model can
enhance embedding robustness, and how a supervised contrastive learning setting
can be adopted when a linear reference genome is available, by introducing a
distance thresholding parameter $\Gamma$. The model can also be trained fully
self-supervised on read data, enabling analysis without the need to construct a
full genome assembly using specialized algorithms. Small prediction heads based
on a pre-trained embedding are shown to perform on par with BWA-aln, the
current gold standard approach for aDNA mapping, in terms of accuracy and
runtime for short genomes. Given the method's favorable scaling properties with
respect to total genome size, inference using our approach is highly promising
for metagenomic applications and for mapping to genomes comparable in size to
the human genome.",http://arxiv.org/abs/2505.16680v1
On the Out-of-Distribution Generalization of Self-Supervised Learning,"In this paper, we focus on the out-of-distribution (OOD) generalization of
self-supervised learning (SSL). By analyzing the mini-batch construction during
the SSL training phase, we first give one plausible explanation for SSL having
OOD generalization. Then, from the perspective of data generation and causal
inference, we analyze and conclude that SSL learns spurious correlations during
the training process, which leads to a reduction in OOD generalization. To
address this issue, we propose a post-intervention distribution (PID) grounded
in the Structural Causal Model. PID offers a scenario where the spurious
variable and label variable is mutually independent. Besides, we demonstrate
that if each mini-batch during SSL training satisfies PID, the resulting SSL
model can achieve optimal worst-case OOD performance. This motivates us to
develop a batch sampling strategy that enforces PID constraints through the
learning of a latent variable model. Through theoretical analysis, we
demonstrate the identifiability of the latent variable model and validate the
effectiveness of the proposed sampling strategy. Experiments conducted on
various downstream OOD tasks demonstrate the effectiveness of the proposed
sampling strategy.",http://arxiv.org/abs/2505.16675v1
Quantum Feature Optimization for Enhanced Clustering of Blockchain Transaction Data,"Blockchain transaction data exhibits high dimensionality, noise, and
intricate feature entanglement, presenting significant challenges for
traditional clustering algorithms. In this study, we conduct a comparative
analysis of three clustering approaches: (1) Classical K-Means Clustering,
applied to pre-processed feature representations; (2) Hybrid Clustering,
wherein classical features are enhanced with quantum random features extracted
using randomly initialized quantum neural networks (QNNs); and (3) Fully
Quantum Clustering, where a QNN is trained in a self-supervised manner
leveraging a SwAV-based loss function to optimize the feature space for
clustering directly. The proposed experimental framework systematically
investigates the impact of quantum circuit depth and the number of learned
prototypes, demonstrating that even shallow quantum circuits can effectively
extract meaningful non-linear representations, significantly improving
clustering performance.",http://arxiv.org/abs/2505.16672v1
End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries,"Accurate prediction of the Remaining Useful Life (RUL) is essential for
enabling timely maintenance of lithium-ion batteries, impacting the operational
efficiency of electric applications that rely on them. This paper proposes a
RUL prediction approach that leverages data from recent charge-discharge cycles
to estimate the number of remaining usable cycles. The approach introduces both
a novel signal processing pipeline and a deep learning prediction model. In the
signal preprocessing pipeline, a derived capacity feature is computed based on
current and capacity signals. Alongside original capacity, voltage and current,
these features are denoised and enhanced using statistical metrics and a
delta-based method to capture differences between the current and previous
cycles. In the prediction model, the processed features are then fed into a
hybrid deep learning architecture composed of 1D Convolutional Neural Networks
(CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential
Equation-based LSTM (ODE-LSTM) modules. This architecture is designed to
capture both local signal characteristics and long-range temporal dependencies
while modeling the continuous-time dynamics of battery degradation. The model
is further evaluated using transfer learning across different learning
strategies and target data partitioning scenarios. Results indicate that the
model maintains robust performance, even when fine-tuned on limited target
data. Experimental results on two publicly available large-scale datasets
demonstrate that the proposed method outperforms a baseline deep learning
approach and machine learning techniques, achieving an RMSE of 101.59,
highlighting its strong potential for real-world RUL prediction applications.",http://arxiv.org/abs/2505.16664v1
Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding,"Recent advancements in multimodal large language models (MLLMs) have
significantly improved performance in visual question answering. However, they
often suffer from hallucinations. In this work, hallucinations are categorized
into two main types: initial hallucinations and snowball hallucinations. We
argue that adequate contextual information can be extracted directly from the
token interaction process. Inspired by causal inference in the decoding
strategy, we propose to leverage causal masks to establish information
propagation between multimodal tokens. The hypothesis is that insufficient
interaction between those tokens may lead the model to rely on outlier tokens,
overlooking dense and rich contextual cues. Therefore, we propose to intervene
in the propagation process by tackling outlier tokens to enhance in-context
inference. With this goal, we present FarSight, a versatile plug-and-play
decoding strategy to reduce attention interference from outlier tokens merely
by optimizing the causal mask. The heart of our method is effective token
propagation. We design an attention register structure within the upper
triangular matrix of the causal mask, dynamically allocating attention to
capture attention diverted to outlier tokens. Moreover, a positional awareness
encoding method with a diminishing masking rate is proposed, allowing the model
to attend to further preceding tokens, especially for video sequence tasks.
With extensive experiments, FarSight demonstrates significant
hallucination-mitigating performance across different MLLMs on both image and
video benchmarks, proving its effectiveness.",http://arxiv.org/abs/2505.16652v1
Stochastic Forward-Forward Learning through Representational Dimensionality Compression,"The Forward-Forward (FF) algorithm provides a bottom-up alternative to
backpropagation (BP) for training neural networks, relying on a layer-wise
""goodness"" function to guide learning. Existing goodness functions, inspired by
energy-based learning (EBL), are typically defined as the sum of squared
post-synaptic activations, neglecting the correlations between neurons. In this
work, we propose a novel goodness function termed dimensionality compression
that uses the effective dimensionality (ED) of fluctuating neural responses to
incorporate second-order statistical structure. Our objective minimizes ED for
clamped inputs when noise is considered while maximizing it across the sample
distribution, promoting structured representations without the need to prepare
negative samples. We demonstrate that this formulation achieves competitive
performance compared to other non-BP methods. Moreover, we show that noise
plays a constructive role that can enhance generalization and improve inference
when predictions are derived from the mean of squared outputs, which is
equivalent to making predictions based on the energy term. Our findings
contribute to the development of more biologically plausible learning
algorithms and suggest a natural fit for neuromorphic computing, where
stochasticity is a computational resource rather than a nuisance. The code is
available at https://github.com/ZhichaoZhu/StochasticForwardForward",http://arxiv.org/abs/2505.16649v1
Learning non-equilibrium diffusions with Schrödinger bridges: from exactly solvable to simulation-free,"We consider the Schr\""odinger bridge problem which, given ensemble
measurements of the initial and final configurations of a stochastic dynamical
system and some prior knowledge on the dynamics, aims to reconstruct the ""most
likely"" evolution of the system compatible with the data. Most existing
literature assume Brownian reference dynamics and are implicitly limited to
potential-driven dynamics. We depart from this regime and consider reference
processes described by a multivariate Ornstein-Uhlenbeck process with generic
drift matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$. When $\mathbf{A}$ is
asymmetric, this corresponds to a non-equilibrium system with non-conservative
forces at play: this is important for applications to biological systems, which
are naturally exist out-of-equilibrium. In the case of Gaussian marginals, we
derive explicit expressions that characterise the solution of both the static
and dynamic Schr\""odinger bridge. For general marginals, we propose mvOU-OTFM,
a simulation-free algorithm based on flow and score matching for learning the
Schr\""odinger bridge. In application to a range of problems based on synthetic
and real single cell data, we demonstrate that mvOU-OTFM achieves higher
accuracy compared to competing methods, whilst being significantly faster to
train.",http://arxiv.org/abs/2505.16644v1
Reconsidering Fairness Through Unawareness from the Perspective of Model Multiplicity,"Fairness through Unawareness (FtU) describes the idea that discrimination
against demographic groups can be avoided by not considering group membership
in the decisions or predictions. This idea has long been criticized in the
machine learning literature as not being sufficient to ensure fairness. In
addition, the use of additional features is typically thought to increase the
accuracy of the predictions for all groups, so that FtU is sometimes thought to
be detrimental to all groups. In this paper, we show both theoretically and
empirically that FtU can reduce algorithmic discrimination without necessarily
reducing accuracy. We connect this insight with the literature on Model
Multiplicity, to which we contribute with novel theoretical and empirical
results. Furthermore, we illustrate how, in a real-life application, FtU can
contribute to the deployment of more equitable policies without losing
efficacy. Our findings suggest that FtU is worth considering in practical
applications, particularly in high-risk scenarios, and that the use of
protected attributes such as gender in predictive models should be accompanied
by a clear and well-founded justification.",http://arxiv.org/abs/2505.16638v1
SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation,"Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.",http://arxiv.org/abs/2505.16637v1
Multivariate Latent Recalibration for Conditional Normalizing Flows,"Reliably characterizing the full conditional distribution of a multivariate
response variable given a set of covariates is crucial for trustworthy
decision-making. However, misspecified or miscalibrated multivariate models may
yield a poor approximation of the joint distribution of the response variables,
leading to unreliable predictions and suboptimal decisions. Furthermore,
standard recalibration methods are primarily limited to univariate settings,
while conformal prediction techniques, despite generating multivariate
prediction regions with coverage guarantees, do not provide a full probability
density function. We address this gap by first introducing a novel notion of
latent calibration, which assesses probabilistic calibration in the latent
space of a conditional normalizing flow. Second, we propose latent
recalibration (LR), a novel post-hoc model recalibration method that learns a
transformation of the latent space with finite-sample bounds on latent
calibration. Unlike existing methods, LR produces a recalibrated distribution
with an explicit multivariate density function while remaining computationally
efficient. Extensive experiments on both tabular and image datasets show that
LR consistently improves latent calibration error and the negative
log-likelihood of the recalibrated models.",http://arxiv.org/abs/2505.16636v1
WikiDBGraph: Large-Scale Database Graph of Wikidata for Collaborative Learning,"Tabular data, ubiquitous and rich in informational value, is an increasing
focus for deep representation learning, yet progress is hindered by studies
centered on single tables or isolated databases, which limits model
capabilities due to data scale. While collaborative learning approaches such as
federated learning, transfer learning, split learning, and tabular foundation
models aim to learn from multiple correlated databases, they are challenged by
a scarcity of real-world interconnected tabular resources. Current data lakes
and corpora largely consist of isolated databases lacking defined
inter-database correlations. To overcome this, we introduce WikiDBGraph, a
large-scale graph of 100,000 real-world tabular databases from WikiData,
interconnected by 17 million edges and characterized by 13 node and 12 edge
properties derived from its database schema and data distribution.
WikiDBGraph's weighted edges identify both instance- and feature-overlapped
databases. Experiments on these newly identified databases confirm that
collaborative learning yields superior performance, thereby offering
considerable promise for structured foundation model training while also
exposing key challenges and future directions for learning from interconnected
tabular data.",http://arxiv.org/abs/2505.16635v1
CausalDynamics: A large-scale benchmark for structural discovery of dynamical causal models,"Causal discovery for dynamical systems poses a major challenge in fields
where active interventions are infeasible. Most methods used to investigate
these systems and their associated benchmarks are tailored to deterministic,
low-dimensional and weakly nonlinear time-series data. To address these
limitations, we present CausalDynamics, a large-scale benchmark and extensible
data generation framework to advance the structural discovery of dynamical
causal models. Our benchmark consists of true causal graphs derived from
thousands of coupled ordinary and stochastic differential equations as well as
two idealized climate models. We perform a comprehensive evaluation of
state-of-the-art causal discovery algorithms for graph reconstruction on
systems with noisy, confounded, and lagged dynamics. CausalDynamics consists of
a plug-and-play, build-your-own coupling workflow that enables the construction
of a hierarchy of physical systems. We anticipate that our framework will
facilitate the development of robust causal discovery algorithms that are
broadly applicable across domains while addressing their unique challenges. We
provide a user-friendly implementation and documentation on
https://kausable.github.io/CausalDynamics.",http://arxiv.org/abs/2505.16620v1
Steering Large Language Models for Machine Translation Personalization,"High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from sparse autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.",http://arxiv.org/abs/2505.16612v1
Temporal Object Captioning for Street Scene Videos from LiDAR Tracks,"Video captioning models have seen notable advancements in recent years,
especially with regard to their ability to capture temporal information. While
many research efforts have focused on architectural advancements, such as
temporal attention mechanisms, there remains a notable gap in understanding how
models capture and utilize temporal semantics for effective temporal feature
extraction, especially in the context of Advanced Driver Assistance Systems. We
propose an automated LiDAR-based captioning procedure that focuses on the
temporal dynamics of traffic participants. Our approach uses a rule-based
system to extract essential details such as lane position and relative motion
from object tracks, followed by a template-based caption generation. Our
findings show that training SwinBERT, a video captioning model, using only
front camera images and supervised with our template-based captions,
specifically designed to encapsulate fine-grained temporal behavior, leads to
improved temporal understanding consistently across three datasets. In
conclusion, our results clearly demonstrate that integrating LiDAR-based
caption supervision significantly enhances temporal understanding, effectively
addressing and reducing the inherent visual/static biases prevalent in current
state-of-the-art model architectures.",http://arxiv.org/abs/2505.16594v1
Training on Plausible Counterfactuals Removes Spurious Correlations,"Plausible counterfactual explanations (p-CFEs) are perturbations that
minimally modify inputs to change classifier decisions while remaining
plausible under the data distribution. In this study, we demonstrate that
classifiers can be trained on p-CFEs labeled with induced \emph{incorrect}
target classes to classify unperturbed inputs with the original labels. While
previous studies have shown that such learning is possible with adversarial
perturbations, we extend this paradigm to p-CFEs. Interestingly, our
experiments reveal that learning from p-CFEs is even more effective: the
resulting classifiers achieve not only high in-distribution accuracy but also
exhibit significantly reduced bias with respect to spurious correlations.",http://arxiv.org/abs/2505.16583v1
How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning,"In the zero-shot policy transfer setting in reinforcement learning, the goal
is to train an agent on a fixed set of training environments so that it can
generalise to similar, but unseen, testing environments. Previous work has
shown that policy distillation after training can sometimes produce a policy
that outperforms the original in the testing environments. However, it is not
yet entirely clear why that is, or what data should be used to distil the
policy. In this paper, we prove, under certain assumptions, a generalisation
bound for policy distillation after training. The theory provides two practical
insights: for improved generalisation, you should 1) train an ensemble of
distilled policies, and 2) distil it on as much data from the training
environments as possible. We empirically verify that these insights hold in
more general settings, when the assumptions required for the theory no longer
hold. Finally, we demonstrate that an ensemble of policies distilled on a
diverse dataset can generalise significantly better than the original agent.",http://arxiv.org/abs/2505.16581v1
Large Language Model-Empowered Interactive Load Forecasting,"The growing complexity of power systems has made accurate load forecasting
more important than ever. An increasing number of advanced load forecasting
methods have been developed. However, the static design of current methods
offers no mechanism for human-model interaction. As the primary users of
forecasting models, system operators often find it difficult to understand and
apply these advanced models, which typically requires expertise in artificial
intelligence (AI). This also prevents them from incorporating their experience
and real-world contextual understanding into the forecasting process. Recent
breakthroughs in large language models (LLMs) offer a new opportunity to
address this issue. By leveraging their natural language understanding and
reasoning capabilities, we propose an LLM-based multi-agent collaboration
framework to bridge the gap between human operators and forecasting models. A
set of specialized agents is designed to perform different tasks in the
forecasting workflow and collaborate via a dedicated communication mechanism.
This framework embeds interactive mechanisms throughout the load forecasting
pipeline, reducing the technical threshold for non-expert users and enabling
the integration of human experience. Our experiments demonstrate that the
interactive load forecasting accuracy can be significantly improved when users
provide proper insight in key stages. Our cost analysis shows that the
framework remains affordable, making it practical for real-world deployment.",http://arxiv.org/abs/2505.16577v1
Finetuning-Activated Backdoors in LLMs,"Finetuning openly accessible Large Language Models (LLMs) has become standard
practice for achieving task-specific performance improvements. Until now,
finetuning has been regarded as a controlled and secure process in which
training on benign datasets led to predictable behaviors. In this paper, we
demonstrate for the first time that an adversary can create poisoned LLMs that
initially appear benign but exhibit malicious behaviors once finetuned by
downstream users. To this end, our proposed attack, FAB (Finetuning-Activated
Backdoor), poisons an LLM via meta-learning techniques to simulate downstream
finetuning, explicitly optimizing for the emergence of malicious behaviors in
the finetuned models. At the same time, the poisoned LLM is regularized to
retain general capabilities and to exhibit no malicious behaviors prior to
finetuning. As a result, when users finetune the seemingly benign model on
their own datasets, they unknowingly trigger its hidden backdoor behavior. We
demonstrate the effectiveness of FAB across multiple LLMs and three target
behaviors: unsolicited advertising, refusal, and jailbreakability.
Additionally, we show that FAB-backdoors are robust to various finetuning
choices made by the user (e.g., dataset, number of steps, scheduler). Our
findings challenge prevailing assumptions about the security of finetuning,
revealing yet another critical attack vector exploiting the complexities of
LLMs.",http://arxiv.org/abs/2505.16567v1
A Two-Stage Data Selection Framework for Data-Efficient Model Training on Edge Devices,"The demand for machine learning (ML) model training on edge devices is
escalating due to data privacy and personalized service needs. However, we
observe that current on-device model training is hampered by the
under-utilization of on-device data, due to low training throughput, limited
storage and diverse data importance. To improve data resource utilization, we
propose a two-stage data selection framework {\sf Titan} to select the most
important data batch from streaming data for model training with guaranteed
efficiency and effectiveness. Specifically, in the first stage, {\sf Titan}
filters out a candidate dataset with potentially high importance in a
coarse-grained manner.In the second stage of fine-grained selection, we propose
a theoretically optimal data selection strategy to identify the data batch with
the highest model performance improvement to current training round. To further
enhance time-and-resource efficiency, {\sf Titan} leverages a pipeline to
co-execute data selection and model training, and avoids resource conflicts by
exploiting idle computing resources. We evaluate {\sf Titan} on real-world edge
devices and three representative edge computing tasks with diverse models and
data modalities. Empirical results demonstrate that {\sf Titan} achieves up to
$43\%$ reduction in training time and $6.2\%$ increase in final accuracy with
minor system overhead, such as data processing delay, memory footprint and
energy consumption.",http://arxiv.org/abs/2505.16563v1
Towards Coordinate- and Dimension-Agnostic Machine Learning for Partial Differential Equations,"The machine learning methods for data-driven identification of partial
differential equations (PDEs) are typically defined for a given number of
spatial dimensions and a choice of coordinates the data have been collected in.
This dependence prevents the learned evolution equation from generalizing to
other spaces. In this work, we reformulate the problem in terms of coordinate-
and dimension-independent representations, paving the way toward what we call
``spatially liberated"" PDE learning. To this end, we employ a machine learning
approach to predict the evolution of scalar field systems expressed in the
formalism of exterior calculus, which is coordinate-free and immediately
generalizes to arbitrary dimensions by construction. We demonstrate the
performance of this approach in the FitzHugh-Nagumo and Barkley
reaction-diffusion models, as well as the Patlak-Keller-Segel model informed by
in-situ chemotactic bacteria observations. We provide extensive numerical
experiments that demonstrate that our approach allows for seamless transitions
across various spatial contexts. We show that the field dynamics learned in one
space can be used to make accurate predictions in other spaces with different
dimensions, coordinate systems, boundary conditions, and curvatures.",http://arxiv.org/abs/2505.16549v1
Incremental Sequence Classification with Temporal Consistency,"We address the problem of incremental sequence classification, where
predictions are updated as new elements in the sequence are revealed. Drawing
on temporal-difference learning from reinforcement learning, we identify a
temporal-consistency condition that successive predictions should satisfy. We
leverage this condition to develop a novel loss function for training
incremental sequence classifiers. Through a concrete example, we demonstrate
that optimizing this loss can offer substantial gains in data efficiency. We
apply our method to text classification tasks and show that it improves
predictive accuracy over competing approaches on several benchmark datasets. We
further evaluate our approach on the task of verifying large language model
generations for correctness in grade-school math problems. Our results show
that models trained with our method are better able to distinguish promising
generations from unpromising ones after observing only a few tokens.",http://arxiv.org/abs/2505.16548v1
HOFT: Householder Orthogonal Fine-tuning,"Adaptation of foundation models using low-rank methods is a widespread
approach. Another way to adapt these models is to employ orthogonal fine-tuning
methods, which are less time and memory efficient despite their good
generalization properties. In this work, we propose Householder Orthogonal
Fine-tuning (HOFT), a novel orthogonal fine-tuning method that aims to
alleviate time and space complexity. Moreover, some theoretical properties of
the orthogonal fine-tuning paradigm are explored. From this exploration, Scaled
Householder Orthogonal Fine-tuning (SHOFT) is proposed. Both HOFT and SHOFT are
evaluated in downstream tasks, namely commonsense reasoning, machine
translation, subject-driven generation and mathematical reasoning. Compared
with state-of-the-art adaptation methods, HOFT and SHOFT show comparable or
better results.",http://arxiv.org/abs/2505.16531v1
Joint Relational Database Generation via Graph-Conditional Diffusion Models,"Building generative models for relational databases (RDBs) is important for
applications like privacy-preserving data release and augmenting real datasets.
However, most prior work either focuses on single-table generation or relies on
autoregressive factorizations that impose a fixed table order and generate
tables sequentially. This approach limits parallelism, restricts flexibility in
downstream applications like missing value imputation, and compounds errors due
to commonly made conditional independence assumptions. We propose a
fundamentally different approach: jointly modeling all tables in an RDB without
imposing any order. By using a natural graph representation of RDBs, we propose
the Graph-Conditional Relational Diffusion Model (GRDM). GRDM leverages a graph
neural network to jointly denoise row attributes and capture complex
inter-table dependencies. Extensive experiments on six real-world RDBs
demonstrate that our approach substantially outperforms autoregressive
baselines in modeling multi-hop inter-table correlations and achieves
state-of-the-art performance on single-table fidelity metrics.",http://arxiv.org/abs/2505.16527v1
CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving,"Maintaining robust 3D perception under dynamic and unpredictable test-time
conditions remains a critical challenge for autonomous driving systems.
Existing test-time adaptation (TTA) methods often fail in high-variance tasks
like 3D object detection due to unstable optimization and sharp minima. While
recent model merging strategies based on linear mode connectivity (LMC) offer
improved stability by interpolating between fine-tuned checkpoints, they are
computationally expensive, requiring repeated checkpoint access and multiple
forward passes. In this paper, we introduce CodeMerge, a lightweight and
scalable model merging framework that bypasses these limitations by operating
in a compact latent space. Instead of loading full models, CodeMerge represents
each checkpoint with a low-dimensional fingerprint derived from the source
model's penultimate features and constructs a key-value codebook. We compute
merging coefficients using ridge leverage scores on these fingerprints,
enabling efficient model composition without compromising adaptation quality.
Our method achieves strong performance across challenging benchmarks, improving
end-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by
over 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as
online mapping, motion prediction and planning even without training. Code and
pretrained models are released in the supplementary material.",http://arxiv.org/abs/2505.16524v1
Computing Exact Shapley Values in Polynomial Time for Product-Kernel Methods,"Kernel methods are widely used in machine learning due to their flexibility
and expressive power. However, their black-box nature poses significant
challenges to interpretability, limiting their adoption in high-stakes
applications. Shapley value-based feature attribution techniques, such as SHAP
and kernel-specific variants like RKHS-SHAP, offer a promising path toward
explainability. Yet, computing exact Shapley values remains computationally
intractable in general, motivating the development of various approximation
schemes. In this work, we introduce PKeX-Shapley, a novel algorithm that
utilizes the multiplicative structure of product kernels to enable the exact
computation of Shapley values in polynomial time. We show that product-kernel
models admit a functional decomposition that allows for a recursive formulation
of Shapley values. This decomposition not only yields computational efficiency
but also enhances interpretability in kernel-based learning. We also
demonstrate how our framework can be generalized to explain kernel-based
statistical discrepancies such as the Maximum Mean Discrepancy (MMD) and the
Hilbert-Schmidt Independence Criterion (HSIC), thus offering new tools for
interpretable statistical inference.",http://arxiv.org/abs/2505.16516v1
Accuracy vs. Accuracy: Computational Tradeoffs Between Classification Rates and Utility,"We revisit the foundations of fairness and its interplay with utility and
efficiency in settings where the training data contain richer labels, such as
individual types, rankings, or risk estimates, rather than just binary
outcomes. In this context, we propose algorithms that achieve stronger notions
of evidence-based fairness than are possible in standard supervised learning.
Our methods support classification and ranking techniques that preserve
accurate subpopulation classification rates, as suggested by the underlying
data distributions, across a broad class of classification rules and downstream
applications. Furthermore, our predictors enable loss minimization, whether
aimed at maximizing utility or in the service of fair treatment.
  Complementing our algorithmic contributions, we present impossibility results
demonstrating that simultaneously achieving accurate classification rates and
optimal loss minimization is, in some cases, computationally infeasible. Unlike
prior impossibility results, our notions are not inherently in conflict and are
simultaneously satisfied by the Bayes-optimal predictor. Furthermore, we show
that each notion can be satisfied individually via efficient learning. Our
separation thus stems from the computational hardness of learning a
sufficiently good approximation of the Bayes-optimal predictor. These
computational impossibilities present a choice between two natural and
attainable notions of accuracy that could both be motivated by fairness.",http://arxiv.org/abs/2505.16494v1
Constrained Non-negative Matrix Factorization for Guided Topic Modeling of Minority Topics,"Topic models often fail to capture low-prevalence, domain-critical themes,
so-called minority topics, such as mental health themes in online comments.
While some existing methods can incorporate domain knowledge, such as expected
topical content, methods allowing guidance may require overly detailed expected
topics, hindering the discovery of topic divisions and variation. We propose a
topic modeling solution via a specially constrained NMF. We incorporate a seed
word list characterizing minority content of interest, but we do not require
experts to pre-specify their division across minority topics. Through
prevalence constraints on minority topics and seed word content across topics,
we learn distinct data-driven minority topics as well as majority topics. The
constrained NMF is fitted via Karush-Kuhn-Tucker (KKT) conditions with
multiplicative updates. We outperform several baselines on synthetic data in
terms of topic purity, normalized mutual information, and also evaluate topic
quality using Jensen-Shannon divergence (JSD). We conduct a case study on
YouTube vlog comments, analyzing viewer discussion of mental health content;
our model successfully identifies and reveals this domain-relevant minority
content.",http://arxiv.org/abs/2505.16493v1
Neighbour-Driven Gaussian Process Variational Autoencoders for Scalable Structured Latent Modelling,"Gaussian Process (GP) Variational Autoencoders (VAEs) extend standard VAEs by
replacing the fully factorised Gaussian prior with a GP prior, thereby
capturing richer correlations among latent variables. However, performing exact
GP inference in large-scale GPVAEs is computationally prohibitive, often
forcing existing approaches to rely on restrictive kernel assumptions or large
sets of inducing points. In this work, we propose a neighbour-driven
approximation strategy that exploits local adjacencies in the latent space to
achieve scalable GPVAE inference. By confining computations to the nearest
neighbours of each data point, our method preserves essential latent
dependencies, allowing more flexible kernel choices and mitigating the need for
numerous inducing points. Through extensive experiments on tasks including
representation learning, data imputation, and conditional generation, we
demonstrate that our approach outperforms other GPVAE variants in both
predictive performance and computational efficiency.",http://arxiv.org/abs/2505.16481v1
Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization,"Deep reinforcement learning (DRL) has been widely used for dynamic algorithm
configuration, particularly in evolutionary computation, which benefits from
the adaptive update of parameters during the algorithmic execution. However,
applying DRL to algorithm configuration for multi-objective combinatorial
optimization (MOCO) problems remains relatively unexplored. This paper presents
a novel graph neural network (GNN) based DRL to configure multi-objective
evolutionary algorithms. We model the dynamic algorithm configuration as a
Markov decision process, representing the convergence of solutions in the
objective space by a graph, with their embeddings learned by a GNN to enhance
the state representation. Experiments on diverse MOCO challenges indicate that
our method outperforms traditional and DRL-based algorithm configuration
methods in terms of efficacy and adaptability. It also exhibits advantageous
generalizability across objective types and problem sizes, and applicability to
different evolutionary computation methods.",http://arxiv.org/abs/2505.16471v1
AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer,"Recently, vision transformers (ViTs) have achieved excellent performance on
vision tasks by measuring the global self-attention among the image patches.
Given $n$ patches, they will have quadratic complexity such as
$\mathcal{O}(n^2)$ and the time cost is high when splitting the input image
with a small granularity. Meanwhile, the pivotal information is often randomly
gathered in a few regions of an input image, some tokens may not be helpful for
the downstream tasks. To handle this problem, we introduce an anchor-based
efficient vision transformer (AnchorFormer), which employs the anchor tokens to
learn the pivotal information and accelerate the inference. Firstly, by
estimating the bipartite attention between the anchors and tokens, the
complexity will be reduced from $\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$, where
$m$ is an anchor number and $m < n$. Notably, by representing the anchors with
the neurons in a neural layer, we can differentiable learn these distributions
and approximate global self-attention through the Markov process. Moreover, we
extend the proposed model to three downstream tasks including classification,
detection, and segmentation. Extensive experiments show the effectiveness of
our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs
reduction on ImageNet classification, 81.3% higher mAP on COCO detection under
comparable FLOPs, as compared to the current baselines.",http://arxiv.org/abs/2505.16463v1
CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI,"Accurate and efficient quantification of cardiac function is essential for
the estimation of prognosis of cardiovascular diseases (CVDs). One of the most
commonly used metrics for evaluating cardiac pumping performance is left
ventricular ejection fraction (LVEF). However, LVEF can be affected by factors
such as inter-observer variability and varying pre-load and after-load
conditions, which can reduce its reproducibility. Additionally, cardiac
dysfunction may not always manifest as alterations in LVEF, such as in heart
failure and cardiotoxicity diseases. An alternative measure that can provide a
relatively load-independent quantitative assessment of myocardial contractility
is myocardial strain and strain rate. By using LVEF in combination with
myocardial strain, it is possible to obtain a thorough description of cardiac
function. Automated estimation of LVEF and other volumetric measures from
cine-MRI sequences can be achieved through segmentation models, while strain
calculation requires the estimation of tissue displacement between sequential
frames, which can be accomplished using registration models. These tasks are
often performed separately, potentially limiting the assessment of cardiac
function. To address this issue, in this study we propose an end-to-end deep
learning (DL) model that jointly estimates groupwise (GW) registration and
segmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep
GW network was trained and validated on a large dataset of 4-chamber view
cine-MRI image series of 374 subjects. A quantitative comparison with
conventional GW registration using elastix and two DL-based methods showed that
the proposed model improved performance and substantially reduced computation
time.",http://arxiv.org/abs/2505.16452v1
Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models,"Multimodal large language models (MLLMs) enable powerful cross-modal
reasoning capabilities. However, the expanded input space introduces new attack
surfaces. Previous jailbreak attacks often inject malicious instructions from
text into less aligned modalities, such as vision. As MLLMs increasingly
incorporate cross-modal consistency and alignment mechanisms, such explicit
attacks become easier to detect and block. In this work, we propose a novel
implicit jailbreak framework termed IJA that stealthily embeds malicious
instructions into images via least significant bit steganography and couples
them with seemingly benign, image-related textual prompts. To further enhance
attack effectiveness across diverse MLLMs, we incorporate adversarial suffixes
generated by a surrogate model and introduce a template optimization module
that iteratively refines both the prompt and embedding based on model feedback.
On commercial models like GPT-4o and Gemini-1.5 Pro, our method achieves attack
success rates of over 90% using an average of only 3 queries.",http://arxiv.org/abs/2505.16446v1
Ranked Entropy Minimization for Continual Test-Time Adaptation,"Test-time adaptation aims to adapt to realistic environments in an online
manner by learning during test time. Entropy minimization has emerged as a
principal strategy for test-time adaptation due to its efficiency and
adaptability. Nevertheless, it remains underexplored in continual test-time
adaptation, where stability is more important. We observe that the entropy
minimization method often suffers from model collapse, where the model
converges to predicting a single class for all images due to a trivial
solution. We propose ranked entropy minimization to mitigate the stability
problem of the entropy minimization method and extend its applicability to
continuous scenarios. Our approach explicitly structures the prediction
difficulty through a progressive masking strategy. Specifically, it gradually
aligns the model's probability distributions across different levels of
prediction difficulty while preserving the rank order of entropy. The proposed
method is extensively evaluated across various benchmarks, demonstrating its
effectiveness through empirical results. Our code is available at
https://github.com/pilsHan/rem",http://arxiv.org/abs/2505.16441v1
WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning,"While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.",http://arxiv.org/abs/2505.16421v1
Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)
combined with external contexts to enhance the accuracy and reliability of
generated responses. However, reliably attributing generated content to
specific context segments, context attribution, remains challenging due to the
computationally intensive nature of current methods, which often require
extensive fine-tuning or human annotation. In this work, we introduce a novel
Jensen-Shannon Divergence driven method to Attribute Response to Context
(ARC-JSD), enabling efficient and accurate identification of essential context
sentences without additional fine-tuning or surrogate modelling. Evaluations on
a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using
instruction-tuned LLMs in different scales demonstrate superior accuracy and
significant computational efficiency improvements compared to the previous
surrogate-based method. Furthermore, our mechanistic analysis reveals specific
attention heads and multilayer perceptron (MLP) layers responsible for context
attribution, providing valuable insights into the internal workings of RAG
models.",http://arxiv.org/abs/2505.16415v1
Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression,"Despite their remarkable progress in multimodal understanding tasks, large
vision language models (LVLMs) often suffer from ""hallucinations"", generating
texts misaligned with the visual context. Existing methods aimed at reducing
hallucinations through inference time intervention incur a significant increase
in latency. To mitigate this, we present SPIN, a task-agnostic attention-guided
head suppression strategy that can be seamlessly integrated during inference,
without incurring any significant compute or latency overhead. We investigate
whether hallucination in LVLMs can be linked to specific model components. Our
analysis suggests that hallucinations can be attributed to a dynamic subset of
attention heads in each layer. Leveraging this insight, for each text query
token, we selectively suppress attention heads that exhibit low attention to
image tokens, keeping the top-K attention heads intact. Extensive evaluations
on visual question answering and image description tasks demonstrate the
efficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining
F1, and improving throughput by 1.8x compared to existing alternatives. Code is
available at https://github.com/YUECHE77/SPIN.",http://arxiv.org/abs/2505.16411v1
Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning,"Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.",http://arxiv.org/abs/2505.16410v1
Performance Guaranteed Poisoning Attacks in Federated Learning: A Sliding Mode Approach,"Manipulation of local training data and local updates, i.e., the poisoning
attack, is the main threat arising from the collaborative nature of the
federated learning (FL) paradigm. Most existing poisoning attacks aim to
manipulate local data/models in a way that causes denial-of-service (DoS)
issues. In this paper, we introduce a novel attack method, named Federated
Learning Sliding Attack (FedSA) scheme, aiming at precisely introducing the
extent of poisoning in a subtle controlled manner. It operates with a
predefined objective, such as reducing global model's prediction accuracy by
10\%. FedSA integrates robust nonlinear control-Sliding Mode Control (SMC)
theory with model poisoning attacks. It can manipulate the updates from
malicious clients to drive the global model towards a compromised state,
achieving this at a controlled and inconspicuous rate. Additionally, leveraging
the robust control properties of FedSA allows precise control over the
convergence bounds, enabling the attacker to set the global accuracy of the
poisoned model to any desired level. Experimental results demonstrate that
FedSA can accurately achieve a predefined global accuracy with fewer malicious
clients while maintaining a high level of stealth and adjustable learning
rates.",http://arxiv.org/abs/2505.16403v1
"Divide-Fuse-Conquer: Eliciting ""Aha Moments"" in Multi-Scenario Games","Large language models (LLMs) have been observed to suddenly exhibit advanced
reasoning abilities during reinforcement learning (RL), resembling an ``aha
moment'' triggered by simple outcome-based rewards. While RL has proven
effective in eliciting such breakthroughs in tasks involving mathematics,
coding, and vision, it faces significant challenges in multi-scenario games.
The diversity of game rules, interaction modes, and environmental complexities
often leads to policies that perform well in one scenario but fail to
generalize to others. Simply combining multiple scenarios during training
introduces additional challenges, such as training instability and poor
performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a
framework designed to enhance generalization in multi-scenario RL. This
approach starts by heuristically grouping games based on characteristics such
as rules and difficulties. Specialized models are then trained for each group
to excel at games in the group is what we refer to as the divide step. Next, we
fuse model parameters from different groups as a new model, and continue
training it for multiple groups, until the scenarios in all groups are
conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align
trained with the Divide-Fuse-Conquer strategy reaches a performance level
comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can
inspire future research on using reinforcement learning to improve the
generalization of LLMs.",http://arxiv.org/abs/2505.16401v1
AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning,"Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.",http://arxiv.org/abs/2505.16400v1
Omni TM-AE: A Scalable and Interpretable Embedding Model Using the Full Tsetlin Machine State Space,"The increasing complexity of large-scale language models has amplified
concerns regarding their interpretability and reusability. While traditional
embedding models like Word2Vec and GloVe offer scalability, they lack
transparency and often behave as black boxes. Conversely, interpretable models
such as the Tsetlin Machine (TM) have shown promise in constructing explainable
learning systems, though they previously faced limitations in scalability and
reusability. In this paper, we introduce Omni Tsetlin Machine AutoEncoder (Omni
TM-AE), a novel embedding model that fully exploits the information contained
in the TM's state matrix, including literals previously excluded from clause
formation. This method enables the construction of reusable, interpretable
embeddings through a single training phase. Extensive experiments across
semantic similarity, sentiment classification, and document clustering tasks
show that Omni TM-AE performs competitively with and often surpasses mainstream
embedding models. These results demonstrate that it is possible to balance
performance, scalability, and interpretability in modern Natural Language
Processing (NLP) systems without resorting to opaque architectures.",http://arxiv.org/abs/2505.16386v1
PaTH Attention: Position Encoding via Accumulating Householder Transformations,"The attention mechanism is a core primitive in modern large language models
(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,
position encoding is essential for modeling structured domains such as
language. Rotary position encoding (RoPE) has emerged as the de facto standard
approach for position encoding and is part of many modern LLMs. However, in
RoPE the key/query transformation between two elements in a sequence is only a
function of their relative position and otherwise independent of the actual
input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme
based on accumulated products of Householder(like) transformations, where each
transformation is data-dependent, i.e., a function of the input. We derive an
efficient parallel algorithm for training through exploiting a compact
representation of products of Householder matrices, and implement a
FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both
targeted synthetic benchmarks and moderate-scale real-world language modeling
experiments, we find that PaTH demonstrates superior performance compared to
RoPE and other recent baselines.",http://arxiv.org/abs/2505.16381v1
SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning,"How to design reinforcement learning (RL) tasks that effectively unleash the
reasoning capability of large language models (LLMs) remains an open question.
Existing RL tasks (e.g., math, programming, and constructing reasoning tasks)
suffer from three key limitations: (1) Scalability. They rely heavily on human
annotation or expensive LLM synthesis to generate sufficient training data. (2)
Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3)
Controllable Difficulty. Most tasks lack fine-grained difficulty control,
making it hard to train LLMs to develop reasoning ability from easy to hard.
  To address these limitations, we propose Saturn, a SAT-based RL framework
that uses Boolean Satisfiability (SAT) problems to train and evaluate LLM
reasoning. Saturn enables scalable task construction, rule-based verification,
and precise difficulty control. Saturn designs a curriculum learning pipeline
that continuously improves LLMs' reasoning capability by constructing SAT tasks
of increasing difficulty and training LLMs from easy to hard. To ensure stable
training, we design a principled mechanism to control difficulty transitions.
  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying
difficulty. It supports the evaluation of how LLM reasoning changes with
problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain
Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT
problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of
+14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B
and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g.,
AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in
constructing RL tasks, Saturn achieves further improvements of +8.8%. We
release the source code, data, and models to support future research.",http://arxiv.org/abs/2505.16368v1
A collaborative constrained graph diffusion model for the generation of realistic synthetic molecules,"Developing new molecular compounds is crucial to address pressing challenges,
from health to environmental sustainability. However, exploring the molecular
space to discover new molecules is difficult due to the vastness of the space.
Here we introduce CoCoGraph, a collaborative and constrained graph diffusion
model capable of generating molecules that are guaranteed to be chemically
valid. Thanks to the constraints built into the model and to the collaborative
mechanism, CoCoGraph outperforms state-of-the-art approaches on standard
benchmarks while requiring up to an order of magnitude fewer parameters.
Analysis of 36 chemical properties also demonstrates that CoCoGraph generates
molecules with distributions more closely matching real molecules than current
models. Leveraging the model's efficiency, we created a database of 8.2M
million synthetically generated molecules and conducted a Turing-like test with
organic chemistry experts to further assess the plausibility of the generated
molecules, and potential biases and limitations of CoCoGraph.",http://arxiv.org/abs/2505.16365v1
AdamS: Momentum Itself Can Be A Normalizer for LLM Pretraining and Post-training,"We introduce AdamS, a simple yet effective alternative to Adam for large
language model (LLM) pretraining and post-training. By leveraging a novel
denominator, i.e., the root of weighted sum of squares of the momentum and the
current gradient, AdamS eliminates the need for second-moment estimates. Hence,
AdamS is efficient, matching the memory and compute footprint of SGD with
momentum while delivering superior optimization performance. Moreover, AdamS is
easy to adopt: it can directly inherit hyperparameters of AdamW, and is
entirely model-agnostic, integrating seamlessly into existing pipelines without
modifications to optimizer APIs or architectures. The motivation behind AdamS
stems from the observed $(L_0, L_1)$ smoothness properties in transformer
objectives, where local smoothness is governed by gradient magnitudes that can
be further approximated by momentum magnitudes. We establish rigorous
theoretical convergence guarantees and provide practical guidelines for
hyperparameter selection. Empirically, AdamS demonstrates strong performance in
various tasks, including pre-training runs on GPT-2 and Llama2 (up to 13B
parameters) and reinforcement learning in post-training regimes. With its
efficiency, simplicity, and theoretical grounding, AdamS stands as a compelling
alternative to existing optimizers.",http://arxiv.org/abs/2505.16363v1
Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation,"Semantic segmentation models trained on synthetic data often perform poorly
on real-world images due to domain gaps, particularly in adverse conditions
where labeled data is scarce. Yet, recent foundation models enable to generate
realistic images without any training. This paper proposes to leverage such
diffusion models to improve the performance of vision models when learned on
synthetic data. We introduce two novel techniques for semantically consistent
style transfer using diffusion models: Class-wise Adaptive Instance
Normalization and Cross-Attention (CACTI) and its extension with selective
attention Filtering (CACTIF). CACTI applies statistical normalization
selectively based on semantic classes, while CACTIF further filters
cross-attention maps based on feature similarity, preventing artifacts in
regions with weak cross-attention correspondences. Our methods transfer style
characteristics while preserving semantic boundaries and structural coherence,
unlike approaches that apply global transformations or generate content without
constraints. Experiments using GTA5 as source and Cityscapes/ACDC as target
domains show that our approach produces higher quality images with lower FID
scores and better content preservation. Our work demonstrates that class-aware
diffusion-based style transfer effectively bridges the synthetic-to-real domain
gap even with minimal target domain data, advancing robust perception systems
for challenging real-world applications. The source code is available at:
https://github.com/echigot/cactif.",http://arxiv.org/abs/2505.16360v1
Arrival Control in Quasi-Reversible Queueing Systems: Optimization and Reinforcement Learning,"In this paper, we introduce a versatile scheme for optimizing the arrival
rates of quasi-reversible queueing systems. We first propose an alternative
definition of quasi-reversibility that encompasses reversibility and highlights
the importance of the definition of customer classes. In a second time, we
introduce balanced arrival control policies, which generalize the notion of
balanced arrival rates introduced in the context of Whittle networks, to the
much broader class of quasi-reversible queueing systems. We prove that
supplementing a quasi-reversible queueing system with a balanced
arrival-control policy preserves the quasi-reversibility, and we specify the
form of the stationary measures. We revisit two canonical examples of
quasi-reversible queueing systems, Whittle networks and order-independent
queues. Lastly, we focus on the problem of admission control and leverage our
results in the frameworks of optimization and reinforcement learning.",http://arxiv.org/abs/2505.16353v1
Graph Attention Network for Optimal User Association in Wireless Networks,"With increased 5G deployments, network densification is higher than ever to
support the exponentially high throughput requirements. However, this has meant
a significant increase in energy consumption, leading to higher operational
expenditure (OpEx) for network operators creating an acute need for
improvements in network energy savings (NES). A key determinant of operational
efficacy in cellular networks is the user association (UA) policy, as it
affects critical aspects like spectral efficiency, load balancing etc. and
therefore impacts the overall energy consumption of the network directly.
Furthermore, with cellular network topologies lending themselves well to
graphical abstractions, use of graphs in network optimization has gained
significant prominence. In this work, we propose and analyze a graphical
abstraction based optimization for UA in cellular networks to improve NES by
determining when energy saving features like cell switch off can be activated.
A comparison with legacy approaches establishes the superiority of the proposed
approach.",http://arxiv.org/abs/2505.16347v1
A Square Peg in a Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning,"This paper studies the long-tailed semi-supervised learning (LTSSL) with
distribution mismatch, where the class distribution of the labeled training
data follows a long-tailed distribution and mismatches with that of the
unlabeled training data. Most existing methods introduce auxiliary classifiers
(experts) to model various unlabeled data distributions and produce
pseudo-labels, but the expertises of various experts are not fully utilized. We
observe that different experts are good at predicting different intervals of
samples, e.g., long-tailed expert is skilled in samples located in the head
interval and uniform expert excels in samples located in the medium interval.
Therefore, we propose a dynamic expert assignment module that can estimate the
class membership (i.e., head, medium, or tail class) of samples, and
dynamically assigns suitable expert to each sample based on the estimated
membership to produce high-quality pseudo-label in the training phase and
produce prediction in the testing phase. We also theoretically reveal that
integrating different experts' strengths will lead to a smaller generalization
error bound. Moreover, we find that the deeper features are more biased toward
the head class but with more discriminative ability, while the shallower
features are less biased but also with less discriminative ability. We,
therefore, propose a multi-depth feature fusion module to utilize different
depth features to mitigate the model bias. Our method demonstrates its
effectiveness through comprehensive experiments on the CIFAR-10-LT, STL-10-LT,
and SVHN-LT datasets across various settings. The code is available at
https://github.com/yaxinhou/Meta-Expert.",http://arxiv.org/abs/2505.16341v1
Improving Chemical Understanding of LLMs via SMILES Parsing,"Large language models (LLMs) are increasingly recognized as powerful tools
for scientific discovery, particularly in molecular science. A fundamental
requirement for these models is the ability to accurately understand molecular
structures, commonly encoded in the SMILES representation. However, current
LLMs struggle to interpret SMILES, even failing to carry out basic tasks such
as counting molecular rings. To address this limitation, we introduce CLEANMOL,
a novel framework that formulates SMILES parsing into a suite of clean and
deterministic tasks explicitly designed to promote graph-level molecular
comprehension. These tasks span from subgraph matching to global graph
matching, providing structured supervision aligned with molecular structural
properties. We construct a molecular pretraining dataset with adaptive
difficulty scoring and pre-train open-source LLMs on these tasks. Our results
show that CLEANMOL not only enhances structural comprehension but also achieves
the best or competes with the baseline on the Mol-Instructions benchmark.",http://arxiv.org/abs/2505.16340v1
Understanding Differential Transformer Unchains Pretrained Self-Attentions,"Differential Transformer has recently gained significant attention for its
impressive empirical performance, often attributed to its ability to perform
noise canceled attention. However, precisely how differential attention
achieves its empirical benefits remains poorly understood. Moreover,
Differential Transformer architecture demands large-scale training from
scratch, hindering utilization of open pretrained weights. In this work, we
conduct an in-depth investigation of Differential Transformer, uncovering three
key factors behind its success: (1) enhanced expressivity via negative
attention, (2) reduced redundancy among attention heads, and (3) improved
learning dynamics. Based on these findings, we propose DEX, a novel method to
efficiently integrate the advantages of differential attention into pretrained
language models. By reusing the softmax attention scores and adding a
lightweight differential operation on the output value matrix, DEX effectively
incorporates the key advantages of differential attention while remaining
lightweight in both training and inference. Evaluations confirm that DEX
substantially improves the pretrained LLMs across diverse benchmarks, achieving
significant performance gains with minimal adaptation data (< 0.01\%).",http://arxiv.org/abs/2505.16333v1
Better Rates for Private Linear Regression in the Proportional Regime via Aggressive Clipping,"Differentially private (DP) linear regression has received significant
attention in the recent theoretical literature, with several works aimed at
obtaining improved error rates. A common approach is to set the clipping
constant much larger than the expected norm of the per-sample gradients. While
simplifying the analysis, this is however in sharp contrast with what empirical
evidence suggests to optimize performance. Our work bridges this gap between
theory and practice: we provide sharper rates for DP stochastic gradient
descent (DP-SGD) by crucially operating in a regime where clipping happens
frequently. Specifically, we consider the setting where the data is
multivariate Gaussian, the number of training samples $n$ is proportional to
the input dimension $d$, and the algorithm guarantees constant-order zero
concentrated DP. Our method relies on establishing a deterministic equivalent
for the trajectory of DP-SGD in terms of a family of ordinary differential
equations (ODEs). As a consequence, the risk of DP-SGD is bounded between two
ODEs, with upper and lower bounds matching for isotropic data. By studying
these ODEs when $n / d$ is large enough, we demonstrate the optimality of
aggressive clipping, and we uncover the benefits of decaying learning rate and
private noise scheduling.",http://arxiv.org/abs/2505.16329v1
ChemMLLM: Chemical Multimodal Large Language Model,"Multimodal large language models (MLLMs) have made impressive progress in
many applications in recent years. However, chemical MLLMs that can handle
cross-modal understanding and generation remain underexplored. To fill this
gap, in this paper, we propose ChemMLLM, a unified chemical multimodal large
language model for molecule understanding and generation. Also, we design five
multimodal tasks across text, molecular SMILES strings, and image, and curate
the datasets. We benchmark ChemMLLM against a range of general leading MLLMs
and Chemical LLMs on these tasks. Experimental results show that ChemMLLM
achieves superior performance across all evaluated tasks. For example, in
molecule image optimization task, ChemMLLM outperforms the best baseline
(GPT-4o) by 118.9\% (4.27 vs 1.95 property improvement). The code is publicly
available at https://github.com/bbsbz/ChemMLLM.git.",http://arxiv.org/abs/2505.16326v1
AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners,"Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.",http://arxiv.org/abs/2505.16322v1
Learning novel representations of variable sources from multi-modal $\textit{Gaia}$ data via autoencoders,"Gaia Data Release 3 (DR3) published for the first time epoch photometry,
BP/RP (XP) low-resolution mean spectra, and supervised classification results
for millions of variable sources. This extensive dataset offers a unique
opportunity to study their variability by combining multiple Gaia data
products. In preparation for DR4, we propose and evaluate a machine learning
methodology capable of ingesting multiple Gaia data products to achieve an
unsupervised classification of stellar and quasar variability. A dataset of 4
million Gaia DR3 sources is used to train three variational autoencoders (VAE),
which are artificial neural networks (ANNs) designed for data compression and
generation. One VAE is trained on Gaia XP low-resolution spectra, another on a
novel approach based on the distribution of magnitude differences in the Gaia G
band, and the third on folded Gaia G band light curves. Each Gaia source is
compressed into 15 numbers, representing the coordinates in a 15-dimensional
latent space generated by combining the outputs of these three models. The
learned latent representation produced by the ANN effectively distinguishes
between the main variability classes present in Gaia DR3, as demonstrated
through both supervised and unsupervised classification analysis of the latent
space. The results highlight a strong synergy between light curves and
low-resolution spectral data, emphasising the benefits of combining the
different Gaia data products. A two-dimensional projection of the latent
variables reveals numerous overdensities, most of which strongly correlate with
astrophysical properties, showing the potential of this latent space for
astrophysical discovery. We show that the properties of our novel latent
representation make it highly valuable for variability analysis tasks,
including classification, clustering and outlier detection.",http://arxiv.org/abs/2505.16320v1
FreshRetailNet-50K: A Stockout-Annotated Censored Demand Dataset for Latent Demand Recovery and Forecasting in Fresh Retail,"Accurate demand estimation is critical for the retail business in guiding the
inventory and pricing policies of perishable products. However, it faces
fundamental challenges from censored sales data during stockouts, where
unobserved demand creates systemic policy biases. Existing datasets lack the
temporal resolution and annotations needed to address this censoring effect. To
fill this gap, we present FreshRetailNet-50K, the first large-scale benchmark
for censored demand estimation. It comprises 50,000 store-product time series
of detailed hourly sales data from 898 stores in 18 major cities, encompassing
863 perishable SKUs meticulously annotated for stockout events. The hourly
stock status records unique to this dataset, combined with rich contextual
covariates, including promotional discounts, precipitation, and temporal
features, enable innovative research beyond existing solutions. We demonstrate
one such use case of two-stage demand modeling: first, we reconstruct the
latent demand during stockouts using precise hourly annotations. We then
leverage the recovered demand to train robust demand forecasting models in the
second stage. Experimental results show that this approach achieves a 2.73\%
improvement in prediction accuracy while reducing the systematic demand
underestimation from 7.37\% to near-zero bias. With unprecedented temporal
granularity and comprehensive real-world information, FreshRetailNet-50K opens
new research directions in demand imputation, perishable inventory
optimization, and causal retail analytics. The unique annotation quality and
scale of the dataset address long-standing limitations in retail AI, providing
immediate solutions and a platform for future methodological innovation. The
data (https://huggingface.co/datasets/Dingdong-Inc/FreshRetailNet-50K) and code
(https://github.com/Dingdong-Inc/frn-50k-baseline}) are openly released.",http://arxiv.org/abs/2505.16319v1
Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings,"Deep neural networks for image classification remain vulnerable to
adversarial examples -- small, imperceptible perturbations that induce
misclassifications. In black-box settings, where only the final prediction is
accessible, crafting targeted attacks that aim to misclassify into a specific
target class is particularly challenging due to narrow decision regions.
Current state-of-the-art methods often exploit the geometric properties of the
decision boundary separating a source image and a target image rather than
incorporating information from the images themselves. In contrast, we propose
Targeted Edge-informed Attack (TEA), a novel attack that utilizes edge
information from the target image to carefully perturb it, thereby producing an
adversarial image that is closer to the source image while still achieving the
desired target classification. Our approach consistently outperforms current
state-of-the-art methods across different models in low query settings (nearly
70\% fewer queries are used), a scenario especially relevant in real-world
applications with limited queries and black-box access. Furthermore, by
efficiently generating a suitable adversarial example, TEA provides an improved
target initialization for established geometry-based attacks.",http://arxiv.org/abs/2505.16313v1
Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions,"Recent advances in generative artificial intelligence (GenAI) models have
enabled the generation of personalized content that adapts to up-to-date user
context. While personalized decision systems are often modeled using bandit
formulations, the integration of GenAI introduces new structure into otherwise
classical sequential learning problems. In GenAI-powered interventions, the
agent selects a query, but the environment experiences a stochastic response
drawn from the generative model. Standard bandit methods do not explicitly
account for this structure, where actions influence rewards only through
stochastic, observed treatments. We introduce generator-mediated
bandit-Thompson sampling (GAMBITTS), a bandit approach designed for this
action/treatment split, using mobile health interventions with large language
model-generated text as a motivating case study. GAMBITTS explicitly models
both the treatment and reward generation processes, using information in the
delivered treatment to accelerate policy learning relative to standard methods.
We establish regret bounds for GAMBITTS by decomposing sources of uncertainty
in treatment and reward, identifying conditions where it achieves stronger
guarantees than standard bandit approaches. In simulation studies, GAMBITTS
consistently outperforms conventional algorithms by leveraging observed
treatments to more accurately estimate expected rewards.",http://arxiv.org/abs/2505.16311v1
CAIFormer: A Causal Informed Transformer for Multivariate Time Series Forecasting,"Most existing multivariate time series forecasting methods adopt an
all-to-all paradigm that feeds all variable histories into a unified model to
predict their future values without distinguishing their individual roles.
However, this undifferentiated paradigm makes it difficult to identify
variable-specific causal influences and often entangles causally relevant
information with spurious correlations. To address this limitation, we propose
an all-to-one forecasting paradigm that predicts each target variable
separately. Specifically, we first construct a Structural Causal Model from
observational data and then, for each target variable, we partition the
historical sequence into four sub-segments according to the inferred causal
structure: endogenous, direct causal, collider causal, and spurious
correlation. The prediction relies solely on the first three causally relevant
sub-segments, while the spurious correlation sub-segment is excluded.
Furthermore, we propose Causal Informed Transformer (CAIFormer), a novel
forecasting model comprising three components: Endogenous Sub-segment
Prediction Block, Direct Causal Sub-segment Prediction Block, and Collider
Causal Sub-segment Prediction Block, which process the endogenous, direct
causal, and collider causal sub-segments, respectively. Their outputs are then
combined to produce the final prediction. Extensive experiments on multiple
benchmark datasets demonstrate the effectiveness of the CAIFormer.",http://arxiv.org/abs/2505.16308v1
PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models,"Prompt optimization offers a practical and broadly applicable alternative to
fine-tuning for improving large language model (LLM) performance. However,
existing methods often rely on costly output generation, self-critiquing
abilities, or human-annotated preferences, which limit their scalability,
especially for smaller or non-instruction-tuned models. We introduce PMPO
(Probabilistic Metric Prompt Optimization), a unified framework that refines
prompts using token-level cross-entropy loss as a direct, lightweight
evaluation signal. PMPO identifies low-quality prompt segments by masking and
measuring their impact on loss, then rewrites and selects improved variants by
minimizing loss over positive and negative examples. Unlike prior methods, it
requires no output sampling or human evaluation during optimization, relying
only on forward passes and log-likelihoods. PMPO supports both supervised and
preference-based tasks through a closely aligned loss-based evaluation
strategy. Experiments show that PMPO consistently outperforms prior methods
across model sizes and tasks: it achieves the highest average accuracy on BBH,
performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates
by over 19 points. These results highlight PMPO's effectiveness, efficiency,
and broad applicability.",http://arxiv.org/abs/2505.16307v1
Large-Scale Bayesian Tensor Reconstruction: An Approximate Message Passing Solution,"Tensor CANDECOMP/PARAFAC decomposition (CPD) is a fundamental model for
tensor reconstruction. Although the Bayesian framework allows for principled
uncertainty quantification and automatic hyperparameter learning, existing
methods do not scale well for large tensors because of high-dimensional matrix
inversions. To this end, we introduce CP-GAMP, a scalable Bayesian CPD
algorithm. This algorithm leverages generalized approximate message passing
(GAMP) to avoid matrix inversions and incorporates an expectation-maximization
routine to jointly infer the tensor rank and noise power. Through multiple
experiments, for synthetic 100x100x100 rank 20 tensors with only 20% elements
observed, the proposed algorithm reduces runtime by 82.7% compared to the
state-of-the-art variational Bayesian CPD method, while maintaining comparable
reconstruction accuracy.",http://arxiv.org/abs/2505.16305v1
Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space,"Molecular dynamics (MD) is a powerful tool for exploring the behavior of
atomistic systems, but its reliance on sequential numerical integration limits
simulation efficiency. We present MDtrajNet-1, a foundational AI model that
directly generates MD trajectories across chemical space, bypassing force
calculations and integration. This approach accelerates simulations by up to
two orders of magnitude compared to traditional MD, even those enhanced by
machine-learning interatomic potentials. MDtrajNet-1 combines equivariant
neural networks with a Transformer-based architecture to achieve strong
accuracy and transferability in predicting long-time trajectories for both
known and unseen systems. Remarkably, the errors of the trajectories generated
by MDtrajNet-1 for various molecular systems are close to those of the
conventional ab initio MD. The model's flexible design supports diverse
application scenarios, including different statistical ensembles, boundary
conditions, and interaction types. By overcoming the intrinsic speed barrier of
conventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable
atomistic simulations.",http://arxiv.org/abs/2505.16301v1
Fairness under Competition,"Algorithmic fairness has emerged as a central issue in ML, and it has become
standard practice to adjust ML algorithms so that they will satisfy fairness
requirements such as Equal Opportunity. In this paper we consider the effects
of adopting such fair classifiers on the overall level of ecosystem fairness.
Specifically, we introduce the study of fairness with competing firms, and
demonstrate the failure of fair classifiers in yielding fair ecosystems. Our
results quantify the loss of fairness in systems, under a variety of
conditions, based on classifiers' correlation and the level of their data
overlap. We show that even if competing classifiers are individually fair, the
ecosystem's outcome may be unfair; and that adjusting biased algorithms to
improve their individual fairness may lead to an overall decline in ecosystem
fairness. In addition to these theoretical results, we also provide supporting
experimental evidence. Together, our model and results provide a novel and
essential call for action.",http://arxiv.org/abs/2505.16291v1
Only Large Weights (And Not Skip Connections) Can Prevent the Perils of Rank Collapse,"Attention mechanisms lie at the heart of modern large language models (LLMs).
Straightforward algorithms for forward and backward (gradient) computation take
quadratic time, and a line of work initiated by [Alman and Song NeurIPS 2023]
and [Alman and Song NeurIPS 2024] has shown that quadratic time is necessary
unless the model weights are small, in which case almost linear time algorithms
are possible. In this paper, we show that large weights are necessary to avoid
a strong preclusion to representational strength we call layer collapse, which
means that the entire network can be approximated well by a network with only a
single layer. Thus, the quadratic running time of attention is unavoidable for
expressive transformers.
  The notion of layer collapse that we introduce is a variant on the notion of
rank collapse from the work of [Dong, Cordonnier, and Loukas ICML 2021]. They
showed that in Self Attention Networks with small weights and with skip
connections, rank collapse must occur. This is typically interpreted as
justifying the necessity of skip connections in expressive networks. However,
our result shows that even with skip connections, if the weights are small,
then layer collapse still occurs. Thus, only large weights, and not skip
connections, can prevent these representational weaknesses.",http://arxiv.org/abs/2505.16284v1
Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning,"Large language models are typically adapted to downstream tasks through
supervised fine-tuning on domain-specific data. While standard fine-tuning
focuses on minimizing generation loss to optimize model parameters, we take a
deeper step by retaining and leveraging the model's own learning signals,
analogous to how human learners reflect on past mistakes to improve future
performance. We first introduce the concept of Mistake Log to systematically
track the model's learning behavior and recurring errors throughout
fine-tuning. Treating the original transformer-based model as the Pilot, we
correspondingly design a Copilot model to refine the Pilot's inference
performance via logits rectification. We name the overall Pilot-Copilot
framework the Transformer Copilot, which introduces (i) a novel Copilot model
design, (ii) a joint training paradigm where the Copilot continuously learns
from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference
paradigm where the Copilot rectifies the Pilot's logits for enhanced
generation. We provide both theoretical and empirical analyses on our new
learning framework. Experiments on 12 benchmarks spanning commonsense,
arithmetic, and recommendation tasks demonstrate that Transformer Copilot
consistently improves performance by up to 34.5%, while introducing marginal
computational overhead to Pilot models and exhibiting strong scalability and
transferability.",http://arxiv.org/abs/2505.16270v1
Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models,"Reinforcement learning from human feedback (RLHF) has become a powerful
post-training paradigm for aligning large language models with human
preferences. A core challenge in RLHF is constructing accurate reward signals,
where the conventional Bradley-Terry reward models (BT RMs) often suffer from
sensitivity to data size and coverage, as well as vulnerability to reward
hacking. Generative reward models (GenRMs) offer a more robust alternative by
generating chain-of-thought (CoT) rationales followed by a final reward.
However, existing GenRMs rely on shallow, vertically scaled reasoning, limiting
their capacity to handle nuanced or complex (e.g., reasoning-intensive) tasks.
Moreover, their pairwise preference outputs are incompatible with standard RLHF
algorithms that require pointwise reward signals. In this work, we introduce
Think-RM, a training framework that enables long-horizon reasoning in GenRMs by
modeling an internal thinking process. Rather than producing structured,
externally provided rationales, Think-RM generates flexible, self-guided
reasoning traces that support advanced capabilities such as self-reflection,
hypothetical reasoning, and divergent reasoning. To elicit these reasoning
abilities, we first warm-up the models by supervised fine-tuning (SFT) over
long CoT data. We then further improve the model's long-horizon abilities by
rule-based reinforcement learning (RL). In addition, we propose a novel
pairwise RLHF pipeline that directly optimizes policies using pairwise
preference rewards, eliminating the need for pointwise reward conversion and
enabling more effective use of Think-RM outputs. Experiments show that Think-RM
achieves state-of-the-art results on RM-Bench, outperforming both BT RM and
vertically scaled GenRM by 8%. When combined with our pairwise RLHF pipeline,
it demonstrates superior end-policy performance compared to traditional
approaches.",http://arxiv.org/abs/2505.16265v1
"All You Need is ""Leet"": Evading Hate-speech Detection AI","Social media and online forums are increasingly becoming popular.
Unfortunately, these platforms are being used for spreading hate speech. In
this paper, we design black-box techniques to protect users from hate-speech on
online platforms by generating perturbations that can fool state of the art
deep learning based hate speech detection models thereby decreasing their
efficiency. We also ensure a minimal change in the original meaning of
hate-speech. Our best perturbation attack is successfully able to evade
hate-speech detection for 86.8 % of hateful text.",http://arxiv.org/abs/2505.16263v1
Small-to-Large Generalization: Data Influences Models Consistently Across Scale,"Choice of training data distribution greatly influences model behavior. Yet,
in large-scale settings, precisely characterizing how changes in training data
affects predictions is often difficult due to model training costs. Current
practice is to instead extrapolate from scaled down, inexpensive-to-train proxy
models. However, changes in data do not influence smaller and larger models
identically. Therefore, understanding how choice of data affects large-scale
models raises the question: how does training data distribution influence model
behavior across compute scale? We find that small- and large-scale language
model predictions (generally) do highly correlate across choice of training
data. Equipped with these findings, we characterize how proxy scale affects
effectiveness in two downstream proxy model applications: data attribution and
dataset selection.",http://arxiv.org/abs/2505.16260v1
Higher-Order Asymptotics of Test-Time Adaptation for Batch Normalization Statistics,"This study develops a higher-order asymptotic framework for test-time
adaptation (TTA) of Batch Normalization (BN) statistics under distribution
shift by integrating classical Edgeworth expansion and saddlepoint
approximation techniques with a novel one-step M-estimation perspective. By
analyzing the statistical discrepancy between training and test distributions,
we derive an Edgeworth expansion for the normalized difference in BN means and
obtain an optimal weighting parameter that minimizes the mean-squared error of
the adapted statistic. Reinterpreting BN TTA as a one-step M-estimator allows
us to derive higher-order local asymptotic normality results, which incorporate
skewness and other higher moments into the estimator's behavior. Moreover, we
quantify the trade-offs among bias, variance, and skewness in the adaptation
process and establish a corresponding generalization bound on the model risk.
The refined saddlepoint approximations further deliver uniformly accurate
density and tail probability estimates for the BN TTA statistic. These
theoretical insights provide a comprehensive understanding of how higher-order
corrections and robust one-step updating can enhance the reliability and
performance of BN layers in adapting to changing data distributions.",http://arxiv.org/abs/2505.16257v1
Graph-Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry,"Label shift adaptation aims to recover target class priors when the labelled
source distribution $P$ and the unlabelled target distribution $Q$ share $P(X
\mid Y) = Q(X \mid Y)$ but $P(Y) \neq Q(Y)$. Classical black-box shift
estimators invert an empirical confusion matrix of a frozen classifier,
producing a brittle point estimate that ignores sampling noise and similarity
among classes. We present Graph-Smoothed Bayesian BBSE (GS-B$^3$SE), a fully
probabilistic alternative that places Laplacian-Gaussian priors on both target
log-priors and confusion-matrix columns, tying them together on a
label-similarity graph. The resulting posterior is tractable with HMC or a fast
block Newton-CG scheme. We prove identifiability, $N^{-1/2}$ contraction,
variance bounds that shrink with the graph's algebraic connectivity, and
robustness to Laplacian misspecification. We also reinterpret GS-B$^3$SE
through information geometry, showing that it generalizes existing shift
estimators.",http://arxiv.org/abs/2505.16251v1
Graph Neural Network-Based Collaborative Perception for Adaptive Scheduling in Distributed Systems,"This paper addresses the limitations of multi-node perception and delayed
scheduling response in distributed systems by proposing a GNN-based multi-node
collaborative perception mechanism. The system is modeled as a graph structure.
Message-passing and state-update modules are introduced. A multi-layer graph
neural network is constructed to enable efficient information aggregation and
dynamic state inference among nodes. In addition, a perception representation
method is designed by fusing local states with global features. This improves
each node's ability to perceive the overall system status. The proposed method
is evaluated within a customized experimental framework. A dataset featuring
heterogeneous task loads and dynamic communication topologies is used.
Performance is measured in terms of task completion rate, average latency, load
balancing, and transmission efficiency. Experimental results show that the
proposed method outperforms mainstream algorithms under various conditions,
including limited bandwidth and dynamic structural changes. It demonstrates
superior perception capabilities and cooperative scheduling performance. The
model achieves rapid convergence and efficient responses to complex system
states.",http://arxiv.org/abs/2505.16248v1
Generalized Power Priors for Improved Bayesian Inference with Historical Data,"The power prior is a class of informative priors designed to incorporate
historical data alongside current data in a Bayesian framework. It includes a
power parameter that controls the influence of historical data, providing
flexibility and adaptability. A key property of the power prior is that the
resulting posterior minimizes a linear combination of KL divergences between
two pseudo-posterior distributions: one ignoring historical data and the other
fully incorporating it. We extend this framework by identifying the posterior
distribution as the minimizer of a linear combination of Amari's
$\alpha$-divergence, a generalization of KL divergence. We show that this
generalization can lead to improved performance by allowing for the data to
adapt to appropriate choices of the $\alpha$ parameter. Theoretical properties
of this generalized power posterior are established, including behavior as a
generalized geodesic on the Riemannian manifold of probability distributions,
offering novel insights into its geometric interpretation.",http://arxiv.org/abs/2505.16244v1
Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies,"When applying offline reinforcement learning (RL) in healthcare scenarios,
the out-of-distribution (OOD) issues pose significant risks, as inappropriate
generalization beyond clinical expertise can result in potentially harmful
recommendations. While existing methods like conservative Q-learning (CQL)
attempt to address the OOD issue, their effectiveness is limited by only
constraining action selection by suppressing uncertain actions. This
action-only regularization imitates clinician actions that prioritize
short-term rewards, but it fails to regulate downstream state trajectories,
thereby limiting the discovery of improved long-term treatment strategies. To
safely improve policy beyond clinician recommendations while ensuring that
state-action trajectories remain in-distribution, we propose \textit{Offline
Guarded Safe Reinforcement Learning} ($\mathsf{OGSRL}$), a theoretically
grounded model-based offline RL framework. $\mathsf{OGSRL}$ introduces a novel
dual constraint mechanism for improving policy with reliability and safety.
First, the OOD guardian is established to specify clinically validated regions
for safe policy exploration. By constraining optimization within these regions,
it enables the reliable exploration of treatment strategies that outperform
clinician behavior by leveraging the full patient state history, without
drifting into unsupported state-action trajectories. Second, we introduce a
safety cost constraint that encodes medical knowledge about physiological
safety boundaries, providing domain-specific safeguards even in areas where
training data might contain potentially unsafe interventions. Notably, we
provide theoretical guarantees on safety and near-optimality: policies that
satisfy these constraints remain in safe and reliable regions and achieve
performance close to the best possible policy supported by the data.",http://arxiv.org/abs/2505.16242v1
"Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning","Personalizing jargon detection and explanation is essential for making
technical documents accessible to readers with diverse disciplinary
backgrounds. However, tailoring models to individual users typically requires
substantial annotation efforts and computational resources due to user-specific
finetuning. To address this, we present a systematic study of personalized
jargon detection, focusing on methods that are both efficient and scalable for
real-world deployment. We explore two personalization strategies: (1)
lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,
and (2) personalized prompting, which tailors model behavior at inference time
without retaining. To reflect realistic constraints, we also investigate hybrid
approaches that combine limited annotated data with unsupervised user
background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in
F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,
our method achieves comparable performance using only 10% of the annotated
training data, demonstrating its practicality for resource-constrained
settings. Our study offers the first work to systematically explore efficient,
low-resource personalization of jargon detection using open-source language
models, offering a practical path toward scalable, user-adaptive NLP system.",http://arxiv.org/abs/2505.16227v1
Realistic Evaluation of TabPFN v2 in Open Environments,"Tabular data, owing to its ubiquitous presence in real-world domains, has
garnered significant attention in machine learning research. While tree-based
models have long dominated tabular machine learning tasks, the recently
proposed deep learning model TabPFN v2 has emerged, demonstrating unparalleled
performance and scalability potential. Although extensive research has been
conducted on TabPFN v2 to further improve performance, the majority of this
research remains confined to closed environments, neglecting the challenges
that frequently arise in open environments. This raises the question: Can
TabPFN v2 maintain good performance in open environments? To this end, we
conduct the first comprehensive evaluation of TabPFN v2's adaptability in open
environments. We construct a unified evaluation framework covering various
real-world challenges and assess the robustness of TabPFN v2 under open
environments scenarios using this framework. Empirical results demonstrate that
TabPFN v2 shows significant limitations in open environments but is suitable
for small-scale, covariate-shifted, and class-balanced tasks. Tree-based models
remain the optimal choice for general tabular tasks in open environments. To
facilitate future research on open environments challenges, we advocate for
open environments tabular benchmarks, multi-metric evaluation, and universal
modules to strengthen model robustness. We publicly release our evaluation
framework at https://anonymous.4open.science/r/tabpfn-ood-4E65.",http://arxiv.org/abs/2505.16226v1
MADCluster: Model-agnostic Anomaly Detection with Self-supervised Clustering Network,"In this paper, we propose MADCluster, a novel model-agnostic anomaly
detection framework utilizing self-supervised clustering. MADCluster is
applicable to various deep learning architectures and addresses the
'hypersphere collapse' problem inherent in existing deep learning-based anomaly
detection methods. The core idea is to cluster normal pattern data into a
'single cluster' while simultaneously learning the cluster center and mapping
data close to this center. Also, to improve expressiveness and enable effective
single clustering, we propose a new 'One-directed Adaptive loss'. The
optimization of this loss is mathematically proven. MADCluster consists of
three main components: Base Embedder capturing high-dimensional temporal
dynamics, Cluster Distance Mapping, and Sequence-wise Clustering for continuous
center updates. Its model-agnostic characteristics are achieved by applying
various architectures to the Base Embedder. Experiments on four time series
benchmark datasets demonstrate that applying MADCluster improves the overall
performance of comparative models. In conclusion, the compatibility of
MADCluster shows potential for enhancing model performance across various
architectures.",http://arxiv.org/abs/2505.16223v1
Reward-Aware Proto-Representations in Reinforcement Learning,"In recent years, the successor representation (SR) has attracted increasing
attention in reinforcement learning (RL), and it has been used to address some
of its key challenges, such as exploration, credit assignment, and
generalization. The SR can be seen as representing the underlying credit
assignment structure of the environment by implicitly encoding its induced
transition dynamics. However, the SR is reward-agnostic. In this paper, we
discuss a similar representation that also takes into account the reward
dynamics of the problem. We study the default representation (DR), a recently
proposed representation with limited theoretical (and empirical) analysis.
Here, we lay some of the theoretical foundation underlying the DR in the
tabular case by (1) deriving dynamic programming and (2) temporal-difference
methods to learn the DR, (3) characterizing the basis for the vector space of
the DR, and (4) formally extending the DR to the function approximation case
through default features. Empirically, we analyze the benefits of the DR in
many of the settings in which the SR has been applied, including (1) reward
shaping, (2) option discovery, (3) exploration, and (4) transfer learning. Our
results show that, compared to the SR, the DR gives rise to qualitatively
different, reward-aware behaviour and quantitatively better performance in
several settings.",http://arxiv.org/abs/2505.16217v1
A Scalable Hierarchical Intrusion Detection System for Internet of Vehicles,"Due to its nature of dynamic, mobility, and wireless data transfer, the
Internet of Vehicles (IoV) is prone to various cyber threats, ranging from
spoofing and Distributed Denial of Services (DDoS) attacks to malware. To
safeguard the IoV ecosystem from intrusions, malicious activities, policy
violations, intrusion detection systems (IDS) play a critical role by
continuously monitoring and analyzing network traffic to identify and mitigate
potential threats in real-time. However, most existing research has focused on
developing centralized, machine learning-based IDS systems for IoV without
accounting for its inherently distributed nature. Due to intensive computing
requirements, these centralized systems often rely on the cloud to detect cyber
threats, increasing delay of system response. On the other hand, edge nodes
typically lack the necessary resources to train and deploy complex machine
learning algorithms. To address this issue, this paper proposes an effective
hierarchical classification framework tailored for IoV networks. Hierarchical
classification allows classifiers to be trained and tested at different levels,
enabling edge nodes to detect specific types of attacks independently. With
this approach, edge nodes can conduct targeted attack detection while
leveraging cloud nodes for comprehensive threat analysis and support. Given the
resource constraints of edge nodes, we have employed the Boruta feature
selection method to reduce data dimensionality, optimizing processing
efficiency. To evaluate our proposed framework, we utilize the latest IoV
security dataset CIC-IoV2024, achieving promising results that demonstrate the
feasibility and effectiveness of our models in securing IoV networks.",http://arxiv.org/abs/2505.16215v1
NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics,"Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (KV)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, quantization is a common and straightforward approach.
Currently, quantization methods for activations are limited to 8-bit, and
quantization to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the KV cache to even lower bits, we analyzed
the element distribution of the KV cache and designed the NQKV algorithm. Since
the elements within each block of the KV cache follow a normal distribution,
NQKV employs per-block quantile quantization to achieve
information-theoretically optimal quantization error. Without significantly
compromising model output quality, NQKV enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the KV cache is not used.",http://arxiv.org/abs/2505.16210v1
Using Echo-State Networks to Reproduce Rare Events in Chaotic Systems,"We apply the Echo-State Networks to predict the time series and statistical
properties of the competitive Lotka-Volterra model in the chaotic regime. In
particular, we demonstrate that Echo-State Networks successfully learn the
chaotic attractor of the competitive Lotka-Volterra model and reproduce
histograms of dependent variables, including tails and rare events. We use the
Generalized Extreme Value distribution to quantify the tail behavior.",http://arxiv.org/abs/2505.16208v1
"Directional Convergence, Benign Overfitting of Gradient Descent in leaky ReLU two-layer Neural Networks","In this paper, we prove directional convergence of network parameters of
fixed width leaky ReLU two-layer neural networks optimized by gradient descent
with exponential loss, which was previously only known for gradient flow. By a
careful analysis of the convergent direction, we establish sufficient
conditions of benign overfitting and discover a new phase transition in the
test error bound. All of these results hold beyond the nearly orthogonal data
setting which was studied in prior works. As an application, we demonstrate
that benign overfitting occurs with high probability in sub-Gaussian mixture
models.",http://arxiv.org/abs/2505.16204v1
SpecMaskFoley: Steering Pretrained Spectral Masked Generative Transformer Toward Synchronized Video-to-audio Synthesis via ControlNet,"Foley synthesis aims to synthesize high-quality audio that is both
semantically and temporally aligned with video frames. Given its broad
application in creative industries, the task has gained increasing attention in
the research community. To avoid the non-trivial task of training audio
generative models from scratch, adapting pretrained audio generative models for
video-synchronized foley synthesis presents an attractive direction.
ControlNet, a method for adding fine-grained controls to pretrained generative
models, has been applied to foley synthesis, but its use has been limited to
handcrafted human-readable temporal conditions. In contrast, from-scratch
models achieved success by leveraging high-dimensional deep features extracted
using pretrained video encoders. We have observed a performance gap between
ControlNet-based and from-scratch foley models. To narrow this gap, we propose
SpecMaskFoley, a method that steers the pretrained SpecMaskGIT model toward
video-synchronized foley synthesis via ControlNet. To unlock the potential of a
single ControlNet branch, we resolve the discrepancy between the temporal video
features and the time-frequency nature of the pretrained SpecMaskGIT via a
frequency-aware temporal feature aligner, eliminating the need for complicated
conditioning mechanisms widely used in prior arts. Evaluations on a common
foley synthesis benchmark demonstrate that SpecMaskFoley could even outperform
strong from-scratch baselines, substantially advancing the development of
ControlNet-based foley synthesis models. Demo page:
https://zzaudio.github.io/SpecMaskFoley_Demo/",http://arxiv.org/abs/2505.16195v1
Enhancing Federated Survival Analysis through Peer-Driven Client Reputation in Healthcare,"Federated Learning (FL) holds great promise for digital health by enabling
collaborative model training without compromising patient data privacy.
However, heterogeneity across institutions, lack of sustained reputation, and
unreliable contributions remain major challenges. In this paper, we propose a
robust, peer-driven reputation mechanism for federated healthcare that employs
a hybrid communication model to integrate decentralized peer feedback with
clustering-based noise handling to enhance model aggregation. Crucially, our
approach decouples the federated aggregation and reputation mechanisms by
applying differential privacy to client-side model updates before sharing them
for peer evaluation. This ensures sensitive information remains protected
during reputation computation, while unaltered updates are sent to the server
for global model training. Using the Cox Proportional Hazards model for
survival analysis across multiple federated nodes, our framework addresses both
data heterogeneity and reputation deficit by dynamically adjusting trust scores
based on local performance improvements measured via the concordance index.
Experimental evaluations on both synthetic datasets and the SEER dataset
demonstrate that our method consistently achieves high and stable C-index
values, effectively down-weighing noisy client updates and outperforming FL
methods that lack a reputation system.",http://arxiv.org/abs/2505.16190v1
Why Can Accurate Models Be Learned from Inaccurate Annotations?,"Learning from inaccurate annotations has gained significant attention due to
the high cost of precise labeling. However, despite the presence of erroneous
labels, models trained on noisy data often retain the ability to make accurate
predictions. This intriguing phenomenon raises a fundamental yet largely
unexplored question: why models can still extract correct label information
from inaccurate annotations remains unexplored. In this paper, we conduct a
comprehensive investigation into this issue. By analyzing weight matrices from
both empirical and theoretical perspectives, we find that label inaccuracy
primarily accumulates noise in lower singular components and subtly perturbs
the principal subspace. Within a certain range, the principal subspaces of
weights trained on inaccurate labels remain largely aligned with those learned
from clean labels, preserving essential task-relevant information. We formally
prove that the angles of principal subspaces exhibit minimal deviation under
moderate label inaccuracy, explaining why models can still generalize
effectively. Building on these insights, we propose LIP, a lightweight plug-in
designed to help classifiers retain principal subspace information while
mitigating noise induced by label inaccuracy. Extensive experiments on tasks
with various inaccuracy conditions demonstrate that LIP consistently enhances
the performance of existing algorithms. We hope our findings can offer valuable
theoretical and practical insights to understand of model robustness under
inaccurate supervision.",http://arxiv.org/abs/2505.16159v1
Integral Imprecise Probability Metrics,"Quantifying differences between probability distributions is fundamental to
statistics and machine learning, primarily for comparing statistical
uncertainty. In contrast, epistemic uncertainty (EU) -- due to incomplete
knowledge -- requires richer representations than those offered by classical
probability. Imprecise probability (IP) theory offers such models, capturing
ambiguity and partial belief. This has driven growing interest in imprecise
probabilistic machine learning (IPML), where inference and decision-making rely
on broader uncertainty models -- highlighting the need for metrics beyond
classical probability. This work introduces the Integral Imprecise Probability
Metric (IIPM) framework, a Choquet integral-based generalisation of classical
Integral Probability Metric (IPM) to the setting of capacities -- a broad class
of IP models encompassing many existing ones, including lower probabilities,
probability intervals, belief functions, and more. Theoretically, we establish
conditions under which IIPM serves as a valid metric and metrises a form of
weak convergence of capacities. Practically, IIPM not only enables comparison
across different IP models but also supports the quantification of epistemic
uncertainty within a single IP model. In particular, by comparing an IP model
with its conjugate, IIPM gives rise to a new class of EU measures -- Maximum
Mean Imprecision -- which satisfy key axiomatic properties proposed in the
Uncertainty Quantification literature. We validate MMI through selective
classification experiments, demonstrating strong empirical performance against
established EU measures, and outperforming them when classical methods struggle
to scale to a large number of classes. Our work advances both theory and
practice in IPML, offering a principled framework for comparing and quantifying
epistemic uncertainty under imprecision.",http://arxiv.org/abs/2505.16156v1
NAN: A Training-Free Solution to Coefficient Estimation in Model Merging,"Model merging offers a training-free alternative to multi-task learning by
combining independently fine-tuned models into a unified one without access to
raw data. However, existing approaches often rely on heuristics to determine
the merging coefficients, limiting their scalability and generality. In this
work, we revisit model merging through the lens of least-squares optimization
and show that the optimal merging weights should scale with the amount of
task-specific information encoded in each model. Based on this insight, we
propose NAN, a simple yet effective method that estimates model merging
coefficients via the inverse of parameter norm. NAN is training-free,
plug-and-play, and applicable to a wide range of merging strategies. Extensive
experiments on show that NAN consistently improves performance of baseline
methods.",http://arxiv.org/abs/2505.16148v1
Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation,"Large vision-language models (LVLMs) have achieved remarkable performance on
multimodal tasks such as visual question answering (VQA) and image captioning.
However, they still suffer from hallucinations, generating text inconsistent
with visual input, posing significant risks in real-world applications.
Existing approaches to address this issue focus on incorporating external
knowledge bases, alignment training, or decoding strategies, all of which
require substantial computational cost and time. Recent works try to explore
more efficient alternatives by adjusting LVLMs' internal representations.
Although promising, these methods may cause hallucinations to be insufficiently
suppressed or lead to excessive interventions that negatively affect normal
semantics. In this work, we leverage sparse autoencoders (SAEs) to identify
semantic directions closely associated with either hallucinations or actuality,
realizing more precise and direct hallucination-related representations. Our
analysis demonstrates that interventions along the faithful direction we
identified can mitigate hallucinations, while those along the hallucinatory
direction can exacerbate them. Building on these insights, we propose Steering
LVLMs via SAE Latent Directions (SSL), a training-free method based on
SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive
experiments demonstrate that SSL significantly outperforms existing decoding
approaches in mitigating hallucinations, while maintaining transferability
across different model architectures with negligible additional time overhead.",http://arxiv.org/abs/2505.16146v1
Exponential Convergence of CAVI for Bayesian PCA,"Probabilistic principal component analysis (PCA) and its Bayesian variant
(BPCA) are widely used for dimension reduction in machine learning and
statistics. The main advantage of probabilistic PCA over the traditional
formulation is allowing uncertainty quantification. The parameters of BPCA are
typically learned using mean-field variational inference, and in particular,
the coordinate ascent variational inference (CAVI) algorithm. So far, the
convergence speed of CAVI for BPCA has not been characterized. In our paper, we
fill this gap in the literature. Firstly, we prove a precise exponential
convergence result in the case where the model uses a single principal
component (PC). Interestingly, this result is established through a connection
with the classical $\textit{power iteration algorithm}$ and it indicates that
traditional PCA is retrieved as points estimates of the BPCA parameters.
Secondly, we leverage recent tools to prove exponential convergence of CAVI for
the model with any number of PCs, thus leading to a more general result, but
one that is of a slightly different flavor. To prove the latter result, we
additionally needed to introduce a novel lower bound for the symmetric
Kullback--Leibler divergence between two multivariate normal distributions,
which, we believe, is of independent interest in information theory.",http://arxiv.org/abs/2505.16145v1
Multimodal Online Federated Learning with Modality Missing in Internet of Things,"The Internet of Things (IoT) ecosystem generates vast amounts of multimodal
data from heterogeneous sources such as sensors, cameras, and microphones. As
edge intelligence continues to evolve, IoT devices have progressed from simple
data collection units to nodes capable of executing complex computational
tasks. This evolution necessitates the adoption of distributed learning
strategies to effectively handle multimodal data in an IoT environment.
Furthermore, the real-time nature of data collection and limited local storage
on edge devices in IoT call for an online learning paradigm. To address these
challenges, we introduce the concept of Multimodal Online Federated Learning
(MMO-FL), a novel framework designed for dynamic and decentralized multimodal
learning in IoT environments. Building on this framework, we further account
for the inherent instability of edge devices, which frequently results in
missing modalities during the learning process. We conduct a comprehensive
theoretical analysis under both complete and missing modality scenarios,
providing insights into the performance degradation caused by missing
modalities. To mitigate the impact of modality missing, we propose the
Prototypical Modality Mitigation (PMM) algorithm, which leverages prototype
learning to effectively compensate for missing modalities. Experimental results
on two multimodal datasets further demonstrate the superior performance of PMM
compared to benchmarks.",http://arxiv.org/abs/2505.16138v1
Interpretable Machine Learning for Macro Alpha: A News Sentiment Case Study,"This study introduces an interpretable machine learning (ML) framework to
extract macroeconomic alpha from global news sentiment. We process the Global
Database of Events, Language, and Tone (GDELT) Project's worldwide news feed
using FinBERT -- a Bidirectional Encoder Representations from Transformers
(BERT) based model pretrained on finance-specific language -- to construct
daily sentiment indices incorporating mean tone, dispersion, and event impact.
These indices drive an XGBoost classifier, benchmarked against logistic
regression, to predict next-day returns for EUR/USD, USD/JPY, and 10-year U.S.
Treasury futures (ZN). Rigorous out-of-sample (OOS) backtesting (5-fold
expanding-window cross-validation, OOS period: c. 2017-April 2025) demonstrates
exceptional, cost-adjusted performance for the XGBoost strategy: Sharpe ratios
achieve 5.87 (EUR/USD), 4.65 (USD/JPY), and 4.65 (Treasuries), with respective
compound annual growth rates (CAGRs) exceeding 50% in Foreign Exchange (FX) and
22% in bonds. Shapley Additive Explanations (SHAP) affirm that sentiment
dispersion and article impact are key predictive features. Our findings
establish that integrating domain-specific Natural Language Processing (NLP)
with interpretable ML offers a potent and explainable source of macro alpha.",http://arxiv.org/abs/2505.16136v1
Sudoku-Bench: Evaluating creative reasoning with Sudoku variants,"Existing reasoning benchmarks for large language models (LLMs) frequently
fail to capture authentic creativity, often rewarding memorization of
previously observed patterns. We address this shortcoming with Sudoku-Bench, a
curated benchmark of challenging and unconventional Sudoku variants
specifically selected to evaluate creative, multi-step logical reasoning.
Sudoku variants form an unusually effective domain for reasoning research: each
puzzle introduces unique or subtly interacting constraints, making memorization
infeasible and requiring solvers to identify novel logical breakthroughs
(``break-ins''). Despite their diversity, Sudoku variants maintain a common and
compact structure, enabling clear and consistent evaluation. Sudoku-Bench
includes a carefully chosen puzzle set, a standardized text-based puzzle
representation, and flexible tools compatible with thousands of publicly
available puzzles -- making it easy to extend into a general research
environment. Baseline experiments show that state-of-the-art LLMs solve fewer
than 15\% of puzzles unaided, highlighting significant opportunities to advance
long-horizon, strategic reasoning capabilities.",http://arxiv.org/abs/2505.16135v1
Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models,"Large language models exhibit positional bias -- systematic neglect of
information at specific context positions -- yet its interplay with linguistic
diversity remains poorly understood. We present a cross-linguistic study across
five typologically distinct languages (English, Russian, German, Hindi,
Vietnamese), examining how positional bias interacts with model uncertainty,
syntax, and prompting. Key findings: (1) Positional bias is model-driven, with
language-specific variations -- Qwen2.5-7B favors late positions, challenging
assumptions of early-token bias; (2) Explicit positional guidance (e.g.,
correct context is at position X) reduces accuracy across languages,
undermining prompt-engineering practices; (3) Aligning context with positional
bias increases entropy, yet minimal entropy does not predict accuracy. (4) We
further uncover that LLMs differently impose dominant word order in
free-word-order languages like Hindi.",http://arxiv.org/abs/2505.16134v1
Machine Learning the 6d Supergravity Landscape,"In this paper, we apply both supervised and unsupervised machine learning
algorithms to the study of the string landscape and swampland in 6-dimensions.
Our data are the (almost) anomaly-free 6-dimensional $\mathcal{N} = (1,0)$
supergravity models, characterised by the Gram matrix of anomaly coefficients.
Our work demonstrates the ability of machine learning algorithms to efficiently
learn highly complex features of the landscape and swampland. Employing an
autoencoder for unsupervised learning, we provide an auto-classification of
these models by compressing the Gram matrix data to 2-dimensions. Through
compression, similar models cluster together, and we identify prominent
features of these clusters. The autoencoder also identifies outlier models
which are difficult to reconstruct. One of these outliers proves to be
incredibly difficult to combine with other models such that the
$\text{tr}R^{4}$ anomaly vanishes, making its presence in the landscape
extremely rare. Further, we utilise supervised learning to build two
classifiers predicting (1) model consistency under probe string insertion
(precision: 0.78, predicting consistency for 214,837 models with reasonable
certainty) and (2) inconsistency under anomaly inflow (precision: 0.91,
predicting inconsistency for 1,909,359 models). Notably, projecting these
predictions onto the autoencoder's 2-dimensional latent layer shows consistent
models clustering together, further indicating that the autoencoder has learnt
interesting and complex features of the set of models and potentially offers a
novel approach to mapping the landscape and swampland of 6-dimensional
supergravity theories.",http://arxiv.org/abs/2505.16131v1
Scalable Graph Generative Modeling via Substructure Sequences,"Graph neural networks (GNNs) has been predominantly driven by
message-passing, where node representations are iteratively updated via local
neighborhood aggregation. Despite their success, message-passing suffers from
fundamental limitations -- including constrained expressiveness,
over-smoothing, over-squashing, and limited capacity to model long-range
dependencies. These issues hinder scalability: increasing data size or model
size often fails to yield improved performance, limiting the viability of GNNs
as backbones for graph foundation models. In this work, we explore pathways
beyond message-passing and introduce Generative Graph Pattern Machine
(G$^2$PM), a generative Transformer pre-training framework for graphs. G$^2$PM
represents graph instances (nodes, edges, or entire graphs) as sequences of
substructures, and employs generative pre-training over the sequences to learn
generalizable, transferable representations. Empirically, G$^2$PM demonstrates
strong scalability: on the ogbn-arxiv benchmark, it continues to improve with
model sizes up to 60M parameters, outperforming prior generative approaches
that plateau at significantly smaller scales (e.g., 3M). In addition, we
systematically analyze the model design space, highlighting key architectural
choices that contribute to its scalability and generalization. Across diverse
tasks -- including node classification, graph classification, and transfer
learning -- G$^2$PM consistently outperforms strong baselines, establishing a
compelling foundation for scalable graph learning. The code and dataset are
available at https://github.com/Zehong-Wang/G2PM.",http://arxiv.org/abs/2505.16130v1
Robust Invariant Representation Learning by Distribution Extrapolation,"Invariant risk minimization (IRM) aims to enable out-of-distribution (OOD)
generalization in deep learning by learning invariant representations. As IRM
poses an inherently challenging bi-level optimization problem, most existing
approaches -- including IRMv1 -- adopt penalty-based single-level
approximations. However, empirical studies consistently show that these methods
often fail to outperform well-tuned empirical risk minimization (ERM),
highlighting the need for more robust IRM implementations. This work
theoretically identifies a key limitation common to many IRM variants: their
penalty terms are highly sensitive to limited environment diversity and
over-parameterization, resulting in performance degradation. To address this
issue, a novel extrapolation-based framework is proposed that enhances
environmental diversity by augmenting the IRM penalty through synthetic
distributional shifts. Extensive experiments -- ranging from synthetic setups
to realistic, over-parameterized scenarios -- demonstrate that the proposed
method consistently outperforms state-of-the-art IRM variants, validating its
effectiveness and robustness.",http://arxiv.org/abs/2505.16126v1
Plan and Budget: Effective and Efficient Test-Time Scaling on Large Language Model Reasoning,"Large Language Models (LLMs) have achieved remarkable success in complex
reasoning tasks, but their inference remains computationally inefficient. We
observe a common failure mode in many prevalent LLMs, overthinking, where
models generate verbose and tangential reasoning traces even for simple
queries. Recent works have tried to mitigate this by enforcing fixed token
budgets, however, this can lead to underthinking, especially on harder
problems. Through empirical analysis, we identify that this inefficiency often
stems from unclear problem-solving strategies. To formalize this, we develop a
theoretical model, BBAM (Bayesian Budget Allocation Model), which models
reasoning as a sequence of sub-questions with varying uncertainty, and
introduce the $E^3$ metric to capture the trade-off between correctness and
computation efficiency. Building on theoretical results from BBAM, we propose
Plan-and-Budget, a model-agnostic, test-time framework that decomposes complex
queries into sub-questions and allocates token budgets based on estimated
complexity using adaptive scheduling. Plan-and-Budget improves reasoning
efficiency across a range of tasks and models, achieving up to +70% accuracy
gains, -39% token reduction, and +187.5% improvement in $E^3$. Notably, it
elevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger
model (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close
performance gaps without retraining. Our code is available at
anonymous.4open.science/r/P-and-B-6513/.",http://arxiv.org/abs/2505.16122v1
A Generic Framework for Conformal Fairness,"Conformal Prediction (CP) is a popular method for uncertainty quantification
with machine learning models. While conformal prediction provides probabilistic
guarantees regarding the coverage of the true label, these guarantees are
agnostic to the presence of sensitive attributes within the dataset. In this
work, we formalize \textit{Conformal Fairness}, a notion of fairness using
conformal predictors, and provide a theoretically well-founded algorithm and
associated framework to control for the gaps in coverage between different
sensitive groups. Our framework leverages the exchangeability assumption
(implicit to CP) rather than the typical IID assumption, allowing us to apply
the notion of Conformal Fairness to data types and tasks that are not IID, such
as graph data. Experiments were conducted on graph and tabular datasets to
demonstrate that the algorithm can control fairness-related gaps in addition to
coverage aligned with theoretical expectations.",http://arxiv.org/abs/2505.16115v1
Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools,"Modern Large Language Models (LLMs) often require external tools, such as
machine learning classifiers or knowledge retrieval systems, to provide
accurate answers in domains where their pre-trained knowledge is insufficient.
This integration of LLMs with external tools expands their utility but also
introduces a critical challenge: determining the trustworthiness of responses
generated by the combined system. In high-stakes applications, such as medical
decision-making, it is essential to assess the uncertainty of both the LLM's
generated text and the tool's output to ensure the reliability of the final
response. However, existing uncertainty quantification methods do not account
for the tool-calling scenario, where both the LLM and external tool contribute
to the overall system's uncertainty. In this work, we present a novel framework
for modeling tool-calling LLMs that quantifies uncertainty by jointly
considering the predictive uncertainty of the LLM and the external tool. We
extend previous methods for uncertainty quantification over token sequences to
this setting and propose efficient approximations that make uncertainty
computation practical for real-world applications. We evaluate our framework on
two new synthetic QA datasets, derived from well-known machine learning
datasets, which require tool-calling for accurate answers. Additionally, we
apply our method to retrieval-augmented generation (RAG) systems and conduct a
proof-of-concept experiment demonstrating the effectiveness of our uncertainty
metrics in scenarios where external information retrieval is needed. Our
results show that the framework is effective in enhancing trust in LLM-based
systems, especially in cases where the LLM's internal knowledge is insufficient
and external tools are required.",http://arxiv.org/abs/2505.16113v1
Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models,"With the increasing size of Large Vision-Language Models (LVLMs), network
pruning techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that pruning often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and pruning strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-pruning.",http://arxiv.org/abs/2505.16104v1
Towards Trustworthy Keylogger detection: A Comprehensive Analysis of Ensemble Techniques and Feature Selections through Explainable AI,"Keylogger detection involves monitoring for unusual system behaviors such as
delays between typing and character display, analyzing network traffic patterns
for data exfiltration. In this study, we provide a comprehensive analysis for
keylogger detection with traditional machine learning models - SVC, Random
Forest, Decision Tree, XGBoost, AdaBoost, Logistic Regression and Naive Bayes
and advanced ensemble methods including Stacking, Blending and Voting.
Moreover, feature selection approaches such as Information gain, Lasso L1 and
Fisher Score are thoroughly assessed to improve predictive performance and
lower computational complexity. The Keylogger Detection dataset from publicly
available Kaggle website is used in this project. In addition to accuracy-based
classification, this study implements the approach for model interpretation
using Explainable AI (XAI) techniques namely SHAP (Global) and LIME (Local) to
deliver finer explanations for how much each feature contributes in assisting
or hindering the detection process. To evaluate the models result, we have used
AUC score, sensitivity, Specificity, Accuracy and F1 score. The best
performance was achieved by AdaBoost with 99.76% accuracy, F1 score of 0.99,
100% precision, 98.6% recall, 1.0 specificity and 0.99 of AUC that is
near-perfect classification with Fisher Score.",http://arxiv.org/abs/2505.16103v1
Reinforcement Learning for Stock Transactions,"Much research has been done to analyze the stock market. After all, if one
can determine a pattern in the chaotic frenzy of transactions, then they could
make a hefty profit from capitalizing on these insights. As such, the goal of
our project was to apply reinforcement learning (RL) to determine the best time
to buy a stock within a given time frame. With only a few adjustments, our
model can be extended to identify the best time to sell a stock as well. In
order to use the format of free, real-world data to train the model, we define
our own Markov Decision Process (MDP) problem. These two papers [5] [6] helped
us in formulating the state space and the reward system of our MDP problem. We
train a series of agents using Q-Learning, Q-Learning with linear function
approximation, and deep Q-Learning. In addition, we try to predict the stock
prices using machine learning regression and classification models. We then
compare our agents to see if they converge on a policy, and if so, which one
learned the best policy to maximize profit on the stock market.",http://arxiv.org/abs/2505.16099v1
Dimension-adapted Momentum Outscales SGD,"We investigate scaling laws for stochastic momentum algorithms with small
batch on the power law random features model, parameterized by data complexity,
target complexity, and model size. When trained with a stochastic momentum
algorithm, our analysis reveals four distinct loss curve shapes determined by
varying data-target complexities. While traditional stochastic gradient descent
with momentum (SGD-M) yields identical scaling law exponents to SGD,
dimension-adapted Nesterov acceleration (DANA) improves these exponents by
scaling momentum hyperparameters based on model size and data complexity. This
outscaling phenomenon, which also improves compute-optimal scaling behavior, is
achieved by DANA across a broad range of data and target complexities, while
traditional methods fall short. Extensive experiments on high-dimensional
synthetic quadratics validate our theoretical predictions and large-scale text
experiments with LSTMs show DANA's improved loss exponents over SGD hold in a
practical setting.",http://arxiv.org/abs/2505.16098v1
A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization,"Large language models (LLMs) are introducing a paradigm shift in molecular
discovery by enabling text-guided interaction with chemical spaces through
natural language, symbolic notations, with emerging extensions to incorporate
multi-modal inputs. To advance the new field of LLM for molecular discovery,
this survey provides an up-to-date and forward-looking review of the emerging
use of LLMs for two central tasks: molecule generation and molecule
optimization. Based on our proposed taxonomy for both problems, we analyze
representative techniques in each category, highlighting how LLM capabilities
are leveraged across different learning settings. In addition, we include the
commonly used datasets and evaluation protocols. We conclude by discussing key
challenges and future directions, positioning this survey as a resource for
researchers working at the intersection of LLMs and molecular science. A
continuously updated reading list is available at
https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.",http://arxiv.org/abs/2505.16094v1
FR-Mamba: Time-Series Physical Field Reconstruction Based on State Space Model,"Physical field reconstruction (PFR) aims to predict the state distribution of
physical quantities (e.g., velocity, pressure, and temperature) based on
limited sensor measurements. It plays a critical role in domains such as fluid
dynamics and thermodynamics. However, existing deep learning methods often fail
to capture long-range temporal dependencies, resulting in suboptimal
performance on time-evolving physical systems. To address this, we propose
FR-Mamba, a novel spatiotemporal flow field reconstruction framework based on
state space modeling. Specifically, we design a hybrid neural network
architecture that combines Fourier Neural Operator (FNO) and State Space Model
(SSM) to capture both global spatial features and long-range temporal
dependencies. We adopt Mamba, a recently proposed efficient SSM architecture,
to model long-range temporal dependencies with linear time complexity. In
parallel, the FNO is employed to capture non-local spatial features by
leveraging frequency-domain transformations. The spatiotemporal representations
extracted by these two components are then fused to reconstruct the full-field
distribution of the physical system. Extensive experiments demonstrate that our
approach significantly outperforms existing PFR methods in flow field
reconstruction tasks, achieving high-accuracy performance on long sequences.",http://arxiv.org/abs/2505.16083v1
Oh SnapMMD! Forecasting Stochastic Dynamics Beyond the Schrödinger Bridge's End,"Scientists often want to make predictions beyond the observed time horizon of
""snapshot"" data following latent stochastic dynamics. For example, in time
course single-cell mRNA profiling, scientists have access to cellular
transcriptional state measurements (snapshots) from different biological
replicates at different time points, but they cannot access the trajectory of
any one cell because measurement destroys the cell. Researchers want to
forecast (e.g.) differentiation outcomes from early state measurements of stem
cells. Recent Schr\""odinger-bridge (SB) methods are natural for interpolating
between snapshots. But past SB papers have not addressed forecasting -- likely
since existing methods either (1) reduce to following pre-set reference
dynamics (chosen before seeing data) or (2) require the user to choose a fixed,
state-independent volatility since they minimize a Kullback-Leibler divergence.
Either case can lead to poor forecasting quality. In the present work, we
propose a new framework, SnapMMD, that learns dynamics by directly fitting the
joint distribution of both state measurements and observation time with a
maximum mean discrepancy (MMD) loss. Unlike past work, our method allows us to
infer unknown and state-dependent volatilities from the observed data. We show
in a variety of real and synthetic experiments that our method delivers
accurate forecasts. Moreover, our approach allows us to learn in the presence
of incomplete state measurements and yields an $R^2$-style statistic that
diagnoses fit. We also find that our method's performance at interpolation (and
general velocity-field reconstruction) is at least as good as (and often better
than) state-of-the-art in almost all of our experiments.",http://arxiv.org/abs/2505.16082v1
Ensembling Sparse Autoencoders,"Sparse autoencoders (SAEs) are used to decompose neural network activations
into human-interpretable features. Typically, features learned by a single SAE
are used for downstream applications. However, it has recently been shown that
SAEs trained with different initial weights can learn different features,
demonstrating that a single SAE captures only a limited subset of features that
can be extracted from the activation space. Motivated by this limitation, we
propose to ensemble multiple SAEs through naive bagging and boosting.
Specifically, SAEs trained with different weight initializations are ensembled
in naive bagging, whereas SAEs sequentially trained to minimize the residual
error are ensembled in boosting. We evaluate our ensemble approaches with three
settings of language models and SAE architectures. Our empirical results
demonstrate that ensembling SAEs can improve the reconstruction of language
model activations, diversity of features, and SAE stability. Furthermore,
ensembling SAEs performs better than applying a single SAE on downstream tasks
such as concept detection and spurious correlation removal, showing improved
practical utility.",http://arxiv.org/abs/2505.16077v1
Bidirectional Variational Autoencoders,"We present the new bidirectional variational autoencoder (BVAE) network
architecture. The BVAE uses a single neural network both to encode and decode
instead of an encoder-decoder network pair. The network encodes in the forward
direction and decodes in the backward direction through the same synaptic web.
Simulations compared BVAEs and ordinary VAEs on the four image tasks of image
reconstruction, classification, interpolation, and generation. The image
datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and
CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter
count by almost 50% and still slightly outperformed the unidirectional VAEs.",http://arxiv.org/abs/2505.16074v1
Merge to Mix: Mixing Datasets via Model Merging,"Mixing datasets for fine-tuning large models (LMs) has become critical for
maximizing performance on downstream tasks. However, composing effective
dataset mixtures typically relies on heuristics and trial-and-error, often
requiring multiple fine-tuning runs to achieve the desired outcome. We propose
a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset
mixtures through model merging. Model merging is a recent technique that
combines the abilities of multiple individually fine-tuned LMs into a single LM
by using a few simple arithmetic operations. Our key insight is that merging
models individually fine-tuned on each dataset in a mixture can effectively
serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix
leverages this insight to accelerate selecting dataset mixtures without
requiring full fine-tuning on each candidate mixture. Our experiments
demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset
selection for fine-tuning LMs.",http://arxiv.org/abs/2505.16066v1
Few-Shot Test-Time Optimization Without Retraining for Semiconductor Recipe Generation and Beyond,"We introduce Model Feedback Learning (MFL), a novel test-time optimization
framework for optimizing inputs to pre-trained AI models or deployed hardware
systems without requiring any retraining of the models or modifications to the
hardware. In contrast to existing methods that rely on adjusting model
parameters, MFL leverages a lightweight reverse model to iteratively search for
optimal inputs, enabling efficient adaptation to new objectives under
deployment constraints. This framework is particularly advantageous in
real-world settings, such as semiconductor manufacturing recipe generation,
where modifying deployed systems is often infeasible or cost-prohibitive. We
validate MFL on semiconductor plasma etching tasks, where it achieves target
recipe generation in just five iterations, significantly outperforming both
Bayesian optimization and human experts. Beyond semiconductor applications, MFL
also demonstrates strong performance in chemical processes (e.g., chemical
vapor deposition) and electronic systems (e.g., wire bonding), highlighting its
broad applicability. Additionally, MFL incorporates stability-aware
optimization, enhancing robustness to process variations and surpassing
conventional supervised learning and random search methods in high-dimensional
control settings. By enabling few-shot adaptation, MFL provides a scalable and
efficient paradigm for deploying intelligent control in real-world
environments.",http://arxiv.org/abs/2505.16060v1
Mesh-free sparse identification of nonlinear dynamics,"Identifying the governing equations of a dynamical system is one of the most
important tasks for scientific modeling. However, this procedure often requires
high-quality spatio-temporal data uniformly sampled on structured grids. In
this paper, we propose mesh-free SINDy, a novel algorithm which leverages the
power of neural network approximation as well as auto-differentiation to
identify governing equations from arbitrary sensor placements and non-uniform
temporal data sampling. We show that mesh-free SINDy is robust to high noise
levels and limited data while remaining computationally efficient. In our
implementation, the training procedure is straight-forward and nearly free of
hyperparameter tuning, making mesh-free SINDy widely applicable to many
scientific and engineering problems. In the experiments, we demonstrate its
effectiveness on a series of PDEs including the Burgers' equation, the heat
equation, the Korteweg-De Vries equation and the 2D advection-diffusion
equation. We conduct detailed numerical experiments on all datasets, varying
the noise levels and number of samples, and we also compare our approach to
previous state-of-the-art methods. It is noteworthy that, even in high-noise
and low-data scenarios, mesh-free SINDy demonstrates robust PDE discovery,
achieving successful identification with up to 75% noise for the Burgers'
equation using 5,000 samples and with as few as 100 samples and 1% noise. All
of this is achieved within a training time of under one minute.",http://arxiv.org/abs/2505.16058v1
Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models,"Mixture-of-Experts (MoE) enables efficient scaling of large language models
(LLMs) with sparsely activated experts during inference. To effectively deploy
large MoE models on memory-constrained devices, many systems introduce *expert
offloading* that caches a subset of experts in fast memory, leaving others on
slow memory to run on CPU or load on demand. While some research has exploited
the locality of expert activations, where consecutive tokens activate similar
experts, the degree of this **local routing consistency** varies across models
and remains understudied. In this paper, we propose two metrics to measure
local routing consistency of MoE models: (1) **Segment Routing Best Performance
(SRP)**, which evaluates how well a fixed group of experts can cover the needs
of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which
measures the optimal segment-level cache hit rate under a given cache size
limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found
that models that apply MoE on every layer and do not use shared experts exhibit
the highest local routing consistency. We further showed that
domain-specialized experts contribute more to routing consistency than
vocabulary-specialized ones, and that most models can balance between cache
effectiveness and efficiency with cache sizes approximately 2x the active
experts. These findings pave the way for memory-efficient MoE design and
deployment without compromising inference speed. We publish the code for
replicating experiments at https://github.com/ljcleo/moe-lrc .",http://arxiv.org/abs/2505.16056v1
Learning from Algorithm Feedback: One-Shot SAT Solver Guidance with GNNs,"Boolean Satisfiability (SAT) solvers are foundational to computer science,
yet their performance typically hinges on hand-crafted heuristics. This work
introduces Reinforcement Learning from Algorithm Feedback (RLAF) as a paradigm
for learning to guide SAT solver branching heuristics with Graph Neural
Networks (GNNs). Central to our approach is a novel and generic mechanism for
injecting inferred variable weights and polarities into the branching
heuristics of existing SAT solvers. In a single forward pass, a GNN assigns
these parameters to all variables. Casting this one-shot guidance as a
reinforcement learning problem lets us train the GNN with off-the-shelf
policy-gradient methods, such as GRPO, directly using the solver's
computational cost as the sole reward signal. Extensive evaluations demonstrate
that RLAF-trained policies significantly reduce the mean solve times of
different base solvers across diverse SAT problem distributions, achieving more
than a 2x speedup in some cases, while generalizing effectively to larger and
harder problems after training. Notably, these policies consistently outperform
expert-supervised approaches based on learning handcrafted weighting
heuristics, offering a promising path towards data-driven heuristic design in
combinatorial optimization.",http://arxiv.org/abs/2505.16053v1
PO-Flow: Flow-based Generative Models for Sampling Potential Outcomes and Counterfactuals,"We propose PO-Flow, a novel continuous normalizing flow (CNF) framework for
causal inference that jointly models potential outcomes and counterfactuals.
Trained via flow matching, PO-Flow provides a unified framework for
individualized potential outcome prediction, counterfactual predictions, and
uncertainty-aware density learning. Among generative models, it is the first to
enable density learning of potential outcomes without requiring explicit
distributional assumptions (e.g., Gaussian mixtures), while also supporting
counterfactual prediction conditioned on factual outcomes in general
observational datasets. On benchmarks such as ACIC, IHDP, and IBM, it
consistently outperforms prior methods across a range of causal inference
tasks. Beyond that, PO-Flow succeeds in high-dimensional settings, including
counterfactual image generation, demonstrating its broad applicability.",http://arxiv.org/abs/2505.16051v1
Multimodal Biomarkers for Schizophrenia: Towards Individual Symptom Severity Estimation,"Studies on schizophrenia assessments using deep learning typically treat it
as a classification task to detect the presence or absence of the disorder,
oversimplifying the condition and reducing its clinical applicability. This
traditional approach overlooks the complexity of schizophrenia, limiting its
practical value in healthcare settings. This study shifts the focus to
individual symptom severity estimation using a multimodal approach that
integrates speech, video, and text inputs. We develop unimodal models for each
modality and a multimodal framework to improve accuracy and robustness. By
capturing a more detailed symptom profile, this approach can help in enhancing
diagnostic precision and support personalized treatment, offering a scalable
and objective tool for mental health assessment.",http://arxiv.org/abs/2505.16044v1
Physics-based machine learning for mantle convection simulations,"Mantle convection simulations are an essential tool for understanding how
rocky planets evolve. However, the poorly known input parameters to these
simulations, the non-linear dependence of transport properties on pressure and
temperature, and the long integration times in excess of several billion years
all pose a computational challenge for numerical solvers. We propose a
physics-based machine learning approach that predicts creeping flow velocities
as a function of temperature while conserving mass, thereby bypassing the
numerical solution of the Stokes problem. A finite-volume solver then uses the
predicted velocities to advect and diffuse the temperature field to the next
time-step, enabling autoregressive rollout at inference. For training, our
model requires temperature-velocity snapshots from a handful of simulations
(94). We consider mantle convection in a two-dimensional rectangular box with
basal and internal heating, pressure- and temperature-dependent viscosity.
Overall, our model is up to 89 times faster than the numerical solver. We also
show the importance of different components in our convolutional neural network
architecture such as mass conservation, learned paddings on the boundaries, and
loss scaling for the overall rollout performance. Finally, we test our approach
on unseen scenarios to demonstrate some of its strengths and weaknesses.",http://arxiv.org/abs/2505.16041v1
Causal LLM Routing: End-to-End Regret Minimization from Observational Data,"LLM routing aims to select the most appropriate model for each query,
balancing competing performance metrics such as accuracy and cost across a pool
of language models. Prior approaches typically adopt a decoupled strategy,
where the metrics are first predicted and the model is then selected based on
these estimates. This setup is prone to compounding errors and often relies on
full-feedback data, where each query is evaluated by all candidate models,
which is costly to obtain and maintain in practice. In contrast, we learn from
observational data, which records only the outcome of the model actually
deployed. We propose a causal end-to-end framework that learns routing policies
by minimizing decision-making regret from observational data. To enable
efficient optimization, we introduce two theoretically grounded surrogate
objectives: a classification-based upper bound, and a softmax-weighted regret
approximation shown to recover the optimal policy at convergence. We further
extend our framework to handle heterogeneous cost preferences via an
interval-conditioned architecture. Experiments on public benchmarks show that
our method outperforms existing baselines, achieving state-of-the-art
performance across different embedding models.",http://arxiv.org/abs/2505.16037v1
"Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces","We introduce Equivariant Neural Eikonal Solvers, a novel framework that
integrates Equivariant Neural Fields (ENFs) with Neural Eikonal Solvers. Our
approach employs a single neural field where a unified shared backbone is
conditioned on signal-specific latent variables - represented as point clouds
in a Lie group - to model diverse Eikonal solutions. The ENF integration
ensures equivariant mapping from these latent representations to the solution
field, delivering three key benefits: enhanced representation efficiency
through weight-sharing, robust geometric grounding, and solution steerability.
This steerability allows transformations applied to the latent point cloud to
induce predictable, geometrically meaningful modifications in the resulting
Eikonal solution. By coupling these steerable representations with
Physics-Informed Neural Networks (PINNs), our framework accurately models
Eikonal travel-time solutions while generalizing to arbitrary Riemannian
manifolds with regular group actions. This includes homogeneous spaces such as
Euclidean, position-orientation, spherical, and hyperbolic manifolds. We
validate our approach through applications in seismic travel-time modeling of
2D and 3D benchmark datasets. Experimental results demonstrate superior
performance, scalability, adaptability, and user controllability compared to
existing Neural Operator-based Eikonal solver methods.",http://arxiv.org/abs/2505.16035v1
Toward Theoretical Insights into Diffusion Trajectory Distillation via Operator Merging,"Diffusion trajectory distillation methods aim to accelerate sampling in
diffusion models, which produce high-quality outputs but suffer from slow
sampling speeds. These methods train a student model to approximate the
multi-step denoising process of a pretrained teacher model in a single step,
enabling one-shot generation. However, theoretical insights into the trade-off
between different distillation strategies and generative quality remain
limited, complicating their optimization and selection. In this work, we take a
first step toward addressing this gap. Specifically, we reinterpret trajectory
distillation as an operator merging problem in the linear regime, where each
step of the teacher model is represented as a linear operator acting on noisy
data. These operators admit a clear geometric interpretation as projections and
rescalings corresponding to the noise schedule. During merging, signal
shrinkage occurs as a convex combination of operators, arising from both
discretization and limited optimization time of the student model. We propose a
dynamic programming algorithm to compute the optimal merging strategy that
maximally preserves signal fidelity. Additionally, we demonstrate the existence
of a sharp phase transition in the optimal strategy, governed by data
covariance structures. Our findings enhance the theoretical understanding of
diffusion trajectory distillation and offer practical insights for improving
distillation strategies.",http://arxiv.org/abs/2505.16024v1
NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning,"Recent advances such as DeepSeek R1-Zero highlight the effectiveness of
incentive training, a reinforcement learning paradigm that computes rewards
solely based on the final answer part of a language model's output, thereby
encouraging the generation of intermediate reasoning steps. However, these
methods fundamentally rely on external verifiers, which limits their
applicability to domains like mathematics and coding where such verifiers are
readily available. Although reward models can serve as verifiers, they require
high-quality annotated data and are costly to train. In this work, we propose
NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning
framework that requires only standard supervised fine-tuning data with no need
for an external verifier. NOVER enables incentive training across a wide range
of text-to-text tasks and outperforms the model of the same size distilled from
large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the
flexibility of NOVER enables new possibilities for optimizing large language
models, such as inverse incentive training.",http://arxiv.org/abs/2505.16022v1
GradPCA: Leveraging NTK Alignment for Reliable Out-of-Distribution Detection,"We introduce GradPCA, an Out-of-Distribution (OOD) detection method that
exploits the low-rank structure of neural network gradients induced by Neural
Tangent Kernel (NTK) alignment. GradPCA applies Principal Component Analysis
(PCA) to gradient class-means, achieving more consistent performance than
existing methods across standard image classification benchmarks. We provide a
theoretical perspective on spectral OOD detection in neural networks to support
GradPCA, highlighting feature-space properties that enable effective detection
and naturally emerge from NTK alignment. Our analysis further reveals that
feature quality -- particularly the use of pretrained versus non-pretrained
representations -- plays a crucial role in determining which detectors will
succeed. Extensive experiments validate the strong performance of GradPCA, and
our theoretical framework offers guidance for designing more principled
spectral OOD detectors.",http://arxiv.org/abs/2505.16017v1
Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations,"Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-sparsity tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.",http://arxiv.org/abs/2505.16004v1
Towards Identifiability of Interventional Stochastic Differential Equations,"We study identifiability of stochastic differential equation (SDE) models
under multiple interventions. Our results give the first provable bounds for
unique recovery of SDE parameters given samples from their stationary
distributions. We give tight bounds on the number of necessary interventions
for linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime.
We experimentally validate the recovery of true parameters in synthetic data,
and motivated by our theoretical results, demonstrate the advantage of
parameterizations with learnable activation functions.",http://arxiv.org/abs/2505.15987v1
"Diffusion Probabilistic Generative Models for Accelerated, in-NICU Permanent Magnet Neonatal MRI","Purpose: Magnetic Resonance Imaging (MRI) enables non-invasive assessment of
brain abnormalities during early life development. Permanent magnet scanners
operating in the neonatal intensive care unit (NICU) facilitate MRI of sick
infants, but have long scan times due to lower signal-to-noise ratios (SNR) and
limited receive coils. This work accelerates in-NICU MRI with diffusion
probabilistic generative models by developing a training pipeline accounting
for these challenges.
  Methods: We establish a novel training dataset of clinical, 1 Tesla neonatal
MR images in collaboration with Aspect Imaging and Sha'are Zedek Medical
Center. We propose a pipeline to handle the low quantity and SNR of our
real-world dataset (1) modifying existing network architectures to support
varying resolutions; (2) training a single model on all data with learned class
embedding vectors; (3) applying self-supervised denoising before training; and
(4) reconstructing by averaging posterior samples. Retrospective under-sampling
experiments, accounting for signal decay, evaluated each item of our proposed
methodology. A clinical reader study with practicing pediatric
neuroradiologists evaluated our proposed images reconstructed from 1.5x
under-sampled data.
  Results: Combining all data, denoising pre-training, and averaging posterior
samples yields quantitative improvements in reconstruction. The generative
model decouples the learned prior from the measurement model and functions at
two acceleration rates without re-training. The reader study suggests that
proposed images reconstructed from approximately 1.5x under-sampled data are
adequate for clinical use.
  Conclusion: Diffusion probabilistic generative models applied with the
proposed pipeline to handle challenging real-world datasets could reduce scan
time of in-NICU neonatal MRI.",http://arxiv.org/abs/2505.15984v1
"Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach","College students are increasingly affected by stress, anxiety, and
depression, yet face barriers to traditional mental health care. This study
evaluated the efficacy of a mobile health (mHealth) intervention, Mental Health
Evaluation and Lookout Program (mHELP), which integrates a smartwatch sensor
and machine learning (ML) algorithms for real-time stress detection and
self-management. In a 12-week randomized controlled trial (n = 117),
participants were assigned to a treatment group using mHELP's full suite of
interventions or a control group using the app solely for real-time stress
logging and weekly psychological assessments. The primary outcome, ""Moments of
Stress"" (MS), was assessed via physiological and self-reported indicators and
analyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly,
secondary outcomes of psychological assessments, including the Generalized
Anxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire
(PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also
analyzed via GLMM. The finding of the objective measure, MS, indicates a
substantial decrease in MS among the treatment group compared to the control
group, while no notable between-group differences were observed in subjective
scores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the
treatment group exhibited a clinically meaningful decline in GAD-7 and PSS
scores. These findings underscore the potential of wearable-enabled mHealth
tools to reduce acute stress in college populations and highlight the need for
extended interventions and tailored features to address chronic symptoms like
depression.",http://arxiv.org/abs/2505.15974v1
Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders,"The ImageNet hierarchy provides a structured taxonomy of object categories,
offering a valuable lens through which to analyze the representations learned
by deep vision models. In this work, we conduct a comprehensive analysis of how
vision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders
(SAEs) to probe their internal representations. SAEs have been widely used as
an explanation tool for large language models (LLMs), where they enable the
discovery of semantically meaningful features. Here, we extend their use to
vision models to investigate whether learned representations align with the
ontological structure defined by the ImageNet taxonomy. Our results show that
SAEs uncover hierarchical relationships in model activations, revealing an
implicit encoding of taxonomic structure. We analyze the consistency of these
representations across different layers of the popular vision foundation model
DINOv2 and provide insights into how deep vision models internalize
hierarchical category information by increasing information in the class token
through each layer. Our study establishes a framework for systematic
hierarchical analysis of vision model representations and highlights the
potential of SAEs as a tool for probing semantic structure in deep networks.",http://arxiv.org/abs/2505.15970v1
Pre-training Large Memory Language Models with Internal and External Knowledge,"Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.",http://arxiv.org/abs/2505.15962v1
Data-driven Verification of Procedural Programs with Integer Arrays,"We address the problem of verifying automatically procedural programs
manipulating parametric-size arrays of integers, encoded as a constrained Horn
clauses solving problem. We propose a new algorithmic method for synthesizing
loop invariants and procedure pre/post-conditions represented as universally
quantified first-order formulas constraining the array elements and program
variables. We adopt a data-driven approach that extends the decision tree
Horn-ICE framework to handle arrays. We provide a powerful learning technique
based on reducing a complex classification problem of vectors of integer arrays
to a simpler classification problem of vectors of integers. The obtained
classifier is generalized to get universally quantified invariants and
procedure pre/post-conditions. We have implemented our method and shown its
efficiency and competitiveness w.r.t. state-of-the-art tools on a significant
benchmark.",http://arxiv.org/abs/2505.15958v1
MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding,"Decoding visual experiences from fMRI offers a powerful avenue to understand
human perception and develop advanced brain-computer interfaces. However,
current progress often prioritizes maximizing reconstruction fidelity while
overlooking interpretability, an essential aspect for deriving neuroscientific
insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework
designed for high-fidelity, adaptable, and interpretable visual reconstruction.
MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture
where distinct experts process fMRI signals from functionally related voxel
groups, mimicking specialized brain networks. The experts are first trained to
encode fMRI into the frozen CLIP space. A finetuned diffusion model then
synthesizes images, guided by expert outputs through a novel dual-stage routing
mechanism that dynamically weighs expert contributions across the diffusion
process. MoRE-Brain offers three main advancements: First, it introduces a
novel Mixture-of-Experts architecture grounded in brain network principles for
neuro-decoding. Second, it achieves efficient cross-subject generalization by
sharing core expert networks while adapting only subject-specific routers.
Third, it provides enhanced mechanistic insight, as the explicit routing
reveals precisely how different modeled brain regions shape the semantic and
spatial attributes of the reconstructed image. Extensive experiments validate
MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further
demonstrating its effective utilization of fMRI signals, distinguishing genuine
neural decoding from over-reliance on generative priors. Consequently,
MoRE-Brain marks a substantial advance towards more generalizable and
interpretable fMRI-based visual decoding. Code will be publicly available soon:
https://github.com/yuxiangwei0808/MoRE-Brain.",http://arxiv.org/abs/2505.15946v1
Improving the Predictability of the Madden-Julian Oscillation at Subseasonal Scales with Gaussian Process Models,"The Madden--Julian Oscillation (MJO) is an influential climate phenomenon
that plays a vital role in modulating global weather patterns. In spite of the
improvement in MJO predictions made by machine learning algorithms, such as
neural networks, most of them cannot provide the uncertainty levels in the MJO
forecasts directly. To address this problem, we develop a nonparametric
strategy based on Gaussian process (GP) models. We calibrate GPs using
empirical correlations and we propose a posteriori covariance correction.
Numerical experiments demonstrate that our model has better prediction skills
than the ANN models for the first five lead days. Additionally, our posteriori
covariance correction extends the probabilistic coverage by more than three
weeks.",http://arxiv.org/abs/2505.15934v1
AllMetrics: A Unified Python Library for Standardized Metric Evaluation and Robust Data Validation in Machine Learning,"Machine learning (ML) models rely heavily on consistent and accurate
performance metrics to evaluate and compare their effectiveness. However,
existing libraries often suffer from fragmentation, inconsistent
implementations, and insufficient data validation protocols, leading to
unreliable results. Existing libraries have often been developed independently
and without adherence to a unified standard, particularly concerning the
specific tasks they aim to support. As a result, each library tends to adopt
its conventions for metric computation, input/output formatting, error
handling, and data validation protocols. This lack of standardization leads to
both implementation differences (ID) and reporting differences (RD), making it
difficult to compare results across frameworks or ensure reliable evaluations.
To address these issues, we introduce AllMetrics, an open-source unified Python
library designed to standardize metric evaluation across diverse ML tasks,
including regression, classification, clustering, segmentation, and
image-to-image translation. The library implements class-specific reporting for
multi-class tasks through configurable parameters to cover all use cases, while
incorporating task-specific parameters to resolve metric computation
discrepancies across implementations. Various datasets from domains like
healthcare, finance, and real estate were applied to our library and compared
with Python, Matlab, and R components to identify which yield similar results.
AllMetrics combines a modular Application Programming Interface (API) with
robust input validation mechanisms to ensure reproducibility and reliability in
model evaluation. This paper presents the design principles, architectural
components, and empirical analyses demonstrating the ability to mitigate
evaluation errors and to enhance the trustworthiness of ML workflows.",http://arxiv.org/abs/2505.15931v1
CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision,"Learning complex functions that involve multi-step reasoning poses a
significant challenge for standard supervised learning from input-output
examples. Chain-of-thought (CoT) supervision, which provides intermediate
reasoning steps together with the final output, has emerged as a powerful
empirical technique, underpinning much of the recent progress in the reasoning
capabilities of large language models. This paper develops a statistical theory
of learning under CoT supervision. A key characteristic of the CoT setting, in
contrast to standard supervision, is the mismatch between the training
objective (CoT risk) and the test objective (end-to-end risk). A central part
of our analysis, distinguished from prior work, is explicitly linking those two
types of risk to achieve sharper sample complexity bounds. This is achieved via
the *CoT information measure* $\mathcal{I}_{\mathcal{D},
h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, which quantifies the additional
discriminative power gained from observing the reasoning process. The main
theoretical results demonstrate how CoT supervision can yield significantly
faster learning rates compared to standard E2E supervision. Specifically, it is
shown that the sample complexity required to achieve a target E2E error
$\epsilon$ scales as $d/\mathcal{I}_{\mathcal{D},
h_\star}^{\mathrm{CoT}}(\epsilon; \calH)$, where $d$ is a measure of hypothesis
class complexity, which can be much faster than standard $d/\epsilon$ rates.
Information-theoretic lower bounds in terms of the CoT information are also
obtained. Together, these results suggest that CoT information is a fundamental
measure of statistical complexity for learning under chain-of-thought
supervision.",http://arxiv.org/abs/2505.15927v1
Is (Selective) Round-To-Nearest Quantization All You Need?,"Quantization became a necessary tool for serving ever-increasing Large
Language Models (LLMs). RTN (Round-to-Nearest) is perhaps the simplest
quantization technique that has been around well before LLMs surged to the
forefront of machine learning (ML) research. Yet, it has been largely dismissed
by recent and more advanced quantization methods that claim superiority over
RTN in nearly every aspect of performance. This work aims to dispel this
established point of view, showing that RTN is not only much cheaper to apply,
but also its token generation throughput can be better than and accuracy can be
similar to more advanced alternatives. In particular, we discuss our
implementation of RTN based on the recent Marlin kernels and demonstrate how
the accuracy of RTN can be gradually improved by selectively increasing the
data precision format of certain model layers and modules. Based on our
results, we argue that RTN presents a viable and practical choice for
quantizing LLMs.",http://arxiv.org/abs/2505.15909v1
Last Layer Empirical Bayes,"The task of quantifying the inherent uncertainty associated with neural
network predictions is a key challenge in artificial intelligence. Bayesian
neural networks (BNNs) and deep ensembles are among the most prominent
approaches to tackle this task. Both approaches produce predictions by
computing an expectation of neural network outputs over some distribution on
the corresponding weights; this distribution is given by the posterior in the
case of BNNs, and by a mixture of point masses for ensembles. Inspired by
recent work showing that the distribution used by ensembles can be understood
as a posterior corresponding to a learned data-dependent prior, we propose last
layer empirical Bayes (LLEB). LLEB instantiates a learnable prior as a
normalizing flow, which is then trained to maximize the evidence lower bound;
to retain tractability we use the flow only on the last layer. We show why LLEB
is well motivated, and how it interpolates between standard BNNs and ensembles
in terms of the strength of the prior that they use. LLEB performs on par with
existing approaches, highlighting that empirical Bayes is a promising direction
for future research in uncertainty quantification.",http://arxiv.org/abs/2505.15888v1
Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex,"Understanding functional representations within higher visual cortex is a
fundamental question in computational neuroscience. While artificial neural
networks pretrained on large-scale datasets exhibit striking representational
alignment with human neural responses, learning image-computable models of
visual cortex relies on individual-level, large-scale fMRI datasets. The
necessity for expensive, time-intensive, and often impractical data acquisition
limits the generalizability of encoders to new subjects and stimuli. BraInCoRL
uses in-context learning to predict voxelwise neural responses from few-shot
examples without any additional finetuning for novel subjects and stimuli. We
leverage a transformer architecture that can flexibly condition on a variable
number of in-context image stimuli, learning an inductive bias over multiple
subjects. During training, we explicitly optimize the model for in-context
learning. By jointly conditioning on image features and voxel activations, our
model learns to directly generate better performing voxelwise models of higher
visual cortex. We demonstrate that BraInCoRL consistently outperforms existing
voxelwise encoder designs in a low-data regime when evaluated on entirely novel
images, while also exhibiting strong test-time scaling behavior. The model also
generalizes to an entirely new visual fMRI dataset, which uses different
subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates
better interpretability of neural signals in higher visual cortex by attending
to semantically relevant stimuli. Finally, we show that our framework enables
interpretable mappings from natural language queries to voxel selectivity.",http://arxiv.org/abs/2505.15813v1
On the creation of narrow AI: hierarchy and nonlocality of neural network skills,"We study the problem of creating strong, yet narrow, AI systems. While recent
AI progress has been driven by the training of large general-purpose foundation
models, the creation of smaller models specialized for narrow domains could be
valuable for both efficiency and safety. In this work, we explore two
challenges involved in creating such systems, having to do with basic
properties of how neural networks learn and structure their representations.
The first challenge regards when it is possible to train narrow models from
scratch. Through experiments on a synthetic task, we find that it is sometimes
necessary to train networks on a wide distribution of data to learn certain
narrow skills within that distribution. This effect arises when skills depend
on each other hierarchically, and training on a broad distribution introduces a
curriculum which substantially accelerates learning. The second challenge
regards how to transfer particular skills from large general models into small
specialized models. We find that model skills are often not perfectly localized
to a particular set of prunable components. However, we find that methods based
on pruning can still outperform distillation. We investigate the use of a
regularization objective to align desired skills with prunable components while
unlearning unnecessary skills.",http://arxiv.org/abs/2505.15811v1
Neural Conditional Transport Maps,"We present a neural framework for learning conditional optimal transport (OT)
maps between probability distributions. Our approach introduces a conditioning
mechanism capable of processing both categorical and continuous conditioning
variables simultaneously. At the core of our method lies a hypernetwork that
generates transport layer parameters based on these inputs, creating adaptive
mappings that outperform simpler conditioning methods. Comprehensive ablation
studies demonstrate the superior performance of our method over baseline
configurations. Furthermore, we showcase an application to global sensitivity
analysis, offering high performance in computing OT-based sensitivity indices.
This work advances the state-of-the-art in conditional optimal transport,
enabling broader application of optimal transport principles to complex,
high-dimensional domains such as generative modeling and black-box model
explainability.",http://arxiv.org/abs/2505.15808v1
The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation,"Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.",http://arxiv.org/abs/2505.15807v1
Adaptive Estimation and Learning under Temporal Distribution Shift,"In this paper, we study the problem of estimation and learning under temporal
distribution shift. Consider an observation sequence of length $n$, which is a
noisy realization of a time-varying groundtruth sequence. Our focus is to
develop methods to estimate the groundtruth at the final time-step while
providing sharp point-wise estimation error rates. We show that, without prior
knowledge on the level of temporal shift, a wavelet soft-thresholding estimator
provides an optimal estimation error bound for the groundtruth. Our proposed
estimation method generalizes existing researches Mazzetto and Upfal (2023) by
establishing a connection between the sequence's non-stationarity level and the
sparsity in the wavelet-transformed domain. Our theoretical findings are
validated by numerical experiments. Additionally, we applied the estimator to
derive sparsity-aware excess risk bounds for binary classification under
distribution shift and to develop computationally efficient training
objectives. As a final contribution, we draw parallels between our results and
the classical signal processing problem of total-variation denoising (Mammen
and van de Geer,1997; Tibshirani, 2014), uncovering novel optimal algorithms
for such task.",http://arxiv.org/abs/2505.15803v1
"A Deep Learning Framework for Two-Dimensional, Multi-Frequency Propagation Factor Estimation","Accurately estimating the refractive environment over multiple frequencies
within the marine atmospheric boundary layer is crucial for the effective
deployment of radar technologies. Traditional parabolic equation simulations,
while effective, can be computationally expensive and time-intensive, limiting
their practical application. This communication explores a novel approach using
deep neural networks to estimate the pattern propagation factor, a critical
parameter for characterizing environmental impacts on signal propagation.
Image-to-image translation generators designed to ingest modified refractivity
data and generate predictions of pattern propagation factors over the same
domain were developed. Findings demonstrate that deep neural networks can be
trained to analyze multiple frequencies and reasonably predict the pattern
propagation factor, offering an alternative to traditional methods.",http://arxiv.org/abs/2505.15802v1
Model Merging is Secretly Certifiable: Non-Vacuous Generalisation Bounds for Low-Shot Learning,"Certifying the IID generalisation ability of deep networks is the first of
many requirements for trusting AI in high-stakes applications from medicine to
security. However, when instantiating generalisation bounds for deep networks
it remains challenging to obtain non-vacuous guarantees, especially when
applying contemporary large models on the small scale data prevalent in such
high-stakes fields. In this paper, we draw a novel connection between a family
of learning methods based on model fusion and generalisation certificates, and
surprisingly show that with minor adjustment several existing learning
strategies already provide non-trivial generalisation guarantees. Essentially,
by focusing on data-driven learning of downstream tasks by fusion rather than
fine-tuning, the certified generalisation gap becomes tiny and independent of
the base network size, facilitating its certification. Our results show for the
first time non-trivial generalisation guarantees for learning with as low as
100 examples, while using vision models such as VIT-B and language models such
as mistral-7B. This observation is significant as it has immediate implications
for facilitating the certification of existing systems as trustworthy, and
opens up new directions for research at the intersection of practice and
theory.",http://arxiv.org/abs/2505.15798v1
HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving,"Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can
enhance autonomous driving (AD) performance in complex scenarios. However,
current LLM-Dominated RL methods over-rely on LLM outputs, which are prone to
hallucinations. Evaluations show that state-of-the-art LLM indicates a
non-hallucination rate of only approximately 57.95% when assessed on essential
driving-related tasks. Thus, in these methods, hallucinations from the LLM can
directly jeopardize the performance of driving policies. This paper argues that
maintaining relative independence between the LLM and the RL is vital for
solving the hallucinations problem. Consequently, this paper is devoted to
propose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic
hints for state augmentation and policy optimization to assist RL agent in
motion planning, while the RL agent counteracts potential erroneous semantic
indications through policy learning to achieve excellent driving performance.
Based on this paradigm, we propose the HCRMP (LLM-Hinted Contextual
Reinforcement Learning Motion Planner) architecture, which is designed that
includes Augmented Semantic Representation Module to extend state space.
Contextual Stability Anchor Module enhances the reliability of multi-critic
weight hints by utilizing information from the knowledge base. Semantic Cache
Module is employed to seamlessly integrate LLM low-frequency guidance with RL
high-frequency control. Extensive experiments in CARLA validate HCRMP's strong
overall driving performance. HCRMP achieves a task success rate of up to 80.3%
under diverse driving conditions with different traffic densities. Under
safety-critical driving conditions, HCRMP significantly reduces the collision
rate by 11.4%, which effectively improves the driving performance in complex
scenarios.",http://arxiv.org/abs/2505.15793v2
Long-Form Information Alignment Evaluation Beyond Atomic Facts,"Information alignment evaluators are vital for various NLG evaluation tasks
and trustworthy LLM deployment, reducing hallucinations and enhancing user
trust. Current fine-grained methods, like FactScore, verify facts individually
but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this
work, we introduce MontageLie, a challenging benchmark that constructs
deceptive narratives by ""montaging"" truthful statements without introducing
explicit hallucinations. We demonstrate that both coarse-grained LLM-based
evaluators and current fine-grained frameworks are susceptible to this attack,
with AUC-ROC scores falling below 65%. To enable more robust fine-grained
evaluation, we propose DoveScore, a novel framework that jointly verifies
factual accuracy and event-order consistency. By modeling inter-fact
relationships, DoveScore outperforms existing fine-grained methods by over 8%,
providing a more robust solution for long-form text alignment evaluation. Our
code and datasets are available at https://github.com/dannalily/DoveScore.",http://arxiv.org/abs/2505.15792v1
VARD: Efficient and Dense Fine-Tuning for Diffusion Models with Value-based RL,"Diffusion models have emerged as powerful generative tools across various
domains, yet tailoring pre-trained models to exhibit specific desirable
properties remains challenging. While reinforcement learning (RL) offers a
promising solution,current methods struggle to simultaneously achieve stable,
efficient fine-tuning and support non-differentiable rewards. Furthermore,
their reliance on sparse rewards provides inadequate supervision during
intermediate steps, often resulting in suboptimal generation quality. To
address these limitations, dense and differentiable signals are required
throughout the diffusion process. Hence, we propose VAlue-based Reinforced
Diffusion (VARD): a novel approach that first learns a value function
predicting expection of rewards from intermediate states, and subsequently uses
this value function with KL regularization to provide dense supervision
throughout the generation process. Our method maintains proximity to the
pretrained model while enabling effective and stable training via
backpropagation. Experimental results demonstrate that our approach facilitates
better trajectory guidance, improves training efficiency and extends the
applicability of RL to diffusion models optimized for complex,
non-differentiable reward functions.",http://arxiv.org/abs/2505.15791v1
Fair Supervised Learning Through Constraints on Smooth Nonconvex Unfairness-Measure Surrogates,"A new strategy for fair supervised machine learning is proposed. The main
advantages of the proposed strategy as compared to others in the literature are
as follows. (a) We introduce a new smooth nonconvex surrogate to approximate
the Heaviside functions involved in discontinuous unfairness measures. The
surrogate is based on smoothing methods from the optimization literature, and
is new for the fair supervised learning literature. The surrogate is a tight
approximation which ensures the trained prediction models are fair, as opposed
to other (e.g., convex) surrogates that can fail to lead to a fair prediction
model in practice. (b) Rather than rely on regularizers (that lead to
optimization problems that are difficult to solve) and corresponding
regularization parameters (that can be expensive to tune), we propose a
strategy that employs hard constraints so that specific tolerances for
unfairness can be enforced without the complications associated with the use of
regularization. (c)~Our proposed strategy readily allows for constraints on
multiple (potentially conflicting) unfairness measures at the same time.
Multiple measures can be considered with a regularization approach, but at the
cost of having even more difficult optimization problems to solve and further
expense for tuning. By contrast, through hard constraints, our strategy leads
to optimization models that can be solved tractably with minimal tuning.",http://arxiv.org/abs/2505.15788v1
Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval,"While an image is worth more than a thousand words, only a few provide
crucial information for a given task and thus should be focused on. In light of
this, ideal text-to-image (T2I) retrievers should prioritize specific visual
attributes relevant to queries. To evaluate current retrievers on handling
attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with
9,112 queries about diverse attributes of interest. We find that CLIP-like
retrievers, which are widely adopted due to their efficiency and zero-shot
ability, have poor and imbalanced performance, possibly because their image
embeddings focus on global semantics and subjects while leaving out other
details. Notably, we reveal that even recent Multimodal Large Language Model
(MLLM)-based, stronger retrievers with a larger output dimension struggle with
this limitation. Hence, we hypothesize that retrieving with general image
embeddings is suboptimal for performing such queries. As a solution, we propose
to use promptable image embeddings enabled by these multimodal retrievers,
which boost performance by highlighting required attributes. Our pipeline for
deriving such embeddings generalizes across query types, image pools, and base
retriever architectures. To enhance real-world applicability, we offer two
acceleration strategies: Pre-processing promptable embeddings and using linear
approximations. We show that the former yields a 15% improvement in Recall@5
when prompts are predefined, while the latter achieves an 8% improvement when
prompts are only available during inference.",http://arxiv.org/abs/2505.15877v1
Large Language Models as Computable Approximations to Solomonoff Induction,"The rapid advancement of large language models (LLMs) calls for a rigorous
theoretical framework to explain their empirical success. While significant
progress has been made in understanding LLM behaviors, existing theoretical
frameworks remain fragmented in explaining emergent phenomena through a unified
mathematical lens. We establish the first formal connection between LLM
architectures and Algorithmic Information Theory (AIT) by proving two
fundamental results: (1) the training process computationally approximates
Solomonoff prior through loss minimization interpreted as program length
optimization, and (2) next-token prediction implements approximate Solomonoff
induction. We leverage AIT to provide a unified theoretical explanation for
in-context learning, few-shot learning, and scaling laws. Furthermore, our
theoretical insights lead to a principled method for few-shot example selection
that prioritizes samples where models exhibit lower predictive confidence. We
demonstrate through experiments on diverse text classification benchmarks that
this strategy yields significant performance improvements, particularly for
smaller model architectures, when compared to selecting high-confidence
examples. Our framework bridges the gap between theoretical foundations and
practical LLM behaviors, providing both explanatory power and actionable
insights for future model development.",http://arxiv.org/abs/2505.15784v1
Solving General-Utility Markov Decision Processes in the Single-Trial Regime with Online Planning,"In this work, we contribute the first approach to solve infinite-horizon
discounted general-utility Markov decision processes (GUMDPs) in the
single-trial regime, i.e., when the agent's performance is evaluated based on a
single trajectory. First, we provide some fundamental results regarding policy
optimization in the single-trial regime, investigating which class of policies
suffices for optimality, casting our problem as a particular MDP that is
equivalent to our original problem, as well as studying the computational
hardness of policy optimization in the single-trial regime. Second, we show how
we can leverage online planning techniques, in particular a Monte-Carlo tree
search algorithm, to solve GUMDPs in the single-trial regime. Third, we provide
experimental results showcasing the superior performance of our approach in
comparison to relevant baselines.",http://arxiv.org/abs/2505.15782v1
Projection-Based Correction for Enhancing Deep Inverse Networks,"Deep learning-based models have demonstrated remarkable success in solving
illposed inverse problems; however, many fail to strictly adhere to the
physical constraints imposed by the measurement process. In this work, we
introduce a projection-based correction method to enhance the inference of deep
inverse networks by ensuring consistency with the forward model. Specifically,
given an initial estimate from a learned reconstruction network, we apply a
projection step that constrains the solution to lie within the valid solution
space of the inverse problem. We theoretically demonstrate that if the recovery
model is a well-trained deep inverse network, the solution can be decomposed
into range-space and null-space components, where the projection-based
correction reduces to an identity transformation. Extensive simulations and
experiments validate the proposed method, demonstrating improved reconstruction
accuracy across diverse inverse problems and deep network architectures.",http://arxiv.org/abs/2505.15777v1
Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention,"Large Language Models (LLMs) encounter significant challenges in
long-sequence inference due to computational inefficiency and redundant
processing, driving interest in context compression techniques. Existing
methods often rely on token importance to perform hard local compression or
encode context into latent representations for soft global compression.
However, the uneven distribution of textual content relevance and the diversity
of demands for user instructions mean these approaches frequently lead to the
loss of potentially valuable information. To address this, we propose
$\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for
LLMs, which integrates both global and local perspectives to guide context
compression while retaining both the essential semantics and critical details
for task completion. Specifically, we employ a hybrid adapter to refine global
semantics with the global view, based on the observation that different
adapters excel at different tasks. Then we incorporate a classification layer
that assigns a retention probability to each context token based on the local
view, determining whether it should be retained or discarded. To foster a
balanced integration of global and local compression, we introduce auxiliary
paraphrasing and completion pretraining before instruction tuning. This
promotes a synergistic integration that emphasizes instruction-relevant
information while preserving essential local details, ultimately balancing
local and global information retention in context compression. Experiments show
that our HyCo$_2$ method significantly enhances long-text reasoning while
reducing token usage. It improves the performance of various LLM series by an
average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover,
HyCo$_2$ matches the performance of uncompressed methods while reducing token
consumption by 88.8\%.",http://arxiv.org/abs/2505.15774v1
Transfer of Structural Knowledge from Synthetic Languages,"This work explores transfer learning from several synthetic languages to
English. We investigate the structure of the embeddings in the fine-tuned
models, the information they contain, and the capabilities of the fine-tuned
models on simple linguistic tasks. We also introduce a new synthetic language
that leads to better transfer to English than the languages used in previous
research. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic
benchmark for natural language understanding that is more informative for less
powerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in
several domains demonstrating that fine-tuning on a new synthetic language
allows for better performance on a variety of tasks.",http://arxiv.org/abs/2505.15769v1
Improving planning and MBRL with temporally-extended actions,"Continuous time systems are often modeled using discrete time dynamics but
this requires a small simulation step to maintain accuracy. In turn, this
requires a large planning horizon which leads to computationally demanding
planning problems and reduced performance. Previous work in model free
reinforcement learning has partially addressed this issue using action repeats
where a policy is learned to determine a discrete action duration. Instead we
propose to control the continuous decision timescale directly by using
temporally-extended actions and letting the planner treat the duration of the
action as an additional optimization variable along with the standard action
variables. This additional structure has multiple advantages. It speeds up
simulation time of trajectories and, importantly, it allows for deep horizon
search in terms of primitive actions while using a shallow search depth in the
planner. In addition, in the model based reinforcement learning (MBRL) setting,
it reduces compounding errors from model learning and improves training time
for models. We show that this idea is effective and that the range for action
durations can be automatically selected using a multi-armed bandit formulation
and integrated into the MBRL framework. An extensive experimental evaluation
both in planning and in MBRL, shows that our approach yields faster planning,
better solutions, and that it enables solutions to problems that are not solved
in the standard formulation.",http://arxiv.org/abs/2505.15754v1
Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval,"Large Language Models (LLMs) are known to be vulnerable to jailbreaking
attacks, wherein adversaries exploit carefully engineered prompts to induce
harmful or unethical responses. Such threats have raised critical concerns
about the safety and reliability of LLMs in real-world deployment. While
existing defense mechanisms partially mitigate such risks, subsequent
advancements in adversarial techniques have enabled novel jailbreaking methods
to circumvent these protections, exposing the limitations of static defense
frameworks. In this work, we explore defending against evolving jailbreaking
threats through the lens of context retrieval. First, we conduct a preliminary
study demonstrating that even a minimal set of safety-aligned examples against
a particular jailbreak can significantly enhance robustness against this attack
pattern. Building on this insight, we further leverage the retrieval-augmented
generation (RAG) techniques and propose Safety Context Retrieval (SCR), a
scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our
comprehensive experiments demonstrate how SCR achieves superior defensive
performance against both established and emerging jailbreaking tactics,
contributing a new paradigm to LLM safety. Our code will be available upon
publication.",http://arxiv.org/abs/2505.15753v1
Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs,"We propose a novel framework for integrating fragmented multi-modal data in
Alzheimer's disease (AD) research using large language models (LLMs) and
knowledge graphs. While traditional multimodal analysis requires matched
patient IDs across datasets, our approach demonstrates population-level
integration of MRI, gene expression, biomarkers, EEG, and clinical indicators
from independent cohorts. Statistical analysis identified significant features
in each modality, which were connected as nodes in a knowledge graph. LLMs then
analyzed the graph to extract potential correlations and generate hypotheses in
natural language. This approach revealed several novel relationships, including
a potential pathway linking metabolic risk factors to tau protein abnormalities
via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between
frontal EEG channels and specific gene expression profiles (r=0.42-0.58,
p<0.01). Cross-validation with independent datasets confirmed the robustness of
major findings, with consistent effect sizes across cohorts (variance <15%).
The reproducibility of these findings was further supported by expert review
(Cohen's k=0.82) and computational validation. Our framework enables cross
modal integration at a conceptual level without requiring patient ID matching,
offering new possibilities for understanding AD pathology through fragmented
data reuse and generating testable hypotheses for future research.",http://arxiv.org/abs/2505.15747v2
Higher-order Structure Boosts Link Prediction on Temporal Graphs,"Temporal Graph Neural Networks (TGNNs) have gained growing attention for
modeling and predicting structures in temporal graphs. However, existing TGNNs
primarily focus on pairwise interactions while overlooking higher-order
structures that are integral to link formation and evolution in real-world
temporal graphs. Meanwhile, these models often suffer from efficiency
bottlenecks, further limiting their expressive power. To tackle these
challenges, we propose a Higher-order structure Temporal Graph Neural Network,
which incorporates hypergraph representations into temporal graph learning. In
particular, we develop an algorithm to identify the underlying higher-order
structures, enhancing the model's ability to capture the group interactions.
Furthermore, by aggregating multiple edge features into hyperedge
representations, HTGN effectively reduces memory cost during training. We
theoretically demonstrate the enhanced expressiveness of our approach and
validate its effectiveness and efficiency through extensive experiments on
various real-world temporal graphs. Experimental results show that HTGN
achieves superior performance on dynamic link prediction while reducing memory
costs by up to 50\% compared to existing methods.",http://arxiv.org/abs/2505.15746v1
Neuro-Argumentative Learning with Case-Based Reasoning,"We introduce Gradual Abstract Argumentation for Case-Based Reasoning (Gradual
AA-CBR), a data-driven, neurosymbolic classification model in which the outcome
is determined by an argumentation debate structure that is learned
simultaneously with neural-based feature extractors. Each argument in the
debate is an observed case from the training data, favouring their labelling.
Cases attack or support those with opposing or agreeing labellings, with the
strength of each argument and relationship learned through gradient-based
methods. This argumentation debate structure provides human-aligned reasoning,
improving model interpretability compared to traditional neural networks (NNs).
Unlike the existing purely symbolic variant, Abstract Argumentation for
Case-Based Reasoning (AA-CBR), Gradual AA-CBR is capable of multi-class
classification, automatic learning of feature and data point importance,
assigning uncertainty values to outcomes, using all available data points, and
does not require binary features. We show that Gradual AA-CBR performs
comparably to NNs whilst significantly outperforming existing AA-CBR
formulations.",http://arxiv.org/abs/2505.15742v1
Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses,"Large language models (LLMs) are rapidly deployed in real-world applications
ranging from chatbots to agentic systems. Alignment is one of the main
approaches used to defend against attacks such as prompt injection and
jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even
against Greedy Coordinate Gradient (GCG), a white-box attack that generates
adversarial suffixes to induce attacker-desired outputs. However, this search
space over discrete tokens is extremely large, making the task of finding
successful attacks difficult. GCG has, for instance, been shown to converge to
local minima, making it sensitive to initialization choices. In this paper, we
assess the future-proof robustness of these defenses using a more informed
threat model: attackers who have access to some information about the alignment
process. Specifically, we propose an informed white-box attack leveraging the
intermediate model checkpoints to initialize GCG, with each checkpoint acting
as a stepping stone for the next one. We show this approach to be highly
effective across state-of-the-art (SOTA) defenses and models. We further show
our informed initialization to outperform other initialization methods and show
a gradient-informed checkpoint selection strategy to greatly improve attack
performance and efficiency. Importantly, we also show our method to
successfully find universal adversarial suffixes -- single suffixes effective
across diverse inputs. Our results show that, contrary to previous beliefs,
effective adversarial suffixes do exist against SOTA alignment-based defenses,
that these can be found by existing attack methods when adversaries exploit
alignment knowledge, and that even universal suffixes exist. Taken together,
our results highlight the brittleness of current alignment-based methods and
the need to consider stronger threat models when testing the safety of LLMs.",http://arxiv.org/abs/2505.15738v1
"DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning","Large language models (LLMs) have improved significantly in their reasoning
through extensive training on massive datasets. However, relying solely on
additional data for improvement is becoming increasingly impractical,
highlighting the need for models to autonomously enhance their reasoning
without external supervision. In this paper, we propose Debate, Train, Evolve
(DTE), a novel ground truth-free training framework that uses multi-agent
debate traces to evolve a single language model. We also introduce a new
prompting strategy Reflect-Critique-Refine, to improve debate quality by
explicitly instructing agents to critique and refine their reasoning. Extensive
evaluations on five reasoning benchmarks with six open-weight models show that
our DTE framework achieve substantial improvements, with an average accuracy
gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe
strong cross-domain generalization, with an average accuracy gain of 5.8% on
all other benchmarks, suggesting that our method captures general reasoning
capabilities.",http://arxiv.org/abs/2505.15734v1
Are machine learning interpretations reliable? A stability study on global interpretations,"As machine learning systems are increasingly used in high-stakes domains,
there is a growing emphasis placed on making them interpretable to improve
trust in these systems. In response, a range of interpretable machine learning
(IML) methods have been developed to generate human-understandable insights
into otherwise black box models. With these methods, a fundamental question
arises: Are these interpretations reliable? Unlike with prediction accuracy or
other evaluation metrics for supervised models, the proximity to the true
interpretation is difficult to define. Instead, we ask a closely related
question that we argue is a prerequisite for reliability: Are these
interpretations stable? We define stability as findings that are consistent or
reliable under small random perturbations to the data or algorithms. In this
study, we conduct the first systematic, large-scale empirical stability study
on popular machine learning global interpretations for both supervised and
unsupervised tasks on tabular data. Our findings reveal that popular
interpretation methods are frequently unstable, notably less stable than the
predictions themselves, and that there is no association between the accuracy
of machine learning predictions and the stability of their associated
interpretations. Moreover, we show that no single method consistently provides
the most stable interpretations across a range of benchmark datasets. Overall,
these results suggest that interpretability alone does not warrant trust, and
underscores the need for rigorous evaluation of interpretation stability in
future work. To support these principles, we have developed and released an
open source IML dashboard and Python package to enable researchers to assess
the stability and reliability of their own data-driven interpretations and
discoveries.",http://arxiv.org/abs/2505.15728v1
Privacy-Preserving Conformal Prediction Under Local Differential Privacy,"Conformal prediction (CP) provides sets of candidate classes with a
guaranteed probability of containing the true class. However, it typically
relies on a calibration set with clean labels. We address privacy-sensitive
scenarios where the aggregator is untrusted and can only access a perturbed
version of the true labels. We propose two complementary approaches under local
differential privacy (LDP). In the first approach, users do not access the
model but instead provide their input features and a perturbed label using a
k-ary randomized response. In the second approach, which enforces stricter
privacy constraints, users add noise to their conformity score by binary search
response. This method requires access to the classification model but preserves
both data and label privacy. Both approaches compute the conformal threshold
directly from noisy data without accessing the true labels. We prove
finite-sample coverage guarantees and demonstrate robust coverage even under
severe randomization. This approach unifies strong local privacy with
predictive uncertainty control, making it well-suited for sensitive
applications such as medical imaging or large language model queries,
regardless of whether users can (or are willing to) compute their own scores.",http://arxiv.org/abs/2505.15721v1
Advancing LLM Safe Alignment with Safety Representation Ranking,"The rapid advancement of large language models (LLMs) has demonstrated
milestone success in a variety of tasks, yet their potential for generating
harmful content has raised significant safety concerns. Existing safety
evaluation approaches typically operate directly on textual responses,
overlooking the rich information embedded in the model's internal
representations. In this paper, we propose Safety Representation Ranking (SRR),
a listwise ranking framework that selects safe responses using hidden states
from the LLM itself. SRR encodes both instructions and candidate completions
using intermediate transformer representations and ranks candidates via a
lightweight similarity-based scorer. Our approach directly leverages internal
model states and supervision at the list level to capture subtle safety
signals. Experiments across multiple benchmarks show that SRR significantly
improves robustness to adversarial prompts. Our code will be available upon
publication.",http://arxiv.org/abs/2505.15710v1
HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases,"Large Language Models (LLMs) have demonstrated their potential in hardware
design tasks, such as Hardware Description Language (HDL) generation and
debugging. Yet, their performance in real-world, repository-level HDL projects
with thousands or even tens of thousands of code lines is hindered. To this
end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval
Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph
representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow
Graphs (DFGs) to capture both code graph view and hardware graph view.
HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the
limited recall issues inherent in similarity-based semantic retrieval by
incorporating structural information, but also enhances its extensibility to
various real-world tasks by a task-specific retrieval finetuning. Additionally,
to address the lack of comprehensive HDL search benchmarks, we introduce
HDLSearch, a multi-granularity evaluation dataset derived from real-world
repository-level projects. Experimental results demonstrate that HDLxGraph
significantly improves average search accuracy, debugging efficiency and
completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based
RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are
available at https://github.com/Nick-Zheng-Q/HDLxGraph.",http://arxiv.org/abs/2505.15701v1
MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation,"The [CLS] token in BERT is commonly used as a fixed-length representation for
classification tasks, yet prior work has shown that both other tokens and
intermediate layers encode valuable contextual information. In this work, we
propose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]
representation by aggregating information across layers and tokens.
Specifically, we explore three modifications: (i) max-pooling the [CLS] token
across multiple layers, (ii) enabling the [CLS] token to attend over the entire
final layer using an additional multi-head attention (MHA) layer, and (iii)
combining max-pooling across the full sequence with MHA. Our approach enhances
BERT's classification accuracy (especially on low-resource tasks) without
requiring pre-training or significantly increasing model size. Experiments on
the GLUE benchmark show that MaxPoolBERT consistently achieves a better
performance on the standard BERT-base model.",http://arxiv.org/abs/2505.15696v1
A Unified Theoretical Analysis of Private and Robust Offline Alignment: from RLHF to DPO,"In this paper, we theoretically investigate the effects of noisy labels in
offline alignment, with a focus on the interplay between privacy and robustness
against adversarial corruption. Specifically, under linear modeling
assumptions, we present a unified analysis covering both reinforcement learning
from human feedback (RLHF) and direct preference optimization (DPO) under
different privacy-corruption scenarios, such as Local differential
privacy-then-Corruption (LTC), where human preference labels are privatized
before being corrupted by an adversary, and Corruption-then-Local differential
privacy (CTL), where labels are corrupted before privacy protection. Our
analysis leverages a reduction framework that reduces the offline alignment
problem under linear modeling assumptions to parameter estimation in logistic
regression. This framework allows us to establish an interesting separation
result between LTC and CTL, demonstrating that LTC presents a greater challenge
than CTL in offline alignment, even under linear models. As important
by-products, our findings also advance the state-of-the-art theoretical results
in offline alignment under privacy-only or corruption-only scenarios.",http://arxiv.org/abs/2505.15694v1
Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities,"Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
(""thought patterns""). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.",http://arxiv.org/abs/2505.15692v1
A packing lemma for VCN${}_k$-dimension and learning high-dimensional data,"Recently, the authors introduced the theory of high-arity PAC learning, which
is well-suited for learning graphs, hypergraphs and relational structures. In
the same initial work, the authors proved a high-arity analogue of the
Fundamental Theorem of Statistical Learning that almost completely
characterizes all notions of high-arity PAC learning in terms of a
combinatorial dimension, called the Vapnik--Chervonenkis--Natarajan (VCN${}_k$)
$k$-dimension, leaving as an open problem only the characterization of
non-partite, non-agnostic high-arity PAC learnability.
  In this work, we complete this characterization by proving that non-partite
non-agnostic high-arity PAC learnability implies a high-arity version of the
Haussler packing property, which in turn implies finiteness of
VCN${}_k$-dimension. This is done by obtaining direct proofs that classic PAC
learnability implies classic Haussler packing property, which in turn implies
finite Natarajan dimension and noticing that these direct proofs nicely lift to
high-arity.",http://arxiv.org/abs/2505.15688v1
Graph Conditional Flow Matching for Relational Data Generation,"Data synthesis is gaining momentum as a privacy-enhancing technology. While
single-table tabular data generation has seen considerable progress, current
methods for multi-table data often lack the flexibility and expressiveness
needed to capture complex relational structures. In particular, they struggle
with long-range dependencies and complex foreign-key relationships, such as
tables with multiple parent tables or multiple types of links between the same
pair of tables. We propose a generative model for relational data that
generates the content of a relational dataset given the graph formed by the
foreign-key relationships. We do this by learning a deep generative model of
the content of the whole relational database by flow matching, where the neural
network trained to denoise records leverages a graph neural network to obtain
information from connected records. Our method is flexible, as it can support
relational datasets with complex structures, and expressive, as the generation
of each record can be influenced by any other record within the same connected
component. We evaluate our method on several benchmark datasets and show that
it achieves state-of-the-art performance in terms of synthetic data fidelity.",http://arxiv.org/abs/2505.15668v1
Deep greedy unfolding: Sorting out argsorting in greedy sparse recovery algorithms,"Gradient-based learning imposes (deep) neural networks to be differentiable
at all steps. This includes model-based architectures constructed by unrolling
iterations of an iterative algorithm onto layers of a neural network, known as
algorithm unrolling. However, greedy sparse recovery algorithms depend on the
non-differentiable argsort operator, which hinders their integration into
neural networks. In this paper, we address this challenge in Orthogonal
Matching Pursuit (OMP) and Iterative Hard Thresholding (IHT), two popular
representative algorithms in this class. We propose permutation-based variants
of these algorithms and approximate permutation matrices using ""soft""
permutation matrices derived from softsort, a continuous relaxation of argsort.
We demonstrate -- both theoretically and numerically -- that Soft-OMP and
Soft-IHT, as differentiable counterparts of OMP and IHT and fully compatible
with neural network training, effectively approximate these algorithms with a
controllable degree of accuracy. This leads to the development of OMP- and
IHT-Net, fully trainable network architectures based on Soft-OMP and Soft-IHT,
respectively. Finally, by choosing weights as ""structure-aware"" trainable
parameters, we connect our approach to structured sparse recovery and
demonstrate its ability to extract latent sparsity patterns from data.",http://arxiv.org/abs/2505.15661v1
FLARE: Robot Learning with Implicit World Modeling,"We introduce $\textbf{F}$uture $\textbf{LA}$tent $\textbf{RE}$presentation
Alignment ($\textbf{FLARE}$), a novel framework that integrates predictive
latent world modeling into robot policy learning. By aligning features from a
diffusion transformer with latent embeddings of future observations,
$\textbf{FLARE}$ enables a diffusion transformer policy to anticipate latent
representations of future observations, allowing it to reason about long-term
consequences while generating actions. Remarkably lightweight, $\textbf{FLARE}$
requires only minimal architectural modifications -- adding a few tokens to
standard vision-language-action (VLA) models -- yet delivers substantial
performance gains. Across two challenging multitask simulation imitation
learning benchmarks spanning single-arm and humanoid tabletop manipulation,
$\textbf{FLARE}$ achieves state-of-the-art performance, outperforming prior
policy learning baselines by up to 26%. Moreover, $\textbf{FLARE}$ unlocks the
ability to co-train with human egocentric video demonstrations without action
labels, significantly boosting policy generalization to a novel object with
unseen geometry with as few as a single robot demonstration. Our results
establish $\textbf{FLARE}$ as a general and scalable approach for combining
implicit world modeling with high-frequency robotic control.",http://arxiv.org/abs/2505.15659v1
LCDB 1.1: A Database Illustrating Learning Curves Are More Ill-Behaved Than Previously Thought,"Sample-wise learning curves plot performance versus training set size. They
are useful for studying scaling laws and speeding up hyperparameter tuning and
model selection. Learning curves are often assumed to be well-behaved: monotone
(i.e. improving with more data) and convex. By constructing the Learning Curves
Database 1.1 (LCDB 1.1), a large-scale database with high-resolution learning
curves, we show that learning curves are less often well-behaved than
previously thought. Using statistically rigorous methods, we observe
significant ill-behavior in approximately 14% of the learning curves, almost
twice as much as in previous estimates. We also identify which learners are to
blame and show that specific learners are more ill-behaved than others.
Additionally, we demonstrate that different feature scalings rarely resolve
ill-behavior. We evaluate the impact of ill-behavior on downstream tasks, such
as learning curve fitting and model selection, and find it poses significant
challenges, underscoring the relevance and potential of LCDB 1.1 as a
challenging benchmark for future research.",http://arxiv.org/abs/2505.15657v1
Learning Small Decision Trees with Few Outliers: A Parameterized Perspective,"Decision trees are a fundamental tool in machine learning for representing,
classifying, and generalizing data. It is desirable to construct ``small''
decision trees, by minimizing either the \textit{size} ($s$) or the
\textit{depth} $(d)$ of the \textit{decision tree} (\textsc{DT}). Recently, the
parameterized complexity of \textsc{Decision Tree Learning} has attracted a lot
of attention. We consider a generalization of \textsc{Decision Tree Learning}
where given a \textit{classification instance} $E$ and an integer $t$, the task
is to find a ``small'' \textsc{DT} that disagrees with $E$ in at most $t$
examples. We consider two problems: \textsc{DTSO} and \textsc{DTDO}, where the
goal is to construct a \textsc{DT} minimizing $s$ and $d$, respectively. We
first establish that both \textsc{DTSO} and \textsc{DTDO} are W[1]-hard when
parameterized by $s+\delta_{max}$ and $d+\delta_{max}$, respectively, where
$\delta_{max}$ is the maximum number of features in which two differently
labeled examples can differ. We complement this result by showing that these
problems become \textsc{FPT} if we include the parameter $t$. We also consider
the kernelization complexity of these problems and establish several positive
and negative results for both \textsc{DTSO} and \textsc{DTDO}.",http://arxiv.org/abs/2505.15648v1
Second-Order Convergence in Private Stochastic Non-Convex Optimization,"We investigate the problem of finding second-order stationary points (SOSP)
in differentially private (DP) stochastic non-convex optimization. Existing
methods suffer from two key limitations: (i) inaccurate convergence error rate
due to overlooking gradient variance in the saddle point escape analysis, and
(ii) dependence on auxiliary private model selection procedures for identifying
DP-SOSP, which can significantly impair utility, particularly in distributed
settings. To address these issues, we propose a generic perturbed stochastic
gradient descent (PSGD) framework built upon Gaussian noise injection and
general gradient oracles. A core innovation of our framework is using model
drift distance to determine whether PSGD escapes saddle points, ensuring
convergence to approximate local minima without relying on second-order
information or additional DP-SOSP identification. By leveraging the adaptive
DP-SPIDER estimator as a specific gradient oracle, we develop a new DP
algorithm that rectifies the convergence error rates reported in prior work. We
further extend this algorithm to distributed learning with arbitrarily
heterogeneous data, providing the first formal guarantees for finding DP-SOSP
in such settings. Our analysis also highlights the detrimental impacts of
private selection procedures in distributed learning under high-dimensional
models, underscoring the practical benefits of our design. Numerical
experiments on real-world datasets validate the efficacy of our approach.",http://arxiv.org/abs/2505.15647v1
Optimal Best-Arm Identification under Fixed Confidence with Multiple Optima,"We study the problem of best-arm identification in stochastic multi-armed
bandits under the fixed-confidence setting, with a particular focus on
instances that admit multiple optimal arms. While the Track-and-Stop algorithm
of Garivier and Kaufmann (2016) is widely conjectured to be instance-optimal,
its performance in the presence of multiple optima has remained insufficiently
understood. In this work, we revisit the Track-and-Stop strategy and propose a
modified stopping rule that ensures instance-optimality even when the set of
optimal arms is not a singleton. Our analysis introduces a new
information-theoretic lower bound that explicitly accounts for multiple optimal
arms, and we demonstrate that our stopping rule tightly matches this bound.",http://arxiv.org/abs/2505.15643v1
A Simple Approximation Algorithm for Optimal Decision Tree,"Optimal decision tree (\odt) is a fundamental problem arising in applications
such as active learning, entity identification, and medical diagnosis. An
instance of \odt is given by $m$ hypotheses, out of which an unknown ``true''
hypothesis is drawn according to some probability distribution. An algorithm
needs to identify the true hypothesis by making queries: each query incurs a
cost and has a known response for each hypothesis. The goal is to minimize the
expected query cost to identify the true hypothesis. We consider the most
general setting with arbitrary costs, probabilities and responses. \odt is
NP-hard to approximate better than $\ln m$ and there are $O(\ln m)$
approximation algorithms known for it. However, these algorithms and/or their
analyses are quite complex. Moreover, the leading constant factors are large.
We provide a simple algorithm and analysis for \odt, proving an approximation
ratio of $8 \ln m$.",http://arxiv.org/abs/2505.15641v1
Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes,"We revisit the classical problem of Bayesian ensembles and address the
challenge of learning optimal combinations of Bayesian models in an online,
continual learning setting. To this end, we reinterpret existing approaches
such as Bayesian model averaging (BMA) and Bayesian stacking through a novel
empirical Bayes lens, shedding new light on the limitations and pathologies of
BMA. Further motivated by insights from online optimization, we propose Online
Bayesian Stacking (OBS), a method that optimizes the log-score over predictive
distributions to adaptively combine Bayesian models. A key contribution of our
work is establishing a novel connection between OBS and portfolio selection,
bridging Bayesian ensemble learning with a rich, well-studied theoretical
framework that offers efficient algorithms and extensive regret analysis. We
further clarify the relationship between OBS and online BMA, showing that they
optimize related but distinct cost functions. Through theoretical analysis and
empirical evaluation, we identify scenarios where OBS outperforms online BMA
and provide principled guidance on when practitioners should prefer one
approach over the other.",http://arxiv.org/abs/2505.15638v1
Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search,"Nearest neighbor search is central in machine learning, information
retrieval, and databases. For high-dimensional datasets, graph-based methods
such as HNSW, DiskANN, and NSG have become popular thanks to their empirical
accuracy and efficiency. These methods construct a directed graph over the
dataset and perform beam search on the graph to find nodes close to a given
query. While significant work has focused on practical refinements and
theoretical understanding of graph-based methods, many questions remain. We
propose a new distance-based termination condition for beam search to replace
the commonly used condition based on beam width. We prove that, as long as the
search graph is navigable, our resulting Adaptive Beam Search method is
guaranteed to approximately solve the nearest-neighbor problem, establishing a
connection between navigability and the performance of graph-based search. We
also provide extensive experiments on our new termination condition for both
navigable graphs and approximately navigable graphs used in practice, such as
HNSW and Vamana graphs. We find that Adaptive Beam Search outperforms standard
beam search over a range of recall values, data sets, graph constructions, and
target number of nearest neighbors. It thus provides a simple and practical way
to improve the performance of popular methods.",http://arxiv.org/abs/2505.15636v1
Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models,"Large Language Models (LLMs) demonstrate the ability to solve reasoning and
mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT
length, as seen in models such as DeepSeek-R1, significantly enhances this
reasoning for complex problems, but requires costly and high-quality long CoT
data and fine-tuning. This work, inspired by the deep thinking paradigm of
DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of
an LLM without external datasets. Our method first employs Sparse Autoencoders
(SAEs) to extract interpretable features from vanilla CoT. These features are
then used to steer the LLM's internal states during generation. Recognizing
that many LLMs do not have corresponding pre-trained SAEs, we further introduce
a novel SAE-free steering algorithm, which directly computes steering
directions from the residual activations of an LLM, obviating the need for an
explicit SAE. Experimental results demonstrate that both our SAE-based and
subsequent SAE-free steering algorithms significantly enhance the reasoning
capabilities of LLMs.",http://arxiv.org/abs/2505.15634v1
Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions,"Large language models that use retrieval augmented generation have the
potential to unlock valuable knowledge for researchers, policymakers, and the
public by making long and technical climate-related documents more accessible.
While this approach can help alleviate factual hallucinations by relying on
retrieved passages as additional context, its effectiveness depends on whether
the model's output remains faithful to these passages. To address this, we
explore the automatic assessment of faithfulness of different models in this
setting. We then focus on ClimateGPT, a large language model specialised in
climate science, to examine which factors in its instruction fine-tuning impact
the model's faithfulness. By excluding unfaithful subsets of the model's
training data, we develop ClimateGPT Faithful+, which achieves an improvement
in faithfulness from 30% to 57% in supported atomic claims according to our
automatic metric.",http://arxiv.org/abs/2505.15633v1
Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks,"Neural Architecture Search (NAS) accelerates progress in deep learning
through systematic refinement of model architectures. The downside is
increasingly large energy consumption during the search process.
Surrogate-based benchmarking mitigates the cost of full training by querying a
pre-trained surrogate to obtain an estimate for the quality of the model.
Specifically, energy-aware benchmarking aims to make it possible for NAS to
favourably trade off model energy consumption against accuracy. Towards this
end, we propose three design principles for such energy-aware benchmarks: (i)
reliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic
cost reporting. We analyse EA-HAS-Bench based on these principles and find that
the choice of GPU measurement API has a large impact on the quality of results.
Using the Nvidia System Management Interface (SMI) on top of its underlying
library influences the sampling rate during the initial data collection,
returning faulty low-power estimations. This results in poor correlation with
accurate measurements obtained from an external power meter. With this study,
we bring to attention several key considerations when performing energy-aware
surrogate-based benchmarking and derive first guidelines that can help design
novel benchmarks. We show a narrow usage range of the four GPUs attached to our
device, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down
even further when using all four GPUs. To improve holistic energy reporting, we
propose calibration experiments over assumptions made in popular tools, such as
Code Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to
8.9 % without and to 6.6 % with prior estimation of the expected load on the
device.",http://arxiv.org/abs/2505.15631v1
Aligning Explanations with Human Communication,"Machine learning explainability aims to make the decision-making process of
black-box models more transparent by finding the most important input features
for a given prediction task. Recent works have proposed composing explanations
from semantic concepts (e.g., colors, patterns, shapes) that are inherently
interpretable to the user of a model. However, these methods generally ignore
the communicative context of explanation-the ability of the user to understand
the prediction of the model from the explanation. For example, while a medical
doctor might understand an explanation in terms of clinical markers, a patient
may need a more accessible explanation to make sense of the same diagnosis. In
this paper, we address this gap with listener-adaptive explanations. We propose
an iterative procedure grounded in principles of pragmatic reasoning and the
rational speech act to generate explanations that maximize communicative
utility. Our procedure only needs access to pairwise preferences between
candidate explanations, relevant in real-world scenarios where a listener model
may not be available. We evaluate our method in image classification tasks,
demonstrating improved alignment between explanations and listener preferences
across three datasets. Furthermore, we perform a user study that demonstrates
our explanations increase communicative utility.",http://arxiv.org/abs/2505.15626v1
Mechanistic Insights into Grokking from the Embedding Layer,"Grokking, a delayed generalization in neural networks after perfect training
performance, has been observed in Transformers and MLPs, but the components
driving it remain underexplored. We show that embeddings are central to
grokking: introducing them into MLPs induces delayed generalization in modular
arithmetic tasks, whereas MLPs without embeddings can generalize immediately.
Our analysis identifies two key mechanisms: (1) Embedding update dynamics,
where rare tokens stagnate due to sparse gradient updates and weight decay, and
(2) Bilinear coupling, where the interaction between embeddings and downstream
weights introduces saddle points and increases sensitivity to initialization.
To confirm these mechanisms, we investigate frequency-aware sampling, which
balances token updates by minimizing gradient variance, and embedding-specific
learning rates, derived from the asymmetric curvature of the bilinear loss
landscape. We prove that an adaptive learning rate ratio,
\(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot
\frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating
convergence. Our methods not only improve grokking dynamics but also extend to
broader challenges in Transformer optimization, where bilinear interactions
hinder efficient training.",http://arxiv.org/abs/2505.15624v1
Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning,"Large language models (LLMs) demonstrate considerable potential in various
natural language tasks but face significant challenges in mathematical
reasoning, particularly in executing precise, multi-step logic. However,
current evaluation frameworks judge their performance solely based on accuracy,
which only accounts for the final answer. This study explores these pitfalls by
employing a novel evaluation framework. We propose an evaluation metric called
the MAPLE score, which holistically quantifies reasoning misalignment by
integrating error rates, redundancy, and validity.",http://arxiv.org/abs/2505.15623v1
Benchmarking Energy and Latency in TinyML: A Novel Method for Resource-Constrained AI,"The rise of IoT has increased the need for on-edge machine learning, with
TinyML emerging as a promising solution for resource-constrained devices such
as MCU. However, evaluating their performance remains challenging due to
diverse architectures and application scenarios. Current solutions have many
non-negligible limitations. This work introduces an alternative benchmarking
methodology that integrates energy and latency measurements while
distinguishing three execution phases pre-inference, inference, and
post-inference. Additionally, the setup ensures that the device operates
without being powered by an external measurement unit, while automated testing
can be leveraged to enhance statistical significance. To evaluate our setup, we
tested the STM32N6 MCU, which includes a NPU for executing neural networks. Two
configurations were considered: high-performance and Low-power. The variation
of the EDP was analyzed separately for each phase, providing insights into the
impact of hardware configurations on energy efficiency. Each model was tested
1000 times to ensure statistically relevant results. Our findings demonstrate
that reducing the core voltage and clock frequency improve the efficiency of
pre- and post-processing without significantly affecting network execution
performance. This approach can also be used for cross-platform comparisons to
determine the most efficient inference platform and to quantify how pre- and
post-processing overhead varies across different hardware implementations.",http://arxiv.org/abs/2505.15622v1
Learn to Reason Efficiently with Adaptive Length-based Reward Shaping,"Large Reasoning Models (LRMs) have shown remarkable capabilities in solving
complex problems through reinforcement learning (RL), particularly by
generating long reasoning traces. However, these extended outputs often exhibit
substantial redundancy, which limits the efficiency of LRMs. In this paper, we
investigate RL-based approaches to promote reasoning efficiency. Specifically,
we first present a unified framework that formulates various efficient
reasoning methods through the lens of length-based reward shaping. Building on
this perspective, we propose a novel Length-bAsed StEp Reward shaping method
(LASER), which employs a step function as the reward, controlled by a target
length. LASER surpasses previous methods, achieving a superior Pareto-optimal
balance between performance and efficiency. Next, we further extend LASER based
on two key intuitions: (1) The reasoning behavior of the model evolves during
training, necessitating reward specifications that are also adaptive and
dynamic; (2) Rather than uniformly encouraging shorter or longer chains of
thought (CoT), we posit that length-based reward shaping should be
difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.
This approach is expected to facilitate a combination of fast and slow
thinking, leading to a better overall tradeoff. The resulting method is termed
LASER-D (Dynamic and Difficulty-aware). Experiments on
DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and
DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both
reasoning performance and response length efficiency. For instance, LASER-D and
its variant achieve a +6.1 improvement on AIME2024 while reducing token usage
by 63%. Further analysis reveals our RL-based compression produces more concise
reasoning patterns with less redundant ""self-reflections"". Resources are at
https://github.com/hkust-nlp/Laser.",http://arxiv.org/abs/2505.15612v1
Deep Learning for Continuous-time Stochastic Control with Jumps,"In this paper, we introduce a model-based deep-learning approach to solve
finite-horizon continuous-time stochastic control problems with jumps. We
iteratively train two neural networks: one to represent the optimal policy and
the other to approximate the value function. Leveraging a continuous-time
version of the dynamic programming principle, we derive two different training
objectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the
networks capture the underlying stochastic dynamics. Empirical evaluations on
different problems illustrate the accuracy and scalability of our approach,
demonstrating its effectiveness in solving complex, high-dimensional stochastic
control tasks.",http://arxiv.org/abs/2505.15602v1
Beyond Classification: Evaluating Diffusion Denoised Smoothing for Security-Utility Trade off,"While foundation models demonstrate impressive performance across various
tasks, they remain vulnerable to adversarial inputs. Current research explores
various approaches to enhance model robustness, with Diffusion Denoised
Smoothing emerging as a particularly promising technique. This method employs a
pretrained diffusion model to preprocess inputs before model inference. Yet,
its effectiveness remains largely unexplored beyond classification. We aim to
address this gap by analyzing three datasets with four distinct downstream
tasks under three different adversarial attack algorithms. Our findings reveal
that while foundation models maintain resilience against conventional
transformations, applying high-noise diffusion denoising to clean images
without any distortions significantly degrades performance by as high as 57%.
Low-noise diffusion settings preserve performance but fail to provide adequate
protection across all attack types. Moreover, we introduce a novel attack
strategy specifically targeting the diffusion process itself, capable of
circumventing defenses in the low-noise regime. Our results suggest that the
trade-off between adversarial robustness and performance remains a challenge to
be addressed.",http://arxiv.org/abs/2505.15594v1
World Models as Reference Trajectories for Rapid Motor Adaptation,"Deploying learned control policies in real-world environments poses a
fundamental challenge. When system dynamics change unexpectedly, performance
degrades until models are retrained on new data. We introduce Reflexive World
Models (RWM), a dual control framework that uses world model predictions as
implicit reference trajectories for rapid adaptation. Our method separates the
control problem into long-term reward maximization through reinforcement
learning and robust motor execution through rapid latent control. This dual
architecture achieves significantly faster adaptation with low online
computational cost compared to model-based RL baselines, while maintaining
near-optimal performance. The approach combines the benefits of flexible policy
learning through reinforcement learning with rapid error correction
capabilities, providing a principled approach to maintaining performance in
high-dimensional continuous control tasks under varying dynamics.",http://arxiv.org/abs/2505.15589v1
MIRB: Mathematical Information Retrieval Benchmark,"Mathematical Information Retrieval (MIR) is the task of retrieving
information from mathematical documents and plays a key role in various
applications, including theorem search in mathematical libraries, answer
retrieval on math forums, and premise selection in automated theorem proving.
However, a unified benchmark for evaluating these diverse retrieval tasks has
been lacking. In this paper, we introduce MIRB (Mathematical Information
Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB
includes four tasks: semantic statement retrieval, question-answer retrieval,
premise retrieval, and formula retrieval, spanning a total of 12 datasets. We
evaluate 13 retrieval models on this benchmark and analyze the challenges
inherent to MIR. We hope that MIRB provides a comprehensive framework for
evaluating MIR systems and helps advance the development of more effective
retrieval models tailored to the mathematical domain.",http://arxiv.org/abs/2505.15585v1
Federated Learning with Unlabeled Clients: Personalization Can Happen in Low Dimensions,"Personalized federated learning has emerged as a popular approach to training
on devices holding statistically heterogeneous data, known as clients. However,
most existing approaches require a client to have labeled data for training or
finetuning in order to obtain their own personalized model. In this paper we
address this by proposing FLowDUP, a novel method that is able to generate a
personalized model using only a forward pass with unlabeled data. The generated
model parameters reside in a low-dimensional subspace, enabling efficient
communication and computation. FLowDUP's learning objective is theoretically
motivated by our new transductive multi-task PAC-Bayesian generalization bound,
that provides performance guarantees for unlabeled clients. The objective is
structured in such a way that it allows both clients with labeled data and
clients with only unlabeled data to contribute to the training process. To
supplement our theoretical results we carry out a thorough experimental
evaluation of FLowDUP, demonstrating strong empirical performance on a range of
datasets with differing sorts of statistically heterogeneous clients. Through
numerous ablation studies, we test the efficacy of the individual components of
the method.",http://arxiv.org/abs/2505.15579v1
Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models,"Vision-Language Models (VLMs) are essential for multimodal tasks, especially
compositional reasoning (CR) tasks, which require distinguishing fine-grained
semantic differences between visual and textual embeddings. However, existing
methods primarily fine-tune the model by generating text-based hard negative
samples, neglecting the importance of image-based negative samples, which
results in insufficient training of the visual encoder and ultimately impacts
the overall performance of the model. Moreover, negative samples are typically
treated uniformly, without considering their difficulty levels, and the
alignment of positive samples is insufficient, which leads to challenges in
aligning difficult sample pairs. To address these issues, we propose Adaptive
Hard Negative Perturbation Learning (AHNPL). AHNPL translates text-based hard
negatives into the visual domain to generate semantically disturbed image-based
negatives for training the model, thereby enhancing its overall performance.
AHNPL also introduces a contrastive learning approach using a multimodal hard
negative loss to improve the model's discrimination of hard negatives within
each modality and a dynamic margin loss that adjusts the contrastive margin
according to sample difficulty to enhance the distinction of challenging sample
pairs. Experiments on three public datasets demonstrate that our method
effectively boosts VLMs' performance on complex CR tasks. The source code is
available at https://github.com/nynu-BDAI/AHNPL.",http://arxiv.org/abs/2505.15576v1
Bridging the Domain Gap in Equation Distillation with Reinforcement Feedback,"The data-to-equation (Data2Eqn) task aims to discover interpretable
mathematical equations that map observed values to labels, offering physical
insights and broad applicability across academic and industrial domains.
Genetic programming and traditional deep learning-based approaches suffer from
search inefficiency and poor generalization on small task-specific datasets.
Foundation models showed promise in this area, but existing approaches suffer
from: 1) They are pretrained on general-purpose data distributions, making them
less effective for domain-specific tasks; and 2) their training objectives
focus on token-level alignment, overlooking mathematical semantics, which can
lead to inaccurate equations. To address these issues, we aim to enhance the
domain adaptability of foundation models for Data2Eqn tasks. In this work, we
propose a reinforcement learning-based finetuning framework that directly
optimizes the generation policy of a pretrained model through reward signals
derived from downstream numerical fitness. Our method allows the model to adapt
to specific and complex data distributions and generate mathematically
meaningful equations. Extensive experiments demonstrate that our approach
improves both the accuracy and robustness of equation generation under complex
distributions.",http://arxiv.org/abs/2505.15572v1
Refining Neural Activation Patterns for Layer-Level Concept Discovery in Neural Network-Based Receivers,"Concept discovery in neural networks often targets individual neurons or
human-interpretable features, overlooking distributed layer-wide patterns. We
study the Neural Activation Pattern (NAP) methodology, which clusters
full-layer activation distributions to identify such layer-level concepts.
Applied to visual object recognition and radio receiver models, we propose
improved normalization, distribution estimation, distance metrics, and varied
cluster selection. In the radio receiver model, distinct concepts did not
emerge; instead, a continuous activation manifold shaped by Signal-to-Noise
Ratio (SNR) was observed -- highlighting SNR as a key learned factor,
consistent with classical receiver behavior and supporting physical
plausibility. Our enhancements to NAP improved in-distribution vs.
out-of-distribution separation, suggesting better generalization and indirectly
validating clustering quality. These results underscore the importance of
clustering design and activation manifolds in interpreting and troubleshooting
neural network behavior.",http://arxiv.org/abs/2505.15570v1
Impact of Data Sparsity on Machine Learning for Fault Detection in Power System Protection,"Germany's transition to a renewable energy-based power system is reshaping
grid operations, requiring advanced monitoring and control to manage
decentralized generation. Machine learning (ML) has emerged as a powerful tool
for power system protection, particularly for fault detection (FD) and fault
line identification (FLI) in transmission grids. However, ML model reliability
depends on data quality and availability. Data sparsity resulting from sensor
failures, communication disruptions, or reduced sampling rates poses a
challenge to ML-based FD and FLI. Yet, its impact has not been systematically
validated prior to this work. In response, we propose a framework to assess the
impact of data sparsity on ML-based FD and FLI performance. We simulate
realistic data sparsity scenarios, evaluate their impact, derive quantitative
insights, and demonstrate the effectiveness of this evaluation strategy by
applying it to an existing ML-based framework. Results show the ML model
remains robust for FD, maintaining an F1-score of 0.999 $\pm$ 0.000 even after
a 50x data reduction. In contrast, FLI is more sensitive, with performance
decreasing by 55.61% for missing voltage measurements and 9.73% due to
communication failures at critical network points. These findings offer
actionable insights for optimizing ML models for real-world grid protection.
This enables more efficient FD and supports targeted improvements in FLI.",http://arxiv.org/abs/2505.15560v1
Robo-DM: Data Management For Large Robot Datasets,"Recent results suggest that very large datasets of teleoperated robot
demonstrations can be used to train transformer-based models that have the
potential to generalize to new scenes, robots, and tasks. However, curating,
distributing, and loading large datasets of robot trajectories, which typically
consist of video, textual, and numerical modalities - including streams from
multiple cameras - remains challenging. We propose Robo-DM, an efficient
open-source cloud-based data management toolkit for collecting, sharing, and
learning with robot data. With Robo-DM, robot datasets are stored in a
self-contained format with Extensible Binary Meta Language (EBML). Robo-DM can
significantly reduce the size of robot trajectory data, transfer costs, and
data load time during training. Compared to the RLDS format used in OXE
datasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x
(lossless). Robo-DM also accelerates data retrieval by load-balancing video
decoding with memory-mapped decoding caches. Compared to LeRobot, a framework
that also uses lossy video compression, Robo-DM is up to 50x faster when
decoding sequentially. We physically evaluate a model trained by Robo-DM with
lossy compression, a pick-and-place task, and In-Context Robot Transformer.
Robo-DM uses 75x compression of the original dataset and does not suffer
reduction in downstream task accuracy.",http://arxiv.org/abs/2505.15558v1
Modular Jump Gaussian Processes,"Gaussian processes (GPs) furnish accurate nonlinear predictions with
well-calibrated uncertainty. However, the typical GP setup has a built-in
stationarity assumption, making it ill-suited for modeling data from processes
with sudden changes, or ""jumps"" in the output variable. The ""jump GP"" (JGP) was
developed for modeling data from such processes, combining local GPs and latent
""level"" variables under a joint inferential framework. But joint modeling can
be fraught with difficulty. We aim to simplify by suggesting a more modular
setup, eschewing joint inference but retaining the main JGP themes: (a)
learning optimal neighborhood sizes that locally respect manifolds of
discontinuity; and (b) a new cluster-based (latent) feature to capture regions
of distinct output levels on both sides of the manifold. We show that each of
(a) and (b) separately leads to dramatic improvements when modeling processes
with jumps. In tandem (but without requiring joint inference) that benefit is
compounded, as illustrated on real and synthetic benchmark examples from the
recent literature.",http://arxiv.org/abs/2505.15557v1
Short-Range Dependency Effects on Transformer Instability and a Decomposed Attention Solution,"Transformer language models have driven significant progress across various
fields, including natural language processing and computer vision. A central
component of these models is the self-attention (SA) mechanism, which learns
rich vector representations of tokens by modeling their relationships with
others in a sequence. However, despite extensive research, transformers
continue to suffer from training instability -- often manifesting as spikes or
divergence in the training loss during a run.
  In this work, we identify one source of this instability: SA's limited
ability to capture short-range dependencies, especially in tasks like language
modeling, where almost every token heavily relies on its nearby neighbors. This
limitation causes the pre-softmax logits of SA to grow rapidly, destabilizing
training. To address this, we propose decomposing the SA into local
(short-range) and global (long-range) attention heads. This decomposed
attention, referred to as Long Short-attention (LS-attention), mitigates logit
explosion and results in more stable training compared to an equivalent
multi-head self-attention (MHSA). Empirical comparisons with two alternative
training stabilization methods show that LS-attention reduces the validation
perplexity to nearly 2/5 of that achieved by one method and reaches a similar
perplexity as the other method using only 1/20 of the GPU hours. Additionally,
our experiments demonstrate that LS-attention reduces inference latency by up
to 36% compared to a state-of-the-art implementation of equivalent MHSA.",http://arxiv.org/abs/2505.15548v1
"Oversmoothing, ""Oversquashing"", Heterophily, Long-Range, and more: Demystifying Common Beliefs in Graph Machine Learning","After a renaissance phase in which researchers revisited the message-passing
paradigm through the lens of deep learning, the graph machine learning
community shifted its attention towards a deeper and practical understanding of
message-passing's benefits and limitations. In this position paper, we notice
how the fast pace of progress around the topics of oversmoothing and
oversquashing, the homophily-heterophily dichotomy, and long-range tasks, came
with the consolidation of commonly accepted beliefs and assumptions that are
not always true nor easy to distinguish from each other. We argue that this has
led to ambiguities around the investigated problems, preventing researchers
from focusing on and addressing precise research questions while causing a good
amount of misunderstandings. Our contribution wants to make such common beliefs
explicit and encourage critical thinking around these topics, supported by
simple but noteworthy counterexamples. The hope is to clarify the distinction
between the different issues and promote separate but intertwined research
directions to address them.",http://arxiv.org/abs/2505.15547v1
A Temporal Difference Method for Stochastic Continuous Dynamics,"For continuous systems modeled by dynamical equations such as ODEs and SDEs,
Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman
(HJB) equation, which provides the theoretical target of reinforcement learning
(RL). Although recent advances in RL successfully leverage this formulation,
the existing methods typically assume the underlying dynamics are known a
priori because they need explicit access to the coefficient functions of
dynamical equations to update the value function following the HJB equation. We
address this inherent limitation of HJB-based RL; we propose a model-free
approach still targeting the HJB equation and propose the corresponding
temporal difference method. We demonstrate its potential advantages over
transition kernel-based formulations, both qualitatively and empirically. The
proposed formulation paves the way toward bridging stochastic optimal control
and model-free reinforcement learning.",http://arxiv.org/abs/2505.15544v2
Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets,"Vision-Language Models (VLMs) acquire real-world knowledge and general
reasoning ability through Internet-scale image-text corpora. They can augment
robotic systems with scene understanding and task planning, and assist
visuomotor policies that are trained on robot trajectory data. We explore the
reverse paradigm - using rich, real, multi-modal robot trajectory data to
enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual
Question Answering (VQA) dataset generation framework for VLMs. Given a human
tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual
and non-descriptive sensory modalities, such as end-effector pose, gripper
aperture, and force sensing. Based on these modalities, it segments the robot
trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses
scene and interaction understanding to identify 3D properties of the robot,
task goal, and the target object. The properties are used to generate
representative VQA queries - images with textural multiple-choice questions -
based on spatial, goal-conditioned, and interaction reasoning question
templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710
questions covering 463 distinct scenes and 3,396 robotic manipulation tasks
from 176k real robot trajectories. Results suggest that Robo2VLM-1 can
benchmark and improve VLM capabilities in spatial and interaction reasoning.",http://arxiv.org/abs/2505.15517v1
Explainable embeddings with Distance Explainer,"While eXplainable AI (XAI) has advanced significantly, few methods address
interpretability in embedded vector spaces where dimensions represent complex
abstractions. We introduce Distance Explainer, a novel method for generating
local, post-hoc explanations of embedded spaces in machine learning models. Our
approach adapts saliency-based techniques from RISE to explain the distance
between two embedded data points by assigning attribution values through
selective masking and distance-ranked mask filtering. We evaluate Distance
Explainer on cross-modal embeddings (image-image and image-caption pairs) using
established XAI metrics including Faithfulness, Sensitivity/Robustness, and
Randomization. Experiments with ImageNet and CLIP models demonstrate that our
method effectively identifies features contributing to similarity or
dissimilarity between embedded data points while maintaining high robustness
and consistency. We also explore how parameter tuning, particularly mask
quantity and selection strategy, affects explanation quality. This work
addresses a critical gap in XAI research and enhances transparency and
trustworthiness in deep learning applications utilizing embedded spaces.",http://arxiv.org/abs/2505.15516v1
AM-PPO: (Advantage) Alpha-Modulation with Proximal Policy Optimization,"Proximal Policy Optimization (PPO) is a widely used reinforcement learning
algorithm that heavily relies on accurate advantage estimates for stable and
efficient training. However, raw advantage signals can exhibit significant
variance, noise, and scale-related issues, impeding optimal learning
performance. To address this challenge, we introduce Advantage Modulation PPO
(AM-PPO), a novel enhancement of PPO that adaptively modulates advantage
estimates using a dynamic, non-linear scaling mechanism. This adaptive
modulation employs an alpha controller that dynamically adjusts the scaling
factor based on evolving statistical properties of the advantage signals, such
as their norm, variance, and a predefined target saturation level. By
incorporating a tanh-based gating function driven by these adaptively scaled
advantages, AM-PPO reshapes the advantage signals to stabilize gradient updates
and improve the conditioning of the policy gradient landscape. Crucially, this
modulation also influences value function training by providing consistent and
adaptively conditioned learning targets. Empirical evaluations across standard
continuous control benchmarks demonstrate that AM-PPO achieves superior reward
trajectories, exhibits sustained learning progression, and significantly
reduces the clipping required by adaptive optimizers. These findings underscore
the potential of advantage modulation as a broadly applicable technique for
enhancing reinforcement learning optimization.",http://arxiv.org/abs/2505.15514v1
NOMAD Projection,"The rapid adoption of generative AI has driven an explosion in the size of
datasets consumed and produced by AI models. Traditional methods for
unstructured data visualization, such as t-SNE and UMAP, have not kept up with
the pace of dataset scaling. This presents a significant challenge for AI
explainability, which relies on methods such as t-SNE and UMAP for exploratory
data analysis. In this paper, we introduce Negative Or Mean Affinity
Discrimination (NOMAD) Projection, the first method for unstructured data
visualization via nonlinear dimensionality reduction that can run on multiple
GPUs at train time. We provide theory that situates NOMAD Projection as an
approximate upper bound on the InfoNC-t-SNE loss, and empirical results that
demonstrate NOMAD Projection's superior performance and speed profile compared
to existing state-of-the-art methods. We demonstrate the scalability of NOMAD
Projection by computing the first complete data map of Multilingual Wikipedia.",http://arxiv.org/abs/2505.15511v1
Directional Non-Commutative Monoidal Structures for Compositional Embeddings in Machine Learning,"We introduce a new algebraic structure for multi-dimensional compositional
embeddings, built on directional non-commutative monoidal operators. The core
contribution of this work is this novel framework, which exhibits appealing
theoretical properties (associativity along each dimension and an interchange
law ensuring global consistency) while remaining compatible with modern machine
learning architectures. Our construction defines a distinct composition
operator circ_i for each axis i, ensuring associative combination along each
axis without imposing global commutativity. Importantly, all axis-specific
operators commute with one another, enforcing a global interchange law that
enables consistent crossaxis compositions. This is, to our knowledge, the first
approach that provides a common foundation that generalizes classical
sequence-modeling paradigms (e.g., structured state-space models (SSMs) and
transformer self-attention) to a unified multi-dimensional framework. For
example, specific one-dimensional instances of our framework can recover the
familiar affine transformation algebra, vanilla self-attention, and the
SSM-style recurrence. The higher-dimensional generalizations naturally support
recursive, structure-aware operations in embedding spaces. We outline several
potential applications unlocked by this structure-including structured
positional encodings in Transformers, directional image embeddings, and
symbolic modeling of sequences or grids-indicating that it could inform future
deep learning model designs. We formally establish the algebraic properties of
our framework and discuss efficient implementations. Finally, as our focus is
theoretical, we include no experiments here and defer empirical validation to
future work, which we plan to undertake.",http://arxiv.org/abs/2505.15507v1
Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts,"Recently, Vision-Language foundation models like CLIP and ALIGN, which are
pre-trained on large-scale data have shown remarkable zero-shot generalization
to diverse datasets with different classes and even domains. In this work, we
take a step further and analyze whether these models can be adapted to target
datasets having very different distributions and classes compared to what these
models have been trained on, using only a few labeled examples from the target
dataset. In such scenarios, finetuning large pretrained models is challenging
due to problems of overfitting as well as loss of generalization, and has not
been well explored in prior literature. Since, the pre-training data of such
models are unavailable, it is difficult to comprehend the performance on
various downstream datasets. First, we try to answer the question: Given a
target dataset with a few labelled examples, can we estimate whether further
fine-tuning can enhance the performance compared to zero-shot evaluation? by
analyzing the common vision-language embedding space. Based on the analysis, we
propose a novel prompt-tuning method, PromptMargin for adapting such
large-scale VLMs directly on the few target samples. PromptMargin effectively
tunes the text as well as visual prompts for this task, and has two main
modules: 1) Firstly, we use a selective augmentation strategy to complement the
few training samples in each task; 2) Additionally, to ensure robust training
in the presence of unfamiliar class names, we increase the inter-class margin
for improved class discrimination using a novel Multimodal Margin Regularizer.
Extensive experiments and analysis across fifteen target benchmark datasets,
with varying degrees of distribution shifts from natural images, shows the
effectiveness of the proposed framework over the existing state-of-the-art
approaches applied to this setting. github.com/debarshigit/PromptMargin.",http://arxiv.org/abs/2505.15506v1
Coloring Between the Lines: Personalization in the Null Space of Planning Constraints,"Generalist robots must personalize in-the-wild to meet the diverse needs and
preferences of long-term users. How can we enable flexible personalization
without sacrificing safety or competency? This paper proposes Coloring Between
the Lines (CBTL), a method for personalization that exploits the null space of
constraint satisfaction problems (CSPs) used in robot planning. CBTL begins
with a CSP generator that ensures safe and competent behavior, then
incrementally personalizes behavior by learning parameterized constraints from
online interaction. By quantifying uncertainty and leveraging the
compositionality of planning constraints, CBTL achieves sample-efficient
adaptation without environment resets. We evaluate CBTL in (1) three diverse
simulation environments; (2) a web-based user study; and (3) a real-robot
assisted feeding system, finding that CBTL consistently achieves more effective
personalization with fewer interactions than baselines. Our results demonstrate
that CBTL provides a unified and practical approach for continual, flexible,
active, and safe robot personalization. Website:
https://emprise.cs.cornell.edu/cbtl/",http://arxiv.org/abs/2505.15503v1
Certified Neural Approximations of Nonlinear Dynamics,"Neural networks hold great potential to act as approximate models of
nonlinear dynamical systems, with the resulting neural approximations enabling
verification and control of such systems. However, in safety-critical contexts,
the use of neural approximations requires formal bounds on their closeness to
the underlying system. To address this fundamental challenge, we propose a
novel, adaptive, and parallelizable verification method based on certified
first-order models. Our approach provides formal error bounds on the neural
approximations of dynamical systems, allowing them to be safely employed as
surrogates by interpreting the error bound as bounded disturbances acting on
the approximated dynamics. We demonstrate the effectiveness and scalability of
our method on a range of established benchmarks from the literature, showing
that it outperforms the state-of-the-art. Furthermore, we highlight the
flexibility of our framework by applying it to two novel scenarios not
previously explored in this context: neural network compression and an
autoencoder-based deep learning architecture for learning Koopman operators,
both yielding compelling results.",http://arxiv.org/abs/2505.15497v1
Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes,"We present new fast-rate generalization bounds for multi-task and
meta-learning in the unbalanced setting, i.e. when the tasks have training sets
of different sizes, as is typically the case in real-world scenarios.
Previously, only standard-rate bounds were known for this situation, while
fast-rate bounds were limited to the setting where all training sets are of
equal size. Our new bounds are numerically computable as well as interpretable,
and we demonstrate their flexibility in handling a number of cases where they
give stronger guarantees than previous bounds. Besides the bounds themselves,
we also make conceptual contributions: we demonstrate that the unbalanced
multi-task setting has different statistical properties than the balanced
situation, specifically that proofs from the balanced situation do not carry
over to the unbalanced setting. Additionally, we shed light on the fact that
the unbalanced situation allows two meaningful definitions of multi-task risk,
depending on whether if all tasks should be considered equally important or if
sample-rich tasks should receive more weight than sample-poor ones.",http://arxiv.org/abs/2505.15496v1
Machine Learning Derived Blood Input for Dynamic PET Images of Rat Heart,"Dynamic FDG PET imaging study of n = 52 rats including 26 control
Wistar-Kyoto (WKY) rats and 26 experimental spontaneously hypertensive rats
(SHR) were performed using a Siemens microPET and Albira trimodal scanner
longitudinally at 1, 2, 3, 5, 9, 12 and 18 months of age. A 15-parameter dual
output model correcting for spill over contamination and partial volume effects
with peak fitting cost functions was developed for simultaneous estimation of
model corrected blood input function (MCIF) and kinetic rate constants for
dynamic FDG PET images of rat heart in vivo. Major drawbacks of this model are
its dependence on manual annotations for the Image Derived Input Function
(IDIF) and manual determination of crucial model parameters to compute MCIF. To
overcome these limitations, we performed semi-automated segmentation and then
formulated a Long-Short-Term Memory (LSTM) cell network to train and predict
MCIF in test data using a concatenation of IDIFs and myocardial inputs and
compared them with reference-modeled MCIF. Thresholding along 2D plane slices
with two thresholds, with T1 representing high-intensity myocardium, and T2
representing lower-intensity rings, was used to segment the area of the LV
blood pool. The resultant IDIF and myocardial TACs were used to compute the
corresponding reference (model) MCIF for all data sets. The segmented IDIF and
the myocardium formed the input for the LSTM network. A k-fold cross validation
structure with a 33:8:11 split and 5 folds was utilized to create the model and
evaluate the performance of the LSTM network for all datasets. To overcome the
sparseness of data as time steps increase, midpoint interpolation was utilized
to increase the density of datapoints beyond time = 10 minutes. The model
utilizing midpoint interpolation was able to achieve a 56.4% improvement over
previous Mean Squared Error (MSE).",http://arxiv.org/abs/2505.15488v1
AI-based Decision Support System for Heritage Aircraft Corrosion Prevention,"The paper presents a decision support system for the long-term preservation
of aeronautical heritage exhibited/stored in sheltered sites. The aeronautical
heritage is characterized by diverse materials of which this heritage is
constituted. Heritage aircraft are made of ancient aluminum alloys, (ply)wood,
and particularly fabrics. The decision support system (DSS) designed, starting
from a conceptual model, is knowledge-based on degradation/corrosion mechanisms
of prevailing materials of aeronautical heritage. In the case of historical
aircraft wooden parts, this knowledge base is filled in by the damage function
models developed within former European projects. Model-based corrosion
prediction is implemented within the new DSS for ancient aluminum alloys. The
novelty of this DSS consists of supporting multi-material heritage protection
and tailoring to peculiarities of aircraft exhibition/storage hangars and the
needs of aviation museums. The novel DSS is tested on WWII aircraft heritage
exhibited in the Aviation Museum Kbely, Military History Institute Prague,
Czech Republic.",http://arxiv.org/abs/2505.15462v1
AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs,"Uncertainty estimation remains a critical challenge in adapting pre-trained
language models to classification tasks, particularly under parameter-efficient
fine-tuning approaches such as adapters. We introduce AdUE1, an efficient
post-hoc uncertainty estimation (UE) method, to enhance softmax-based
estimates. Our approach (1) uses a differentiable approximation of the maximum
function and (2) applies additional regularization through L2-SP, anchoring the
fine-tuned head weights and regularizing the model. Evaluations on five NLP
classification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2,
Qwen) demonstrate that our method consistently outperforms established
baselines such as Mahalanobis distance and softmax response. Our approach is
lightweight (no base-model changes) and produces better-calibrated confidence.",http://arxiv.org/abs/2505.15443v1
Stronger ViTs With Octic Equivariance,"Recent efforts at scaling computer vision models have established Vision
Transformers (ViTs) as the leading architecture. ViTs incorporate weight
sharing over image patches as an important inductive bias. In this work, we
show that ViTs benefit from incorporating equivariance under the octic group,
i.e., reflections and 90-degree rotations, as a further inductive bias. We
develop new architectures, octic ViTs, that use octic-equivariant layers and
put them to the test on both supervised and self-supervised learning. Through
extensive experiments on DeiT-III and DINOv2 training on ImageNet-1K, we show
that octic ViTs yield more computationally efficient networks while also
improving performance. In particular, we achieve approximately 40% reduction in
FLOPs for ViT-H while simultaneously improving both classification and
segmentation results.",http://arxiv.org/abs/2505.15441v2
Adaptive Temperature Scaling with Conformal Prediction,"Conformal prediction enables the construction of high-coverage prediction
sets for any pre-trained model, guaranteeing that the true label lies within
the set with a specified probability. However, these sets do not provide
probability estimates for individual labels, limiting their practical use. In
this paper, we propose, to the best of our knowledge, the first method for
assigning calibrated probabilities to elements of a conformal prediction set.
Our approach frames this as an adaptive calibration problem, selecting an
input-specific temperature parameter to match the desired coverage level.
Experiments on several challenging image classification datasets demonstrate
that our method maintains coverage guarantees while significantly reducing
expected calibration error.",http://arxiv.org/abs/2505.15437v1
Set-LLM: A Permutation-Invariant LLM,"While large language models (LLMs) demonstrate impressive capabilities across
numerous applications, their robustness remains a critical concern. This paper
is motivated by a specific vulnerability: the order sensitivity of LLMs. This
vulnerability manifests itself as the order bias observed when LLMs decide
between possible options (for example, a preference for the first option) and
the tendency of LLMs to provide different answers when options are reordered.
The use cases for this scenario extend beyond the classical case of
multiple-choice question answering to the use of LLMs as automated evaluators
in AI pipelines, comparing output generated by different models. We introduce
Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the
processing of mixed set-text inputs with permutation invariance guarantees. The
adaptations involve a new attention mask and new positional encodings
specifically designed for sets. We provide a theoretical proof of invariance
and demonstrate through experiments that Set-LLM can be trained effectively,
achieving comparable or improved performance and maintaining the runtime of the
original model, while eliminating order sensitivity.",http://arxiv.org/abs/2505.15433v1
Uncertainty Quantification in SVM prediction,"This paper explores Uncertainty Quantification (UQ) in SVM predictions,
particularly for regression and forecasting tasks. Unlike the Neural Network,
the SVM solutions are typically more stable, sparse, optimal and interpretable.
However, there are only few literature which addresses the UQ in SVM
prediction. At first, we provide a comprehensive summary of existing Prediction
Interval (PI) estimation and probabilistic forecasting methods developed in the
SVM framework and evaluate them against the key properties expected from an
ideal PI model. We find that none of the existing SVM PI models achieves a
sparse solution. To introduce sparsity in SVM model, we propose the Sparse
Support Vector Quantile Regression (SSVQR) model, which constructs PIs and
probabilistic forecasts by solving a pair of linear programs. Further, we
develop a feature selection algorithm for PI estimation using SSVQR that
effectively eliminates a significant number of features while improving PI
quality in case of high-dimensional dataset. Finally we extend the SVM models
in Conformal Regression setting for obtaining more stable prediction set with
finite test set guarantees. Extensive experiments on artificial, real-world
benchmark datasets compare the different characteristics of both existing and
proposed SVM-based PI estimation methods and also highlight the advantages of
the feature selection in PI estimation. Furthermore, we compare both, the
existing and proposed SVM-based PI estimation models, with modern deep learning
models for probabilistic forecasting tasks on benchmark datasets. Furthermore,
SVM models show comparable or superior performance to modern complex deep
learning models for probabilistic forecasting task in our experiments.",http://arxiv.org/abs/2505.15429v1
SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding,"Capturing nonlinear relationships without sacrificing interpretability
remains a persistent challenge in regression modeling. We introduce SplitWise,
a novel framework that enhances stepwise regression. It adaptively transforms
numeric predictors into threshold-based binary features using shallow decision
trees, but only when such transformations improve model fit, as assessed by the
Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC).
This approach preserves the transparency of linear models while flexibly
capturing nonlinear effects. Implemented as a user-friendly R package,
SplitWise is evaluated on both synthetic and real-world datasets. The results
show that it consistently produces more parsimonious and generalizable models
than traditional stepwise and penalized regression techniques.",http://arxiv.org/abs/2505.15423v1
Guided Policy Optimization under Partial Observability,"Reinforcement Learning (RL) in partially observable environments poses
significant challenges due to the complexity of learning under uncertainty.
While additional information, such as that available in simulations, can
enhance training, effectively leveraging it remains an open problem. To address
this, we introduce Guided Policy Optimization (GPO), a framework that co-trains
a guider and a learner. The guider takes advantage of privileged information
while ensuring alignment with the learner's policy that is primarily trained
via imitation learning. We theoretically demonstrate that this learning scheme
achieves optimality comparable to direct RL, thereby overcoming key limitations
inherent in existing approaches. Empirical evaluations show strong performance
of GPO across various tasks, including continuous control with partial
observability and noise, and memory-based challenges, significantly
outperforming existing methods.",http://arxiv.org/abs/2505.15418v1
Robust Multimodal Learning via Entropy-Gated Contrastive Fusion,"Real-world multimodal systems routinely face missing-input scenarios, and in
reality, robots lose audio in a factory or a clinical record omits lab tests at
inference time. Standard fusion layers either preserve robustness or
calibration but never both. We introduce Adaptive Entropy-Gated Contrastive
Fusion (AECF), a single light-weight layer that (i) adapts its entropy
coefficient per instance, (ii) enforces monotone calibration across all
modality subsets, and (iii) drives a curriculum mask directly from
training-time entropy. On AV-MNIST and MS-COCO, AECF improves masked-input mAP
by +18 pp at a 50% drop rate while reducing ECE by up to 200%, yet adds 1%
run-time. All back-bones remain frozen, making AECF an easy drop-in layer for
robust, calibrated multimodal inference.",http://arxiv.org/abs/2505.15417v1
SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval,"Despite the dominance of convolutional and transformer-based architectures in
image-to-image retrieval, these models are prone to biases arising from
low-level visual features, such as color. Recognizing the lack of semantic
understanding as a key limitation, we propose a novel scene graph-based
retrieval framework that emphasizes semantic content over superficial image
characteristics. Prior approaches to scene graph retrieval predominantly rely
on supervised Graph Neural Networks (GNNs), which require ground truth graph
pairs driven from image captions. However, the inconsistency of caption-based
supervision stemming from variable text encodings undermine retrieval
reliability. To address these, we present SCENIR, a Graph Autoencoder-based
unsupervised retrieval framework, which eliminates the dependence on labeled
training data. Our model demonstrates superior performance across metrics and
runtime efficiency, outperforming existing vision-based, multimodal, and
supervised GNN approaches. We further advocate for Graph Edit Distance (GED) as
a deterministic and robust ground truth measure for scene graph similarity,
replacing the inconsistent caption-based alternatives for the first time in
image-to-image retrieval evaluation. Finally, we validate the generalizability
of our method by applying it to unannotated datasets via automated scene graph
generation, while substantially contributing in advancing state-of-the-art in
counterfactual image retrieval.",http://arxiv.org/abs/2505.15867v1
Multi-omic Causal Discovery using Genotypes and Gene Expression,"Causal discovery in multi-omic datasets is crucial for understanding the
bigger picture of gene regulatory mechanisms, but remains challenging due to
high dimensionality, differentiation of direct from indirect relationships, and
hidden confounders. We introduce GENESIS (GEne Network inference from
Expression SIgnals and SNPs), a constraint-based algorithm that leverages the
natural causal precedence of genotypes to infer ancestral relationships in
transcriptomic data. Unlike traditional causal discovery methods that start
with a fully connected graph, GENESIS initialises an empty ancestrality matrix
and iteratively populates it with direct, indirect or non-causal relationships
using a series of provably sound marginal and conditional independence tests.
By integrating genotypes as fixed causal anchors, GENESIS provides a principled
``head start'' to classical causal discovery algorithms, restricting the search
space to biologically plausible edges. We test GENESIS on synthetic and
real-world genomic datasets. This framework offers a powerful avenue for
uncovering causal pathways in complex traits, with promising applications to
functional genomics, drug discovery, and precision medicine.",http://arxiv.org/abs/2505.15866v1
Efficient Differentiable Approximation of Generalized Low-rank Regularization,"Low-rank regularization (LRR) has been widely applied in various machine
learning tasks, but the associated optimization is challenging. Directly
optimizing the rank function under constraints is NP-hard in general. To
overcome this difficulty, various relaxations of the rank function were
studied. However, optimization of these relaxed LRRs typically depends on
singular value decomposition, which is a time-consuming and nondifferentiable
operator that cannot be optimized with gradient-based techniques. To address
these challenges, in this paper we propose an efficient differentiable
approximation of the generalized LRR. The considered LRR form subsumes many
popular choices like the nuclear norm, the Schatten-$p$ norm, and various
nonconvex relaxations. Our method enables LRR terms to be appended to loss
functions in a plug-and-play fashion, and the GPU-friendly operations enable
efficient and convenient implementation. Furthermore, convergence analysis is
presented, which rigorously shows that both the bias and the variance of our
rank estimator rapidly reduce with increased sample size and iteration steps.
In the experimental study, the proposed method is applied to various tasks,
which demonstrates its versatility and efficiency. Code is available at
https://github.com/naiqili/EDLRR.",http://arxiv.org/abs/2505.15407v1
HOPSE: Scalable Higher-Order Positional and Structural Encoder for Combinatorial Representations,"While Graph Neural Networks (GNNs) have proven highly effective at modeling
relational data, pairwise connections cannot fully capture multi-way
relationships naturally present in complex real-world systems. In response to
this, Topological Deep Learning (TDL) leverages more general combinatorial
representations -- such as simplicial or cellular complexes -- to accommodate
higher-order interactions. Existing TDL methods often extend GNNs through
Higher-Order Message Passing (HOMP), but face critical \emph{scalability
challenges} due to \textit{(i)} a combinatorial explosion of message-passing
routes, and \textit{(ii)} significant complexity overhead from the propagation
mechanism. To overcome these limitations, we propose HOPSE (Higher-Order
Positional and Structural Encoder) -- a \emph{message passing-free} framework
that uses Hasse graph decompositions to derive efficient and expressive
encodings over \emph{arbitrary higher-order domains}. Notably, HOPSE scales
linearly with dataset size while preserving expressive power and permutation
equivariance. Experiments on molecular, expressivity and topological benchmarks
show that HOPSE matches or surpasses state-of-the-art performance while
achieving up to 7 $times$ speedups over HOMP-based models, opening a new path
for scalable TDL.",http://arxiv.org/abs/2505.15405v1
InTreeger: An End-to-End Framework for Integer-Only Decision Tree Inference,"Integer quantization has emerged as a critical technique to facilitate
deployment on resource-constrained devices. Although they do reduce the
complexity of the learning models, their inference performance is often prone
to quantization-induced errors. To this end, we introduce InTreeger: an
end-to-end framework that takes a training dataset as input, and outputs an
architecture-agnostic integer-only C implementation of tree-based machine
learning model, without loss of precision. This framework enables anyone, even
those without prior experience in machine learning, to generate a highly
optimized integer-only classification model that can run on any hardware simply
by providing an input dataset and target variable. We evaluated our generated
implementations across three different architectures (ARM, x86, and RISC-V),
resulting in significant improvements in inference latency. In addition, we
show the energy efficiency compared to typical decision tree implementations
that rely on floating-point arithmetic. The results underscore the advantages
of integer-only inference, making it particularly suitable for energy- and
area-constrained devices such as embedded systems and edge computing platforms,
while also enabling the execution of decision trees on existing ultra-low power
devices.",http://arxiv.org/abs/2505.15391v1
Inter-Subject Variance Transfer Learning for EMG Pattern Classification Based on Bayesian Inference,"In electromyogram (EMG)-based motion recognition, a subject-specific
classifier is typically trained with sufficient labeled data. However, this
process demands extensive data collection over extended periods, burdening the
subject. To address this, utilizing information from pre-training on multiple
subjects for the training of the target subject could be beneficial. This paper
proposes an inter-subject variance transfer learning method based on a Bayesian
approach. This method is founded on the simple hypothesis that while the means
of EMG features vary greatly across subjects, their variances may exhibit
similar patterns. Our approach transfers variance information, acquired through
pre-training on multiple source subjects, to a target subject within a Bayesian
updating framework, thereby allowing accurate classification using limited
target calibration data. A coefficient was also introduced to adjust the amount
of information transferred for efficient transfer learning. Experimental
evaluations using two EMG datasets demonstrated the effectiveness of our
variance transfer strategy and its superiority compared to existing methods.",http://arxiv.org/abs/2505.15381v1
Distributionally Robust Federated Learning with Client Drift Minimization,"Federated learning (FL) faces critical challenges, particularly in
heterogeneous environments where non-independent and identically distributed
data across clients can lead to unfair and inefficient model performance. In
this work, we introduce \textit{DRDM}, a novel algorithm that addresses these
issues by combining a distributionally robust optimization (DRO) framework with
dynamic regularization to mitigate client drift. \textit{DRDM} frames the
training as a min-max optimization problem aimed at maximizing performance for
the worst-case client, thereby promoting robustness and fairness. This robust
objective is optimized through an algorithm leveraging dynamic regularization
and efficient local updates, which significantly reduces the required number of
communication rounds. Moreover, we provide a theoretical convergence analysis
for convex smooth objectives under partial participation. Extensive experiments
on three benchmark datasets, covering various model architectures and data
heterogeneity levels, demonstrate that \textit{DRDM} significantly improves
worst-case test accuracy while requiring fewer communication rounds than
existing state-of-the-art baselines. Furthermore, we analyze the impact of
signal-to-noise ratio (SNR) and bandwidth on the energy consumption of
participating clients, demonstrating that the number of local update steps can
be adaptively selected to achieve a target worst-case test accuracy with
minimal total energy cost across diverse communication environments.",http://arxiv.org/abs/2505.15371v1
Decoding Phone Pairs from MEG Signals Across Speech Modalities,"Understanding the neural mechanisms underlying speech production is essential
for both advancing cognitive neuroscience theory and developing practical
communication technologies. In this study, we investigated
magnetoencephalography signals to decode phones from brain activity during
speech production and perception (passive listening and voice playback) tasks.
Using a dataset comprising 17 participants, we performed pairwise phone
classification, extending our analysis to 15 phonetic pairs. Multiple machine
learning approaches, including regularized linear models and neural network
architectures, were compared to determine their effectiveness in decoding
phonetic information. Our results demonstrate significantly higher decoding
accuracy during speech production (76.6%) compared to passive listening and
playback modalities (~51%), emphasizing the richer neural information available
during overt speech. Among the models, the Elastic Net classifier consistently
outperformed more complex neural networks, highlighting the effectiveness of
traditional regularization techniques when applied to limited and
high-dimensional MEG datasets. Besides, analysis of specific brain frequency
bands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz)
and Theta (4-7 Hz), contributed the most substantially to decoding accuracy,
suggesting that these bands encode critical speech production-related neural
processes. Despite using advanced denoising methods, it remains unclear whether
decoding solely reflects neural activity or if residual muscular or movement
artifacts also contributed, indicating the need for further methodological
refinement. Overall, our findings underline the critical importance of
examining overt speech production paradigms, which, despite their complexity,
offer opportunities to improve brain-computer interfaces to help individuals
with severe speech impairments.",http://arxiv.org/abs/2505.15355v1
Human in the Loop Adaptive Optimization for Improved Time Series Forecasting,"Time series forecasting models often produce systematic, predictable errors
even in critical domains such as energy, finance, and healthcare. We introduce
a novel post training adaptive optimization framework that improves forecast
accuracy without retraining or architectural changes. Our method automatically
applies expressive transformations optimized via reinforcement learning,
contextual bandits, or genetic algorithms to correct model outputs in a
lightweight and model agnostic way. Theoretically, we prove that affine
corrections always reduce the mean squared error; practically, we extend this
idea with dynamic action based optimization. The framework also supports an
optional human in the loop component: domain experts can guide corrections
using natural language, which is parsed into actions by a language model.
Across multiple benchmarks (e.g., electricity, weather, traffic), we observe
consistent accuracy gains with minimal computational overhead. Our interactive
demo shows the framework's real time usability. By combining automated post hoc
refinement with interpretable and extensible mechanisms, our approach offers a
powerful new direction for practical forecasting systems.",http://arxiv.org/abs/2505.15354v1
The Super Emotion Dataset,"Despite the wide-scale usage and development of emotion classification
datasets in NLP, the field lacks a standardized, large-scale resource that
follows a psychologically grounded taxonomy. Existing datasets either use
inconsistent emotion categories, suffer from limited sample size, or focus on
specific domains. The Super Emotion Dataset addresses this gap by harmonizing
diverse text sources into a unified framework based on Shaver's empirically
validated emotion taxonomy, enabling more consistent cross-domain emotion
recognition research.",http://arxiv.org/abs/2505.15348v1
Hadamax Encoding: Elevating Performance in Model-Free Atari,"Neural network architectures have a large impact in machine learning. In
reinforcement learning, network architectures have remained notably simple, as
changes often lead to small gains in performance. This work introduces a novel
encoder architecture for pixel-based model-free reinforcement learning. The
Hadamax (\textbf{Hada}mard \textbf{max}-pooling) encoder achieves
state-of-the-art performance by max-pooling Hadamard products between
GELU-activated parallel hidden layers. Based on the recent PQN algorithm, the
Hadamax encoder achieves state-of-the-art model-free performance in the
Atari-57 benchmark. Specifically, without applying any algorithmic
hyperparameter modifications, Hadamax-PQN achieves an 80\% performance gain
over vanilla PQN and significantly surpasses Rainbow-DQN. For reproducibility,
the full code is available on
\href{https://github.com/Jacobkooi/Hadamax}{GitHub}.",http://arxiv.org/abs/2505.15345v1
Policy Testing in Markov Decision Processes,"We study the policy testing problem in discounted Markov decision processes
(MDPs) under the fixed-confidence setting. The goal is to determine whether the
value of a given policy exceeds a specified threshold while minimizing the
number of observations. We begin by deriving an instance-specific lower bound
that any algorithm must satisfy. This lower bound is characterized as the
solution to an optimization problem with non-convex constraints. We propose a
policy testing algorithm inspired by this optimization problem--a common
approach in pure exploration problems such as best-arm identification, where
asymptotically optimal algorithms often stem from such optimization-based
characterizations. As for other pure exploration tasks in MDPs, however, the
non-convex constraints in the lower-bound problem present significant
challenges, raising doubts about whether statistically optimal and
computationally tractable algorithms can be designed. To address this, we
reformulate the lower-bound problem by interchanging the roles of the objective
and the constraints, yielding an alternative problem with a non-convex
objective but convex constraints. Strikingly, this reformulated problem admits
an interpretation as a policy optimization task in a newly constructed reversed
MDP. Leveraging recent advances in policy gradient methods, we efficiently
solve this problem and use it to design a policy testing algorithm that is
statistically optimal--matching the instance-specific lower bound on sample
complexity--while remaining computationally tractable. We validate our approach
with numerical experiments.",http://arxiv.org/abs/2505.15342v1
SSR: Speculative Parallel Scaling Reasoning in Test-time,"Large language models (LLMs) have achieved impressive results on multi-step
mathematical reasoning, yet at the cost of high computational overhead. This
challenge is particularly acute for test-time scaling methods such as parallel
decoding, which increase answer diversity but scale poorly in efficiency. To
address this efficiency-accuracy trade-off, we propose SSR (Speculative
Parallel Scaling Reasoning), a training-free framework that leverages a key
insight: by introducing speculative decoding at the step level, we can
accelerate reasoning without sacrificing correctness. SSR integrates two
components: a Selective Parallel Module (SPM) that identifies a small set of
promising reasoning strategies via model-internal scoring, and Step-level
Speculative Decoding (SSD), which enables efficient draft-target collaboration
for fine-grained reasoning acceleration. Experiments on three mathematical
benchmarks-AIME 2024, MATH-500, and LiveMathBench - demonstrate that SSR
achieves strong gains over baselines. For instance, on LiveMathBench, SSR
improves pass@1 accuracy by 13.84% while reducing computation to 80.5% of the
baseline FLOPs. On MATH-500, SSR reduces compute to only 30% with no loss in
accuracy.",http://arxiv.org/abs/2505.15340v1
Fourier-Invertible Neural Encoder (FINE) for Homogeneous Flows,"Invertible neural architectures have recently attracted attention for their
compactness, interpretability, and information-preserving properties. In this
work, we propose the Fourier-Invertible Neural Encoder (FINE), which combines
invertible monotonic activation functions with reversible filter structures,
and could be extended using Invertible ResNets. This architecture is examined
in learning low-dimensional representations of one-dimensional nonlinear wave
interactions and exact circular translation symmetry. Dimensionality is
preserved across layers, except for a Fourier truncation step in the latent
space, which enables dimensionality reduction while maintaining shift
equivariance and interpretability. Our results demonstrate that FINE
significantly outperforms classical linear methods such as Discrete Fourier
Transformation (DFT) and Proper Orthogonal Decomposition (POD), and achieves
reconstruction accuracy better than conventional deep autoencoders with
convolutional layers (CNN) - while using substantially smaller models and
offering superior physical interpretability. These findings suggest that
invertible single-neuron networks, when combined with spectral truncation,
offer a promising framework for learning compact and interpretable
representations of physics datasets, and symmetry-aware representation learning
in physics-informed machine learning.",http://arxiv.org/abs/2505.15329v1
Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting,"Multivariable time series forecasting methods can integrate information from
exogenous variables, leading to significant prediction accuracy gains.
Transformer architecture has been widely applied in various time series
forecasting models due to its ability to capture long-range sequential
dependencies. However, a na\""ive application of transformers often struggles to
effectively model complex relationships among variables over time. To mitigate
against this, we propose a novel architecture, namely the Spectral Operator
Neural Network (Sonnet). Sonnet applies learnable wavelet transformations to
the input and incorporates spectral analysis using the Koopman operator. Its
predictive skill relies on the Multivariable Coherence Attention (MVCA), an
operation that leverages spectral coherence to model variable dependencies. Our
empirical analysis shows that Sonnet yields the best performance on $34$ out of
$47$ forecasting tasks with an average mean absolute error (MAE) reduction of
$1.1\%$ against the most competitive baseline (different per task). We further
show that MVCA -- when put in place of the na\""ive attention used in various
deep learning models -- can remedy its deficiencies, reducing MAE by $10.7\%$
on average in the most challenging forecasting tasks.",http://arxiv.org/abs/2505.15312v1
Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning,"Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.",http://arxiv.org/abs/2505.15311v1
Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak Reinforcement Learning Agents into a Supreme One,"Model ensemble is a useful approach in reinforcement learning (RL) for
training effective agents. Despite wide success of RL, training effective
agents remains difficult due to the multitude of factors requiring careful
tuning, such as algorithm selection, hyperparameter settings, and even random
seed choices, all of which can significantly influence an agent's performance.
Model ensemble helps overcome this challenge by combining multiple weak agents
into a single, more powerful one, enhancing overall performance. However,
existing ensemble methods, such as majority voting and Boltzmann addition, are
designed as fixed strategies and lack a semantic understanding of specific
tasks, limiting their adaptability and effectiveness. To address this, we
propose LLM-Ens, a novel approach that enhances RL model ensemble with
task-specific semantic understandings driven by large language models (LLMs).
Given a task, we first design an LLM to categorize states in this task into
distinct 'situations', incorporating high-level descriptions of the task
conditions. Then, we statistically analyze the strengths and weaknesses of each
individual agent to be used in the ensemble in each situation. During the
inference time, LLM-Ens dynamically identifies the changing task situation and
switches to the agent that performs best in the current situation, ensuring
dynamic model selection in the evolving task condition. Our approach is
designed to be compatible with agents trained with different random seeds,
hyperparameter settings, and various RL algorithms. Extensive experiments on
the Atari benchmark show that LLM-Ens significantly improves the RL model
ensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,
our code is open-source at
https://anonymous.4open.science/r/LLM4RLensemble-F7EE.",http://arxiv.org/abs/2505.15306v1
Laplace Sample Information: Data Informativeness Through a Bayesian Lens,"Accurately estimating the informativeness of individual samples in a dataset
is an important objective in deep learning, as it can guide sample selection,
which can improve model efficiency and accuracy by removing redundant or
potentially harmful samples. We propose Laplace Sample Information (LSI)
measure of sample informativeness grounded in information theory widely
applicable across model architectures and learning settings. LSI leverages a
Bayesian approximation to the weight posterior and the KL divergence to measure
the change in the parameter distribution induced by a sample of interest from
the dataset. We experimentally show that LSI is effective in ordering the data
with respect to typicality, detecting mislabeled samples, measuring class-wise
informativeness, and assessing dataset difficulty. We demonstrate these
capabilities of LSI on image and text data in supervised and unsupervised
settings. Moreover, we show that LSI can be computed efficiently through probes
and transfers well to the training of large models.",http://arxiv.org/abs/2505.15303v1
R3GS: Gaussian Splatting for Robust Reconstruction and Relocalization in Unconstrained Image Collections,"We propose R3GS, a robust reconstruction and relocalization framework
tailored for unconstrained datasets. Our method uses a hybrid representation
during training. Each anchor combines a global feature from a convolutional
neural network (CNN) with a local feature encoded by the multiresolution hash
grids [2]. Subsequently, several shallow multi-layer perceptrons (MLPs) predict
the attributes of each Gaussians, including color, opacity, and covariance. To
mitigate the adverse effects of transient objects on the reconstruction
process, we ffne-tune a lightweight human detection network. Once ffne-tuned,
this network generates a visibility map that efffciently generalizes to other
transient objects (such as posters, banners, and cars) with minimal need for
further adaptation. Additionally, to address the challenges posed by sky
regions in outdoor scenes, we propose an effective sky-handling technique that
incorporates a depth prior as a constraint. This allows the inffnitely distant
sky to be represented on the surface of a large-radius sky sphere,
signiffcantly reducing ffoaters caused by errors in sky reconstruction.
Furthermore, we introduce a novel relocalization method that remains robust to
changes in lighting conditions while estimating the camera pose of a given
image within the reconstructed 3DGS scene. As a result, R3GS significantly
enhances rendering ffdelity, improves both training and rendering efffciency,
and reduces storage requirements. Our method achieves state-of-the-art
performance compared to baseline methods on in-the-wild datasets. The code will
be made open-source following the acceptance of the paper.",http://arxiv.org/abs/2505.15294v1
LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models,"Policy exploration is critical in reinforcement learning (RL), where existing
approaches include greedy, Gaussian process, etc. However, these approaches
utilize preset stochastic processes and are indiscriminately applied in all
kinds of RL tasks without considering task-specific features that influence
policy exploration. Moreover, during RL training, the evolution of such
stochastic processes is rigid, which typically only incorporates a decay in the
variance, failing to adjust flexibly according to the agent's real-time
learning status. Inspired by the analyzing and reasoning capability of large
language models (LLMs), we design LLM-Explorer to adaptively generate
task-specific exploration strategies with LLMs, enhancing the policy
exploration in RL. In our design, we sample the learning trajectory of the
agent during the RL training in a given task and prompt the LLM to analyze the
agent's current policy learning status and then generate a probability
distribution for future policy exploration. Updating the probability
distribution periodically, we derive a stochastic process specialized for the
particular task and dynamically adjusted to adapt to the learning process. Our
design is a plug-in module compatible with various widely applied RL
algorithms, including the DQN series, DDPG, TD3, and any possible variants
developed based on them. Through extensive experiments on the Atari and MuJoCo
benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy
exploration, achieving an average performance improvement up to 37.27%. Our
code is open-source at https://anonymous.4open.science/r/LLM-Explorer-19BE for
reproducibility.",http://arxiv.org/abs/2505.15293v1
Kernel PCA for Out-of-Distribution Detection: Non-Linear Kernel Selections and Approximations,"Out-of-Distribution (OoD) detection is vital for the reliability of deep
neural networks, the key of which lies in effectively characterizing the
disparities between OoD and In-Distribution (InD) data. In this work, such
disparities are exploited through a fresh perspective of non-linear feature
subspace. That is, a discriminative non-linear subspace is learned from InD
features to capture representative patterns of InD, while informative patterns
of OoD features cannot be well captured in such a subspace due to their
different distribution. Grounded on this perspective, we exploit the deviations
of InD and OoD features in such a non-linear subspace for effective OoD
detection. To be specific, we leverage the framework of Kernel Principal
Component Analysis (KPCA) to attain the discriminative non-linear subspace and
deploy the reconstruction error on such subspace to distinguish InD and OoD
data. Two challenges emerge: (i) the learning of an effective non-linear
subspace, i.e., the selection of kernel function in KPCA, and (ii) the
computation of the kernel matrix with large-scale InD data. For the former, we
reveal two vital non-linear patterns that closely relate to the InD-OoD
disparity, leading to the establishment of a Cosine-Gaussian kernel for
constructing the subspace. For the latter, we introduce two techniques to
approximate the Cosine-Gaussian kernel with significantly cheap computations.
In particular, our approximation is further tailored by incorporating the InD
data confidence, which is demonstrated to promote the learning of
discriminative subspaces for OoD data. Our study presents new insights into the
non-linear feature subspace for OoD detection and contributes practical
explorations on the associated kernel design and efficient computations,
yielding a KPCA detection method with distinctively improved efficacy and
efficiency.",http://arxiv.org/abs/2505.15284v1
Learning-based Autonomous Oversteer Control and Collision Avoidance,"Oversteer, wherein a vehicle's rear tires lose traction and induce
unintentional excessive yaw, poses critical safety challenges. Failing to
control oversteer often leads to severe traffic accidents. Although recent
autonomous driving efforts have attempted to handle oversteer through
stabilizing maneuvers, the majority rely on expert-defined trajectories or
assume obstacle-free environments, limiting real-world applicability. This
paper introduces a novel end-to-end (E2E) autonomous driving approach that
tackles oversteer control and collision avoidance simultaneously. Existing E2E
techniques, including Imitation Learning (IL), Reinforcement Learning (RL), and
Hybrid Learning (HL), generally require near-optimal demonstrations or
extensive experience. Yet even skilled human drivers struggle to provide
perfect demonstrations under oversteer, and high transition variance hinders
accumulating sufficient data. Hence, we present Q-Compared Soft Actor-Critic
(QC-SAC), a new HL algorithm that effectively learns from suboptimal
demonstration data and adapts rapidly to new conditions. To evaluate QC-SAC, we
introduce a benchmark inspired by real-world driver training: a vehicle
encounters sudden oversteer on a slippery surface and must avoid randomly
placed obstacles ahead. Experimental results show QC-SAC attains near-optimal
driving policies, significantly surpassing state-of-the-art IL, RL, and HL
baselines. Our method demonstrates the world's first safe autonomous oversteer
control with obstacle avoidance.",http://arxiv.org/abs/2505.15275v1
Scaling Diffusion Transformers Efficiently via $μ$P,"Diffusion Transformers have emerged as the foundation for vision generative
models, but their scalability is limited by the high cost of hyperparameter
(HP) tuning at large scales. Recently, Maximal Update Parametrization ($\mu$P)
was proposed for vanilla Transformers, which enables stable HP transfer from
small to large language models, and dramatically reduces tuning costs. However,
it remains unclear whether $\mu$P of vanilla Transformers extends to diffusion
Transformers, which differ architecturally and objectively. In this work, we
generalize standard $\mu$P to diffusion Transformers and validate its
effectiveness through large-scale experiments. First, we rigorously prove that
$\mu$P of mainstream diffusion Transformers, including DiT, U-ViT,
PixArt-$\alpha$, and MMDiT, aligns with that of the vanilla Transformer,
enabling the direct application of existing $\mu$P methodologies. Leveraging
this result, we systematically demonstrate that DiT-$\mu$P enjoys robust HP
transferability. Notably, DiT-XL-2-$\mu$P with transferred learning rate
achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we
validate the effectiveness of $\mu$P on text-to-image generation by scaling
PixArt-$\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases,
models under $\mu$P outperform their respective baselines while requiring small
tuning cost, only 5.5% of one training run for PixArt-$\alpha$ and 3% of
consumption by human experts for MMDiT-18B. These results establish $\mu$P as a
principled and efficient framework for scaling diffusion Transformers.",http://arxiv.org/abs/2505.15270v1
gen2seg: Generative Models Enable Generalizable Instance Segmentation,"By pretraining to synthesize coherent images from perturbed inputs,
generative models inherently learn to understand object boundaries and scene
compositions. How can we repurpose these generative representations for
general-purpose perceptual organization? We finetune Stable Diffusion and MAE
(encoder+decoder) for category-agnostic instance segmentation using our
instance coloring loss exclusively on a narrow set of object types (indoor
furnishings and cars). Surprisingly, our models exhibit strong zero-shot
generalization, accurately segmenting objects of types and styles unseen in
finetuning (and in many cases, MAE's ImageNet-1K pretraining too). Our
best-performing models closely approach the heavily supervised SAM when
evaluated on unseen object types and styles, and outperform it when segmenting
fine structures and ambiguous boundaries. In contrast, existing promptable
segmentation architectures or discriminatively pretrained models fail to
generalize. This suggests that generative models learn an inherent grouping
mechanism that transfers across categories and domains, even without
internet-scale pretraining. Code, pretrained models, and demos are available on
our website.",http://arxiv.org/abs/2505.15263v1
ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search,"Recent advances in Multimodal Large Language Models (MLLMs) have enabled
autonomous agents to interact with computers via Graphical User Interfaces
(GUIs), where accurately localizing the coordinates of interface elements
(e.g., buttons) is often required for fine-grained actions. However, this
remains significantly challenging, leading prior works to rely on large-scale
web datasets to improve the grounding accuracy. In this work, we propose
Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a
novel and effective framework for web grounding that enables MLLMs to learn
data efficiently through self-generated reasoning and spatial-aware criticism.
More specifically, ReGUIDE learns to (i) self-generate a language reasoning
process for the localization via online reinforcement learning, and (ii)
criticize the prediction using spatial priors that enforce equivariance under
input transformations. At inference time, ReGUIDE further boosts performance
through a test-time scaling strategy, which combines spatial search with
coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly
advances web grounding performance across multiple benchmarks, outperforming
baselines with substantially fewer training data points (e.g., only 0.2%
samples compared to the best open-sourced baselines).",http://arxiv.org/abs/2505.15259v1
An Efficient Private GPT Never Autoregressively Decodes,"The wide deployment of the generative pre-trained transformer (GPT) has
raised privacy concerns for both clients and servers. While cryptographic
primitives can be employed for secure GPT inference to protect the privacy of
both parties, they introduce considerable performance overhead.To accelerate
secure inference, this study proposes a public decoding and secure verification
approach that utilizes public GPT models, motivated by the observation that
securely decoding one and multiple tokens takes a similar latency. The client
uses the public model to generate a set of tokens, which are then securely
verified by the private model for acceptance. The efficiency of our approach
depends on the acceptance ratio of tokens proposed by the public model, which
we improve from two aspects: (1) a private sampling protocol optimized for
cryptographic primitives and (2) model alignment using knowledge distillation.
Our approach improves the efficiency of secure decoding while maintaining the
same level of privacy and generation quality as standard secure decoding.
Experiments demonstrate a $2.1\times \sim 6.0\times$ speedup compared to
standard decoding across three pairs of public-private models and different
network conditions.",http://arxiv.org/abs/2505.15252v1
Loss-Guided Auxiliary Agents for Overcoming Mode Collapse in GFlowNets,"Although Generative Flow Networks (GFlowNets) are designed to capture
multiple modes of a reward function, they often suffer from mode collapse in
practice, getting trapped in early discovered modes and requiring prolonged
training to find diverse solutions. Existing exploration techniques may rely on
heuristic novelty signals. We propose Loss-Guided GFlowNets (LGGFN), a novel
approach where an auxiliary GFlowNet's exploration is directly driven by the
main GFlowNet's training loss. By prioritizing trajectories where the main
model exhibits high loss, LGGFN focuses sampling on poorly understood regions
of the state space. This targeted exploration significantly accelerates the
discovery of diverse, high-reward samples. Empirically, across various
benchmarks including grid environments, structured sequence generation, and
Bayesian structure learning, LGGFN consistently enhances exploration efficiency
and sample diversity compared to baselines. For instance, on a challenging
sequence generation task, it discovered over 40 times more unique valid modes
while simultaneously reducing the exploration error metric by approximately
99\%.",http://arxiv.org/abs/2505.15251v1
Margin-aware Fuzzy Rough Feature Selection: Bridging Uncertainty Characterization and Pattern Classification,"Fuzzy rough feature selection (FRFS) is an effective means of addressing the
curse of dimensionality in high-dimensional data. By removing redundant and
irrelevant features, FRFS helps mitigate classifier overfitting, enhance
generalization performance, and lessen computational overhead. However, most
existing FRFS algorithms primarily focus on reducing uncertainty in pattern
classification, neglecting that lower uncertainty does not necessarily result
in improved classification performance, despite it commonly being regarded as a
key indicator of feature selection effectiveness in the FRFS literature. To
bridge uncertainty characterization and pattern classification, we propose a
Margin-aware Fuzzy Rough Feature Selection (MAFRFS) framework that considers
both the compactness and separation of label classes. MAFRFS effectively
reduces uncertainty in pattern classification tasks, while guiding the feature
selection towards more separable and discriminative label class structures.
Extensive experiments on 15 public datasets demonstrate that MAFRFS is highly
scalable and more effective than FRFS. The algorithms developed using MAFRFS
outperform six state-of-the-art feature selection algorithms.",http://arxiv.org/abs/2505.15250v1
VET-DINO: Learning Anatomical Understanding Through Multi-View Distillation in Veterinary Imaging,"Self-supervised learning has emerged as a powerful paradigm for training deep
neural networks, particularly in medical imaging where labeled data is scarce.
While current approaches typically rely on synthetic augmentations of single
images, we propose VET-DINO, a framework that leverages a unique characteristic
of medical imaging: the availability of multiple standardized views from the
same study. Using a series of clinical veterinary radiographs from the same
patient study, we enable models to learn view-invariant anatomical structures
and develop an implied 3D understanding from 2D projections. We demonstrate our
approach on a dataset of 5 million veterinary radiographs from 668,000 canine
studies. Through extensive experimentation, including view synthesis and
downstream task performance, we show that learning from real multi-view pairs
leads to superior anatomical understanding compared to purely synthetic
augmentations. VET-DINO achieves state-of-the-art performance on various
veterinary imaging tasks. Our work establishes a new paradigm for
self-supervised learning in medical imaging that leverages domain-specific
properties rather than merely adapting natural image techniques.",http://arxiv.org/abs/2505.15248v1
Mitigating Spurious Correlations with Causal Logit Perturbation,"Deep learning has seen widespread success in various domains such as science,
industry, and society. However, it is acknowledged that certain approaches
suffer from non-robustness, relying on spurious correlations for predictions.
Addressing these limitations is of paramount importance, necessitating the
development of methods that can disentangle spurious correlations. {This study
attempts to implement causal models via logit perturbations and introduces a
novel Causal Logit Perturbation (CLP) framework to train classifiers with
generated causal logit perturbations for individual samples, thereby mitigating
the spurious associations between non-causal attributes (i.e., image
backgrounds) and classes.} {Our framework employs a} perturbation network to
generate sample-wise logit perturbations using a series of training
characteristics of samples as inputs. The whole framework is optimized by an
online meta-learning-based learning algorithm and leverages human causal
knowledge by augmenting metadata in both counterfactual and factual manners.
Empirical evaluations on four typical biased learning scenarios, including
long-tail learning, noisy label learning, generalized long-tail learning, and
subpopulation shift learning, demonstrate that CLP consistently achieves
state-of-the-art performance. Moreover, visualization results support the
effectiveness of the generated causal perturbations in redirecting model
attention towards causal image attributes and dismantling spurious
associations.",http://arxiv.org/abs/2505.15246v1
Reliable Vertical Federated Learning in 5G Core Network Architecture,"This work proposes a new algorithm to mitigate model generalization loss in
Vertical Federated Learning (VFL) operating under client reliability
constraints within 5G Core Networks (CNs). Recently studied and endorsed by
3GPP, VFL enables collaborative and load-balanced model training and inference
across the CN. However, the performance of VFL significantly degrades when the
Network Data Analytics Functions (NWDAFs) - which serve as primary clients for
VFL model training and inference - experience reliability issues stemming from
resource constraints and operational overhead. Unlike edge environments, CN
environments adopt fundamentally different data management strategies,
characterized by more centralized data orchestration capabilities. This
presents opportunities to implement better distributed solutions that take full
advantage of the CN data handling flexibility. Leveraging this flexibility, we
propose a method that optimizes the vertical feature split among clients while
centrally defining their local models based on reliability metrics. Our
empirical evaluation demonstrates the effectiveness of our proposed algorithm,
showing improved performance over traditional baseline methods.",http://arxiv.org/abs/2505.15244v1
Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge,"This paper explores generalised probabilistic modelling and uncertainty
estimation in comparative LLM-as-a-judge frameworks. We show that existing
Product-of-Experts methods are specific cases of a broader framework, enabling
diverse modelling options. Furthermore, we propose improved uncertainty
estimates for individual comparisons, enabling more efficient selection and
achieving strong performance with fewer evaluations. We also introduce a method
for estimating overall ranking uncertainty. Finally, we demonstrate that
combining absolute and comparative scoring improves performance. Experiments
show that the specific expert model has a limited impact on final rankings but
our proposed uncertainty estimates, especially the probability of reordering,
significantly improve the efficiency of systems reducing the number of needed
comparisons by ~50%. Furthermore, ranking-level uncertainty metrics can be used
to identify low-performing predictions, where the nature of the probabilistic
model has a notable impact on the quality of the overall uncertainty.",http://arxiv.org/abs/2505.15240v1
Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers,"The empirical emergence of neural collapse -- a surprising symmetry in the
feature representations of the training data in the penultimate layer of deep
neural networks -- has spurred a line of theoretical research aimed at its
understanding. However, existing work focuses on data-agnostic models or, when
data structure is taken into account, it remains limited to multi-layer
perceptrons. Our paper fills both these gaps by analyzing modern architectures
in a data-aware regime: we prove that global optima of deep regularized
transformers and residual networks (ResNets) with LayerNorm trained with cross
entropy or mean squared error loss are approximately collapsed, and the
approximation gets tighter as the depth grows. More generally, we formally
reduce any end-to-end large-depth ResNet or transformer training into an
equivalent unconstrained features model, thus justifying its wide use in the
literature even beyond data-agnostic settings. Our theoretical results are
supported by experiments on computer vision and language datasets showing that,
as the depth grows, neural collapse indeed becomes more prominent.",http://arxiv.org/abs/2505.15239v1
Finding separatrices of dynamical flows with Deep Koopman Eigenfunctions,"Many natural systems, including neural circuits involved in decision making,
can be modeled as high-dimensional dynamical systems with multiple stable
states. While existing analytical tools primarily describe behavior near stable
equilibria, characterizing separatrices -- the manifolds that delineate
boundaries between different basins of attraction -- remains challenging,
particularly in high-dimensional settings. Here, we introduce a numerical
framework leveraging Koopman Theory combined with Deep Neural Networks to
effectively characterize separatrices. Specifically, we approximate Koopman
Eigenfunctions (KEFs) associated with real positive eigenvalues, which vanish
precisely at the separatrices. Utilizing these scalar KEFs, optimization
methods efficiently locate separatrices even in complex systems. We demonstrate
our approach on synthetic benchmarks, ecological network models, and recurrent
neural networks trained on neuroscience-inspired tasks. Moreover, we illustrate
the practical utility of our method by designing optimal perturbations that can
shift systems across separatrices, enabling predictions relevant to optogenetic
stimulation experiments in neuroscience.",http://arxiv.org/abs/2505.15231v1
Degree-Optimized Cumulative Polynomial Kolmogorov-Arnold Networks,"We introduce cumulative polynomial Kolmogorov-Arnold networks (CP-KAN), a
neural architecture combining Chebyshev polynomial basis functions and
quadratic unconstrained binary optimization (QUBO). Our primary contribution
involves reformulating the degree selection problem as a QUBO task, reducing
the complexity from $O(D^N)$ to a single optimization step per layer. This
approach enables efficient degree selection across neurons while maintaining
computational tractability. The architecture performs well in regression tasks
with limited data, showing good robustness to input scales and natural
regularization properties from its polynomial basis. Additionally, theoretical
analysis establishes connections between CP-KAN's performance and properties of
financial time series. Our empirical validation across multiple domains
demonstrates competitive performance compared to several traditional
architectures tested, especially in scenarios where data efficiency and
numerical stability are important. Our implementation, including strategies for
managing computational overhead in larger networks is available in
Ref.~\citep{cpkan_implementation}.",http://arxiv.org/abs/2505.15228v1
Estimation methods of Matrix-valued AR model,"This article proposes novel estimation methods for the Matrix Autoregressive
(MAR) model, specifically adaptations of the Yule-Walker equations and Burg's
method, addressing limitations in existing techniques. The MAR model, by
maintaining a matrix structure and requiring significantly fewer parameters
than vector autoregressive (VAR) models, offers a parsimonious, yet effective,
alternative for high-dimensional time series. Empirical results demonstrate
that MAR models estimated via the proposed methods achieve a comparable fit to
VAR models across metrics such as MAE and RMSE. These findings underscore the
utility of Yule-Walker and Burg-type estimators in constructing efficient and
interpretable models for complex temporal data.",http://arxiv.org/abs/2505.15220v1
Versatile Reservoir Computing for Heterogeneous Complex Networks,"A new machine learning scheme, termed versatile reservoir computing, is
proposed for sustaining the dynamics of heterogeneous complex networks. We show
that a single, small-scale reservoir computer trained on time series from a
subset of elements is able to replicate the dynamics of any element in a
large-scale complex network, though the elements are of different intrinsic
parameters and connectivities. Furthermore, by substituting failed elements
with the trained machine, we demonstrate that the collective dynamics of the
network can be preserved accurately over a finite time horizon. The capability
and effectiveness of the proposed scheme are validated on three representative
network models: a homogeneous complex network of non-identical phase
oscillators, a heterogeneous complex network of non-identical phase
oscillators, and a heterogeneous complex network of non-identical chaotic
oscillators.",http://arxiv.org/abs/2505.15219v1
Recognition of Unseen Combined Motions via Convex Combination-based EMG Pattern Synthesis for Myoelectric Control,"Electromyogram (EMG) signals recorded from the skin surface enable intuitive
control of assistive devices such as prosthetic limbs. However, in EMG-based
motion recognition, collecting comprehensive training data for all target
motions remains challenging, particularly for complex combined motions. This
paper proposes a method to efficiently recognize combined motions using
synthetic EMG data generated through convex combinations of basic motion
patterns. Instead of measuring all possible combined motions, the proposed
method utilizes measured basic motion data along with synthetically combined
motion data for training. This approach expands the range of recognizable
combined motions while minimizing the required training data collection. We
evaluated the effectiveness of the proposed method through an upper limb motion
classification experiment with eight subjects. The experimental results
demonstrated that the proposed method improved the classification accuracy for
unseen combined motions by approximately 17%.",http://arxiv.org/abs/2505.15218v1
BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems,"AI agents have the potential to significantly alter the cybersecurity
landscape. To help us understand this change, we introduce the first framework
to capture offensive and defensive cyber-capabilities in evolving real-world
systems. Instantiating this framework with BountyBench, we set up 25 systems
with complex, real-world codebases. To capture the vulnerability lifecycle, we
define three task types: Detect (detecting a new vulnerability), Exploit
(exploiting a specific vulnerability), and Patch (patching a specific
vulnerability). For Detect, we construct a new success indicator, which is
general across vulnerability types and provides localized evaluation. We
manually set up the environment for each system, including installing packages,
setting up server(s), and hydrating database(s). We add 40 bug bounties, which
are vulnerabilities with monetary awards from \$10 to \$30,485, and cover 9 of
the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy
based on information to guide detection, interpolating from identifying a zero
day to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,
OpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and
Claude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing
agents are Claude Code (5% on Detect, mapping to \$1,350), Custom Agent with
Claude 3.7 Sonnet Thinking (5% on Detect, mapping to \$1,025; 67.5% on
Exploit), and OpenAI Codex CLI (5% on Detect, mapping to \$2,400; 90% on Patch,
mapping to \$14,422). OpenAI Codex CLI and Claude Code are more capable at
defense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit
scores of 32.5% and 57.5% respectively; in contrast, the custom agents are
relatively balanced between offense and defense, achieving Exploit scores of
40-67.5% and Patch scores of 45-60%.",http://arxiv.org/abs/2505.15216v1
Clustering and Pruning in Causal Data Fusion,"Data fusion, the process of combining observational and experimental data,
can enable the identification of causal effects that would otherwise remain
non-identifiable. Although identification algorithms have been developed for
specific scenarios, do-calculus remains the only general-purpose tool for
causal data fusion, particularly when variables are present in some data
sources but not others. However, approaches based on do-calculus may encounter
computational challenges as the number of variables increases and the causal
graph grows in complexity. Consequently, there exists a need to reduce the size
of such models while preserving the essential features. For this purpose, we
propose pruning (removing unnecessary variables) and clustering (combining
variables) as preprocessing operations for causal data fusion. We generalize
earlier results on a single data source and derive conditions for applying
pruning and clustering in the case of multiple data sources. We give sufficient
conditions for inferring the identifiability or non-identifiability of a causal
effect in a larger graph based on a smaller graph and show how to obtain the
corresponding identifying functional for identifiable causal effects. Examples
from epidemiology and social science demonstrate the use of the results.",http://arxiv.org/abs/2505.15215v1
KernelOracle: Predicting the Linux Scheduler's Next Move with Deep Learning,"Efficient task scheduling is paramount in the Linux kernel, where the
Completely Fair Scheduler (CFS) meticulously manages CPU resources to balance
high utilization with interactive responsiveness. This research pioneers the
use of deep learning techniques to predict the sequence of tasks selected by
CFS, aiming to evaluate the feasibility of a more generalized and potentially
more adaptive task scheduler for diverse workloads. Our core contributions are
twofold: first, the systematic generation and curation of a novel scheduling
dataset from a running Linux kernel, capturing real-world CFS behavior; and
second, the development, training, and evaluation of a Long Short-Term Memory
(LSTM) network designed to accurately forecast the next task to be scheduled.
This paper further discusses the practical pathways and implications of
integrating such a predictive model into the kernel's scheduling framework. The
findings and methodologies presented herein open avenues for data-driven
advancements in kernel scheduling, with the full source code provided for
reproducibility and further exploration.",http://arxiv.org/abs/2505.15213v1
Group Distributionally Robust Optimization with Flexible Sample Queries,"Group distributionally robust optimization (GDRO) aims to develop models that
perform well across $m$ distributions simultaneously. Existing GDRO algorithms
can only process a fixed number of samples per iteration, either 1 or $m$, and
therefore can not support scenarios where the sample size varies dynamically.
To address this limitation, we investigate GDRO with flexible sample queries
and cast it as a two-player game: one player solves an online convex
optimization problem, while the other tackles a prediction with limited advice
(PLA) problem. Within such a game, we propose a novel PLA algorithm,
constructing appropriate loss estimators for cases where the sample size is
either 1 or not, and updating the decision using follow-the-regularized-leader.
Then, we establish the first high-probability regret bound for non-oblivious
PLA. Building upon the above approach, we develop a GDRO algorithm that allows
an arbitrary and varying sample size per round, achieving a high-probability
optimization error bound of $O\left(\frac{1}{t}\sqrt{\sum_{j=1}^t
\frac{m}{r_j}\log m}\right)$, where $r_t$ denotes the sample size at round $t$.
This result demonstrates that the optimization error decreases as the number of
samples increases and implies a consistent sample complexity of $O(m\log
(m)/\epsilon^2)$ for any fixed sample size $r\in[m]$, aligning with existing
bounds for cases of $r=1$ or $m$. We validate our approach on synthetic binary
and real-world multi-class datasets.",http://arxiv.org/abs/2505.15212v1
EEG-Based Inter-Patient Epileptic Seizure Detection Combining Domain Adversarial Training with CNN-BiLSTM Network,"Automated epileptic seizure detection from electroencephalogram (EEG) remains
challenging due to significant individual differences in EEG patterns across
patients. While existing studies achieve high accuracy with patient-specific
approaches, they face difficulties in generalizing to new patients. To address
this, we propose a detection framework combining domain adversarial training
with a convolutional neural network (CNN) and a bidirectional long short-term
memory (BiLSTM). First, the CNN extracts local patient-invariant features
through domain adversarial training, which optimizes seizure detection accuracy
while minimizing patient-specific characteristics. Then, the BiLSTM captures
temporal dependencies in the extracted features to model seizure evolution
patterns. Evaluation using EEG recordings from 20 patients with focal epilepsy
demonstrated superior performance over non-adversarial methods, achieving high
detection accuracy across different patients. The integration of adversarial
training with temporal modeling enables robust cross-patient seizure detection.",http://arxiv.org/abs/2505.15203v1
Reconstruction of Graph Signals on Complex Manifolds with Kernel Methods,"Graph signals are widely used to describe vertex attributes or features in
graph-structured data, with applications spanning the internet, social media,
transportation, sensor networks, and biomedicine. Graph signal processing (GSP)
has emerged to facilitate the analysis, processing, and sampling of such
signals. While kernel methods have been extensively studied for estimating
graph signals from samples provided on a subset of vertices, their application
to complex-valued graph signals remains largely unexplored. This paper
introduces a novel framework for reconstructing graph signals using kernel
methods on complex manifolds. By embedding graph vertices into a
higher-dimensional complex ambient space that approximates a lower-dimensional
manifold, the framework extends the reproducing kernel Hilbert space to complex
manifolds. It leverages Hermitian metrics and geometric measures to
characterize kernels and graph signals. Additionally, several traditional
kernels and graph topology-driven kernels are proposed for reconstructing
complex graph signals. Finally, experimental results on synthetic and
real-world datasets demonstrate the effectiveness of this framework in
accurately reconstructing complex graph signals, outperforming conventional
kernel-based approaches. This work lays a foundational basis for integrating
complex geometry and kernel methods in GSP.",http://arxiv.org/abs/2505.15202v1
Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems,"Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.",http://arxiv.org/abs/2505.15201v1
Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing,"Retraining a model using its own predictions together with the original,
potentially noisy labels is a well-known strategy for improving the model
performance. While prior works have demonstrated the benefits of specific
heuristic retraining schemes, the question of how to optimally combine the
model's predictions and the provided labels remains largely open. This paper
addresses this fundamental question for binary classification tasks. We develop
a principled framework based on approximate message passing (AMP) to analyze
iterative retraining procedures for two ground truth settings: Gaussian mixture
model (GMM) and generalized linear model (GLM). Our main contribution is the
derivation of the Bayes optimal aggregator function to combine the current
model's predictions and the given labels, which when used to retrain the same
model, minimizes its prediction error. We also quantify the performance of this
optimal retraining strategy over multiple rounds. We complement our theoretical
results by proposing a practically usable version of the theoretically-optimal
aggregator function for linear probing with the cross-entropy loss, and
demonstrate its superiority over baseline methods in the high label noise
regime.",http://arxiv.org/abs/2505.15195v1
ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection,"Recent advances in LLM agents have largely built on reasoning backbones like
ReAct, which interleave thought and action in complex environments. However,
ReAct often produces ungrounded or incoherent reasoning steps, leading to
misalignment between the agent's actual state and goal. Our analysis finds that
this stems from ReAct's inability to maintain consistent internal beliefs and
goal alignment, causing compounding errors and hallucinations. To address this,
we introduce ReflAct, a novel backbone that shifts reasoning from merely
planning next actions to continuously reflecting on the agent's state relative
to its goal. By explicitly grounding decisions in states and enforcing ongoing
goal alignment, ReflAct dramatically improves strategic reliability. This
design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%
on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even
outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),
showing that strengthening the core reasoning backbone is key to reliable agent
performance.",http://arxiv.org/abs/2505.15182v1
NeuBM: Mitigating Model Bias in Graph Neural Networks through Neutral Input Calibration,"Graph Neural Networks (GNNs) have shown remarkable performance across various
domains, yet they often struggle with model bias, particularly in the presence
of class imbalance. This bias can lead to suboptimal performance and unfair
predictions, especially for underrepresented classes. We introduce NeuBM
(Neutral Bias Mitigation), a novel approach to mitigate model bias in GNNs
through neutral input calibration. NeuBM leverages a dynamically updated
neutral graph to estimate and correct the inherent biases of the model. By
subtracting the logits obtained from the neutral graph from those of the input
graph, NeuBM effectively recalibrates the model's predictions, reducing bias
across different classes. Our method integrates seamlessly into existing GNN
architectures and training procedures, requiring minimal computational
overhead. Extensive experiments on multiple benchmark datasets demonstrate that
NeuBM significantly improves the balanced accuracy and recall of minority
classes, while maintaining strong overall performance. The effectiveness of
NeuBM is particularly pronounced in scenarios with severe class imbalance and
limited labeled data, where traditional methods often struggle. We provide
theoretical insights into how NeuBM achieves bias mitigation, relating it to
the concept of representation balancing. Our analysis reveals that NeuBM not
only adjusts the final predictions but also influences the learning of balanced
feature representations throughout the network.",http://arxiv.org/abs/2505.15180v1
A Unified Gradient-based Framework for Task-agnostic Continual Learning-Unlearning,"Recent advancements in deep models have highlighted the need for intelligent
systems that combine continual learning (CL) for knowledge acquisition with
machine unlearning (MU) for data removal, forming the Continual
Learning-Unlearning (CLU) paradigm. While existing work treats CL and MU as
separate processes, we reveal their intrinsic connection through a unified
optimization framework based on Kullback-Leibler divergence minimization. This
framework decomposes gradient updates for approximate CLU into four components:
learning new knowledge, unlearning targeted data, preserving existing
knowledge, and modulation via weight saliency. A critical challenge lies in
balancing knowledge update and retention during sequential learning-unlearning
cycles. To resolve this stability-plasticity dilemma, we introduce a
remain-preserved manifold constraint to induce a remaining Hessian compensation
for CLU iterations. A fast-slow weight adaptation mechanism is designed to
efficiently approximate the second-order optimization direction, combined with
adaptive weighting coefficients and a balanced weight saliency mask, proposing
a unified implementation framework for gradient-based CLU. Furthermore, we
pioneer task-agnostic CLU scenarios that support fine-grained unlearning at the
cross-task category and random sample levels beyond the traditional task-aware
setups. Experiments demonstrate that the proposed UG-CLU framework effectively
coordinates incremental learning, precise unlearning, and knowledge stability
across multiple datasets and model architectures, providing a theoretical
foundation and methodological support for dynamic, compliant intelligent
systems.",http://arxiv.org/abs/2505.15178v1
SpectralGap: Graph-Level Out-of-Distribution Detection via Laplacian Eigenvalue Gaps,"The task of graph-level out-of-distribution (OOD) detection is crucial for
deploying graph neural networks in real-world settings. In this paper, we
observe a significant difference in the relationship between the largest and
second-largest eigenvalues of the Laplacian matrix for in-distribution (ID) and
OOD graph samples: \textit{OOD samples often exhibit anomalous spectral gaps
(the difference between the largest and second-largest eigenvalues)}. This
observation motivates us to propose SpecGap, an effective post-hoc approach for
OOD detection on graphs. SpecGap adjusts features by subtracting the component
associated with the second-largest eigenvalue, scaled by the spectral gap, from
the high-level features (i.e., $\mathbf{X}-\left(\lambda_n-\lambda_{n-1}\right)
\mathbf{u}_{n-1} \mathbf{v}_{n-1}^T$). SpecGap achieves state-of-the-art
performance across multiple benchmark datasets. We present extensive ablation
studies and comprehensive theoretical analyses to support our empirical
results. As a parameter-free post-hoc method, SpecGap can be easily integrated
into existing graph neural network models without requiring any additional
training or model modification.",http://arxiv.org/abs/2505.15177v1
A Linear Approach to Data Poisoning,"We investigate the theoretical foundations of data poisoning attacks in
machine learning models. Our analysis reveals that the Hessian with respect to
the input serves as a diagnostic tool for detecting poisoning, exhibiting
spectral signatures that characterize compromised datasets. We use random
matrix theory (RMT) to develop a theory for the impact of poisoning proportion
and regularisation on attack efficacy in linear regression. Through QR stepwise
regression, we study the spectral signatures of the Hessian in multi-output
regression. We perform experiments on deep networks to show experimentally that
this theory extends to modern convolutional and transformer networks under the
cross-entropy loss. Based on these insights we develop preliminary algorithms
to determine if a network has been poisoned and remedies which do not require
further training.",http://arxiv.org/abs/2505.15175v1
Enhancing Certified Robustness via Block Reflector Orthogonal Layers and Logit Annealing Loss,"Lipschitz neural networks are well-known for providing certified robustness
in deep learning. In this paper, we present a novel, efficient Block Reflector
Orthogonal (BRO) layer that enhances the capability of orthogonal layers on
constructing more expressive Lipschitz neural architectures. In addition, by
theoretically analyzing the nature of Lipschitz neural networks, we introduce a
new loss function that employs an annealing mechanism to increase margin for
most data points. This enables Lipschitz models to provide better certified
robustness. By employing our BRO layer and loss function, we design BRONet - a
simple yet effective Lipschitz neural network that achieves state-of-the-art
certified robustness. Extensive experiments and empirical analysis on
CIFAR-10/100, Tiny-ImageNet, and ImageNet validate that our method outperforms
existing baselines. The implementation is available at
\href{https://github.com/ntuaislab/BRONet}{https://github.com/ntuaislab/BRONet}.",http://arxiv.org/abs/2505.15174v1
Cascaded Diffusion Models for Neural Motion Planning,"Robots in the real world need to perceive and move to goals in complex
environments without collisions. Avoiding collisions is especially difficult
when relying on sensor perception and when goals are among clutter. Diffusion
policies and other generative models have shown strong performance in solving
local planning problems, but often struggle at avoiding all of the subtle
constraint violations that characterize truly challenging global motion
planning problems. In this work, we propose an approach for learning global
motion planning using diffusion policies, allowing the robot to generate full
trajectories through complex scenes and reasoning about multiple obstacles
along the path. Our approach uses cascaded hierarchical models which unify
global prediction and local refinement together with online plan repair to
ensure the trajectories are collision free. Our method outperforms (by ~5%) a
wide variety of baselines on challenging tasks in multiple domains including
navigation and manipulation.",http://arxiv.org/abs/2505.15157v1
R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization,"Financial markets pose fundamental challenges for asset return prediction due
to their high dimensionality, non-stationarity, and persistent volatility.
Despite advances in large language models and multi-agent systems, current
quantitative research pipelines suffer from limited automation, weak
interpretability, and fragmented coordination across key components such as
factor mining and model innovation. In this paper, we propose R&D-Agent for
Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent
framework designed to automate the full-stack research and development of
quantitative strategies via coordinated factor-model co-optimization.
RD-Agent(Q) decomposes the quant process into two iterative stages: a Research
stage that dynamically sets goal-aligned prompts, formulates hypotheses based
on domain priors, and maps them to concrete tasks, and a Development stage that
employs a code-generation agent, Co-STEER, to implement task-specific code,
which is then executed in real-market backtests. The two stages are connected
through a feedback stage that thoroughly evaluates experimental outcomes and
informs subsequent iterations, with a multi-armed bandit scheduler for adaptive
direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher
annualized returns than classical factor libraries using 70% fewer factors, and
outperforms state-of-the-art deep time-series models on real markets. Its joint
factor-model optimization delivers a strong balance between predictive accuracy
and strategy robustness. Our code is available at:
https://github.com/microsoft/RD-Agent.",http://arxiv.org/abs/2505.15155v1
Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation,"Feature Transformation (FT) crafts new features from original ones via
mathematical operations to enhance dataset expressiveness for downstream
models. However, existing FT methods exhibit critical limitations: discrete
search struggles with enormous combinatorial spaces, impeding practical use;
and continuous search, being highly sensitive to initialization and step sizes,
often becomes trapped in local optima, restricting global exploration. To
overcome these limitations, DIFFT redefines FT as a reward-guided generative
task. It first learns a compact and expressive latent space for feature sets
using a Variational Auto-Encoder (VAE). A Latent Diffusion Model (LDM) then
navigates this space to generate high-quality feature embeddings, its
trajectory guided by a performance evaluator towards task-specific optima. This
synthesis of global distribution learning (from LDM) and targeted optimization
(reward guidance) produces potent embeddings, which a novel semi-autoregressive
decoder efficiently converts into structured, discrete features, preserving
intra-feature dependencies while allowing parallel inter-feature generation.
Extensive experiments on 14 benchmark datasets show DIFFT consistently
outperforms state-of-the-art baselines in predictive accuracy and robustness,
with significantly lower training and inference times.",http://arxiv.org/abs/2505.15152v1
Time Tracker: Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines,"In the past few years, time series foundation models have achieved superior
predicting accuracy. However, real-world time series often exhibit significant
diversity in their temporal patterns across different time spans and domains,
making it challenging for a single model architecture to fit all complex
scenarios. In addition, time series data may have multiple variables exhibiting
complex correlations between each other. Recent mainstream works have focused
on modeling times series in a channel-independent manner in both pretraining
and finetuning stages, overlooking the valuable inter-series dependencies. To
this end, we propose \textbf{Time Tracker} for better predictions on
multivariate time series data. Firstly, we leverage sparse mixture of experts
(MoE) within Transformers to handle the modeling of diverse time series
patterns, thereby alleviating the learning difficulties of a single model while
improving its generalization. Besides, we propose Any-variate Attention,
enabling a unified model structure to seamlessly handle both univariate and
multivariate time series, thereby supporting channel-independent modeling
during pretraining and channel-mixed modeling for finetuning. Furthermore, we
design a graph learning module that constructs relations among sequences from
frequency-domain features, providing more precise guidance to capture
inter-series dependencies in channel-mixed modeling. Based on these
advancements, Time Tracker achieves state-of-the-art performance in predicting
accuracy, model generalization and adaptability.",http://arxiv.org/abs/2505.15151v1
Filtering Learning Histories Enhances In-Context Reinforcement Learning,"Transformer models (TMs) have exhibited remarkable in-context reinforcement
learning (ICRL) capabilities, allowing them to generalize to and improve in
previously unseen environments without re-training or fine-tuning. This is
typically accomplished by imitating the complete learning histories of a source
RL algorithm over a substantial amount of pretraining environments, which,
however, may transfer suboptimal behaviors inherited from the source
algorithm/dataset. Therefore, in this work, we address the issue of inheriting
suboptimality from the perspective of dataset preprocessing. Motivated by the
success of the weighted empirical risk minimization, we propose a simple yet
effective approach, learning history filtering (LHF), to enhance ICRL by
reweighting and filtering the learning histories based on their improvement and
stability characteristics. To the best of our knowledge, LHF is the first
approach to avoid source suboptimality by dataset preprocessing, and can be
combined with the current state-of-the-art (SOTA) ICRL algorithms. We
substantiate the effectiveness of LHF through a series of experiments conducted
on the well-known ICRL benchmarks, encompassing both discrete environments and
continuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD,
DPT, DICP) as the backbones. LHF exhibits robust performance across a variety
of suboptimal scenarios, as well as under varying hyperparameters and sampling
strategies. Notably, the superior performance of LHF becomes more pronounced in
the presence of noisy data, indicating the significance of filtering learning
histories.",http://arxiv.org/abs/2505.15143v1
BanditSpec: Adaptive Speculative Decoding via Bandit Algorithms,"Speculative decoding has emerged as a popular method to accelerate the
inference of Large Language Models (LLMs) while retaining their superior text
generation performance. Previous methods either adopt a fixed speculative
decoding configuration regardless of the prefix tokens, or train draft models
in an offline or online manner to align them with the context. This paper
proposes a training-free online learning framework to adaptively choose the
configuration of the hyperparameters for speculative decoding as text is being
generated. We first formulate this hyperparameter selection problem as a
Multi-Armed Bandit problem and provide a general speculative decoding framework
BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,
UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,
the stopping time regret. We upper bound this regret under both stochastic and
adversarial reward settings. By deriving an information-theoretic impossibility
result, it is shown that the regret performance of UCBSpec is optimal up to
universal constants. Finally, extensive empirical experiments with LLaMA3 and
Qwen2 demonstrate that our algorithms are effective compared to existing
methods, and the throughput is close to the oracle best hyperparameter in
simulated real-life LLM serving scenarios with diverse input prompts.",http://arxiv.org/abs/2505.15141v1
EC-LDA : Label Distribution Inference Attack against Federated Graph Learning with Embedding Compression,"Graph Neural Networks (GNNs) have been widely used for graph analysis.
Federated Graph Learning (FGL) is an emerging learning framework to
collaboratively train graph data from various clients. However, since clients
are required to upload model parameters to the server in each round, this
provides the server with an opportunity to infer each client's data privacy. In
this paper, we focus on label distribution attacks(LDAs) that aim to infer the
label distributions of the clients' local data. We take the first step to
attack client's label distributions in FGL. Firstly, we observe that the
effectiveness of LDA is closely related to the variance of node embeddings in
GNNs. Next, we analyze the relation between them and we propose a new attack
named EC-LDA, which significantly improves the attack effectiveness by
compressing node embeddings. Thirdly, extensive experiments on node
classification and link prediction tasks across six widely used graph datasets
show that EC-LDA outperforms the SOTA LDAs. For example, EC-LDA attains optimal
values under both Cos-sim and JS-div evaluation metrics in the CoraFull and
LastFM datasets. Finally, we explore the robustness of EC-LDA under
differential privacy protection.",http://arxiv.org/abs/2505.15140v1
Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm,"This paper investigates infinite-horizon average reward Constrained Markov
Decision Processes (CMDPs) with general parametrization. We propose a
Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints
while ensuring a high convergence rate. In particular, our algorithm achieves
global convergence and constraint violation rates of
$\tilde{\mathcal{O}}(1/\sqrt{T})$ over a horizon of length $T$ when the mixing
time, $\tau_{\mathrm{mix}}$, is known to the learner. In absence of knowledge
of $\tau_{\mathrm{mix}}$, the achievable rates change to
$\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ provided that $T \geq
\tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$. Our results
match the theoretical lower bound for Markov Decision Processes and establish a
new benchmark in the theoretical exploration of average reward CMDPs.",http://arxiv.org/abs/2505.15138v1
The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning,"Entropy minimization (EM) trains the model to concentrate even more
probability mass on its most confident outputs. We show that this simple
objective alone, without any labeled data, can substantially improve large
language models' (LLMs) performance on challenging math, physics, and coding
tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy
similarly to instruction finetuning, but on unlabeled outputs drawn from the
model; (2) EM-RL: reinforcement learning with negative entropy as the only
reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce
entropy without any training data or parameter updates. On Qwen-7B, EM-RL,
without any labeled data, achieves comparable or better performance than strong
RL baselines such as GRPO and RLOO that are trained on 60K labeled examples.
Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of
proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the
challenging SciCode benchmark, while being 3x more efficient than
self-consistency and sequential refinement. Our findings reveal that many
pretrained LLMs possess previously underappreciated reasoning capabilities that
can be effectively elicited through entropy minimization alone, without any
labeled data or even any parameter updates.",http://arxiv.org/abs/2505.15134v1
DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer,"Recent advances in knowledge distillation have emphasized the importance of
decoupling different knowledge components. While existing methods utilize
momentum mechanisms to separate task-oriented and distillation gradients, they
overlook the inherent conflict between target-class and non-target-class
knowledge flows. Furthermore, low-confidence dark knowledge in non-target
classes introduces noisy signals that hinder effective knowledge transfer. To
address these limitations, we propose DeepKD, a novel training framework that
integrates dual-level decoupling with adaptive denoising. First, through
theoretical analysis of gradient signal-to-noise ratio (GSNR) characteristics
in task-oriented and non-task-oriented knowledge distillation, we design
independent momentum updaters for each component to prevent mutual
interference. We observe that the optimal momentum coefficients for
task-oriented gradient (TOG), target-class gradient (TCG), and non-target-class
gradient (NCG) should be positively related to their GSNR. Second, we introduce
a dynamic top-k mask (DTM) mechanism that gradually increases K from a small
initial value to incorporate more non-target classes as training progresses,
following curriculum learning principles. The DTM jointly filters
low-confidence logits from both teacher and student models, effectively
purifying dark knowledge during early training. Extensive experiments on
CIFAR-100, ImageNet, and MS-COCO demonstrate DeepKD's effectiveness. Our code
is available at https://github.com/haiduo/DeepKD.",http://arxiv.org/abs/2505.15133v1
Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models,"Vision-Language Models (VLMs) such as CLIP have shown remarkable performance
in cross-modal tasks through large-scale contrastive pre-training. To adapt
these large transformer-based models efficiently for downstream tasks,
Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA have emerged as
scalable alternatives to full fine-tuning, especially in few-shot scenarios.
However, like traditional deep neural networks, VLMs are highly vulnerable to
adversarial attacks, where imperceptible perturbations can significantly
degrade model performance. Adversarial training remains the most effective
strategy for improving model robustness in PEFT. In this work, we propose
AdvCLIP-LoRA, the first algorithm designed to enhance the adversarial
robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method
formulates adversarial fine-tuning as a minimax optimization problem and
provides theoretical guarantees for convergence under smoothness and
nonconvex-strong-concavity assumptions. Empirical results across eight datasets
using ViT-B/16 and ViT-B/32 models show that AdvCLIP-LoRA significantly
improves robustness against common adversarial attacks (e.g., FGSM, PGD),
without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA
as a practical and theoretically grounded approach for robust adaptation of
VLMs in resource-constrained settings.",http://arxiv.org/abs/2505.15130v1
Lung Nodule-SSM: Self-Supervised Lung Nodule Detection and Classification in Thoracic CT Images,"Lung cancer remains among the deadliest types of cancer in recent decades,
and early lung nodule detection is crucial for improving patient outcomes. The
limited availability of annotated medical imaging data remains a bottleneck in
developing accurate computer-aided diagnosis (CAD) systems. Self-supervised
learning can help leverage large amounts of unlabeled data to develop more
robust CAD systems. With the recent advent of transformer-based architecture
and their ability to generalize to unseen tasks, there has been an effort
within the healthcare community to adapt them to various medical downstream
tasks. Thus, we propose a novel ""LungNodule-SSM"" method, which utilizes
selfsupervised learning with DINOv2 as a backbone to enhance lung nodule
detection and classification without annotated data. Our methodology has two
stages: firstly, the DINOv2 model is pre-trained on unlabeled CT scans to learn
robust feature representations, then secondly, these features are fine-tuned
using transformer-based architectures for lesionlevel detection and accurate
lung nodule diagnosis. The proposed method has been evaluated on the
challenging LUNA 16 dataset, consisting of 888 CT scans, and compared with SOTA
methods. Our experimental results show the superiority of our proposed method
with an accuracy of 98.37%, explaining its effectiveness in lung nodule
detection. The source code, datasets, and pre-processed data can be accessed
using the
link:https://github.com/EMeRALDsNRPU/Lung-Nodule-SSM-Self-Supervised-Lung-Nodule-Detection-and-Classification/tree/main",http://arxiv.org/abs/2505.15120v1
Graph Foundation Models: A Comprehensive Survey,"Graph-structured data pervades domains such as social networks, biological
systems, knowledge graphs, and recommender systems. While foundation models
have transformed natural language processing, vision, and multimodal learning
through large-scale pretraining and generalization, extending these
capabilities to graphs -- characterized by non-Euclidean structures and complex
relational semantics -- poses unique challenges and opens new opportunities. To
this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose
intelligence to structured data, enabling broad transfer across graph-centric
tasks and domains. This survey provides a comprehensive overview of GFMs,
unifying diverse efforts under a modular framework comprising three key
components: backbone architectures, pretraining strategies, and adaptation
mechanisms. We categorize GFMs by their generalization scope -- universal,
task-specific, and domain-specific -- and review representative methods, key
innovations, and theoretical insights within each category. Beyond methodology,
we examine theoretical foundations including transferability and emergent
capabilities, and highlight key challenges such as structural alignment,
heterogeneity, scalability, and evaluation. Positioned at the intersection of
graph learning and general-purpose AI, GFMs are poised to become foundational
infrastructure for open-ended reasoning over structured data. This survey
consolidates current progress and outlines future directions to guide research
in this rapidly evolving field. Resources are available at
https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.",http://arxiv.org/abs/2505.15116v1
Khan-GCL: Kolmogorov-Arnold Network Based Graph Contrastive Learning with Hard Negatives,"Graph contrastive learning (GCL) has demonstrated great promise for learning
generalizable graph representations from unlabeled data. However, conventional
GCL approaches face two critical limitations: (1) the restricted expressive
capacity of multilayer perceptron (MLP) based encoders, and (2) suboptimal
negative samples that either from random augmentations-failing to provide
effective 'hard negatives'-or generated hard negatives without addressing the
semantic distinctions crucial for discriminating graph data. To this end, we
propose Khan-GCL, a novel framework that integrates the Kolmogorov-Arnold
Network (KAN) into the GCL encoder architecture, substantially enhancing its
representational capacity. Furthermore, we exploit the rich information
embedded within KAN coefficient parameters to develop two novel critical
feature identification techniques that enable the generation of semantically
meaningful hard negative samples for each graph representation. These
strategically constructed hard negatives guide the encoder to learn more
discriminative features by emphasizing critical semantic differences between
graphs. Extensive experiments demonstrate that our approach achieves
state-of-the-art performance compared to existing GCL methods across a variety
of datasets and tasks.",http://arxiv.org/abs/2505.15103v1
Cost-aware LLM-based Online Dataset Annotation,"Recent advances in large language models (LLMs) have enabled automated
dataset labeling with minimal human supervision. While majority voting across
multiple LLMs can improve label reliability by mitigating individual model
biases, it incurs high computational costs due to repeated querying. In this
work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo),
for efficient and accurate LLM-based dataset annotation. CaMVo adaptively
selects a subset of LLMs for each data instance based on contextual embeddings,
balancing confidence and cost without requiring pre-training or ground-truth
labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator
over confidence scores, CaMVo estimates a lower bound on labeling accuracy for
each LLM and aggregates responses through weighted majority voting. Our
empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates
that CaMVo achieves comparable or superior accuracy to full majority voting
while significantly reducing labeling costs. This establishes CaMVo as a
practical and robust solution for cost-efficient annotation in dynamic labeling
environments.",http://arxiv.org/abs/2505.15101v1
Steering Generative Models with Experimental Data for Protein Fitness Optimization,"Protein fitness optimization involves finding a protein sequence that
maximizes desired quantitative properties in a combinatorially large design
space of possible sequences. Recent developments in steering protein generative
models (e.g diffusion models, language models) offer a promising approach.
However, by and large, past studies have optimized surrogate rewards and/or
utilized large amounts of labeled data for steering, making it unclear how well
existing methods perform and compare to each other in real-world optimization
campaigns where fitness is measured by low-throughput wet-lab assays. In this
study, we explore fitness optimization using small amounts (hundreds) of
labeled sequence-fitness pairs and comprehensively evaluate strategies such as
classifier guidance and posterior sampling for guiding generation from
different discrete diffusion models of protein sequences. We also demonstrate
how guidance can be integrated into adaptive sequence selection akin to
Thompson sampling in Bayesian optimization, showing that plug-and-play guidance
strategies offer advantages compared to alternatives such as reinforcement
learning with protein language models.",http://arxiv.org/abs/2505.15093v1
DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer,"Effective cross-lingual transfer remains a critical challenge in scaling the
benefits of large language models from high-resource to low-resource languages.
Towards this goal, prior studies have explored many approaches to combine task
knowledge from task-specific data in a (high-resource) source language and
language knowledge from unlabeled text in a (low-resource) target language. One
notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual
transfer that learns task-specific and language-specific sparse masks to select
a subset of the pretrained model's parameters that are further fine-tuned.
These sparse fine-tuned vectors (SFTs) are subsequently composed with the
pretrained model to facilitate zero-shot cross-lingual transfer to a task in a
target language, using only task-specific data from a source language. These
sparse masks for SFTs were identified using a simple magnitude-based pruning.
In our work, we introduce DeFT-X, a novel composable SFT approach that denoises
the weight matrices of a pretrained model before magnitude pruning using
singular value decomposition, thus yielding more robust SFTs. We evaluate
DeFT-X on a diverse set of extremely low-resource languages for sentiment
classification (NusaX) and natural language inference (AmericasNLI) and
demonstrate that it performs at par or outperforms SFT and other prominent
cross-lingual transfer baselines.",http://arxiv.org/abs/2505.15090v1
Robust Multi-Modal Forecasting: Integrating Static and Dynamic Features,"Time series forecasting plays a crucial role in various applications,
particularly in healthcare, where accurate predictions of future health
trajectories can significantly impact clinical decision-making. Ensuring
transparency and explainability of the models responsible for these tasks is
essential for their adoption in critical settings. Recent work has explored a
top-down approach to bi-level transparency, focusing on understanding trends
and properties of predicted time series using static features. In this work, we
extend this framework by incorporating exogenous time series features alongside
static features in a structured manner, while maintaining cohesive
interpretation. Our approach leverages the insights of trajectory comprehension
to introduce an encoding mechanism for exogenous time series, where they are
decomposed into meaningful trends and properties, enabling the extraction of
interpretable patterns. Through experiments on several synthetic datasets, we
demonstrate that our approach remains predictive while preserving
interpretability and robustness. This work represents a step towards developing
robust, and generalized time series forecasting models. The code is available
at https://github.com/jeremy-qin/TIMEVIEW",http://arxiv.org/abs/2505.15083v1
SUS backprop: linear backpropagation algorithm for long inputs in transformers,"It is straightforward to design an unbiased gradient estimator that
stochastically cuts the backpropagation flow through any part of a
computational graph. By cutting the parts that have little effect on the
computation, one can potentially save a significant amount of back-propagation
computation in exchange for a minimal increase in the stochastic gradient
variance, in some situations. Such a situation occurs in the attention
mechanism of the transformer architecture. For long sequences, attention
becomes the limiting factor, as its compute requirements increase quadratically
with sequence length $n$. At the same time, most attention weights become very
small, as most attention heads tend to connect a given token with only a small
fraction of other tokens in the sequence. These weights become promising
targets for cutting backpropagation. We propose a simple probabilistic rule
controlled by a single parameter $c$ that cuts backpropagation through most
attention weights, leaving at most $c$ interactions per token per attention
head. This brings a factor of $c/n$ reduction in the compute required for the
attention backpropagation, turning it from quadratic $O(n^2)$ to linear
complexity $O(nc)$. We have empirically verified that, for a typical
transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing
$c \sim 20-30$) results in relative gradient variance increase of only about
$1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable
to efficient sparse matrix implementation, thus being promising for making the
cost of a backward pass negligible relative to the cost of a forward pass when
training a transformer model on long sequences.",http://arxiv.org/abs/2505.15080v1
Data Augmentation and Resolution Enhancement using GANs and Diffusion Models for Tree Segmentation,"Urban forests play a key role in enhancing environmental quality and
supporting biodiversity in cities. Mapping and monitoring these green spaces
are crucial for urban planning and conservation, yet accurately detecting trees
is challenging due to complex landscapes and the variability in image
resolution caused by different satellite sensors or UAV flight altitudes. While
deep learning architectures have shown promise in addressing these challenges,
their effectiveness remains strongly dependent on the availability of large and
manually labeled datasets, which are often expensive and difficult to obtain in
sufficient quantity. In this work, we propose a novel pipeline that integrates
domain adaptation with GANs and Diffusion models to enhance the quality of
low-resolution aerial images. Our proposed pipeline enhances low-resolution
imagery while preserving semantic content, enabling effective tree segmentation
without requiring large volumes of manually annotated data. Leveraging models
such as pix2pix, Real-ESRGAN, Latent Diffusion, and Stable Diffusion, we
generate realistic and structurally consistent synthetic samples that expand
the training dataset and unify scale across domains. This approach not only
improves the robustness of segmentation models across different acquisition
conditions but also provides a scalable and replicable solution for remote
sensing scenarios with scarce annotation resources. Experimental results
demonstrated an improvement of over 50% in IoU for low-resolution images,
highlighting the effectiveness of our method compared to traditional pipelines.",http://arxiv.org/abs/2505.15077v1
"Agentic Feature Augmentation: Unifying Selection and Generation with Teaming, Planning, and Memories","As a widely-used and practical tool, feature engineering transforms raw data
into discriminative features to advance AI model performance. However, existing
methods usually apply feature selection and generation separately, failing to
strive a balance between reducing redundancy and adding meaningful dimensions.
To fill this gap, we propose an agentic feature augmentation concept, where the
unification of feature generation and selection is modeled as agentic teaming
and planning. Specifically, we develop a Multi-Agent System with Long and
Short-Term Memory (MAGS), comprising a selector agent to eliminate redundant
features, a generator agent to produce informative new dimensions, and a router
agent that strategically coordinates their actions. We leverage in-context
learning with short-term memory for immediate feedback refinement and long-term
memory for globally optimal guidance. Additionally, we employ offline Proximal
Policy Optimization (PPO) reinforcement fine-tuning to train the router agent
for effective decision-making to navigate a vast discrete feature space.
Extensive experiments demonstrate that this unified agentic framework
consistently achieves superior task performance by intelligently orchestrating
feature selection and generation.",http://arxiv.org/abs/2505.15076v1
Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs,"The rapid evolution of multimodal large language models (MLLMs) has
significantly enhanced their real-world applications. However, achieving
consistent performance across languages, especially when integrating cultural
knowledge, remains a significant challenge. To better assess this issue, we
introduce two new benchmarks: KnowRecall and VisRecall, which evaluate
cross-lingual consistency in MLLMs. KnowRecall is a visual question answering
benchmark designed to measure factual knowledge consistency in 15 languages,
focusing on cultural and historical questions about global landmarks. VisRecall
assesses visual memory consistency by asking models to describe landmark
appearances in 9 languages without access to images. Experimental results
reveal that state-of-the-art MLLMs, including proprietary ones, still struggle
to achieve cross-lingual consistency. This underscores the need for more robust
approaches that produce truly multilingual and culturally aware models.",http://arxiv.org/abs/2505.15075v1
DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data,"Large Language Models (LLMs) are increasingly aligned with human preferences
through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,
Group Relative Policy Optimization (GRPO) has gained attention for its
simplicity and strong performance, notably eliminating the need for a learned
value function. However, GRPO implicitly assumes a balanced domain distribution
and uniform semantic alignment across groups - assumptions that rarely hold in
real-world datasets. When applied to multi-domain, imbalanced data, GRPO
disproportionately optimizes for dominant domains, neglecting underrepresented
ones and resulting in poor generalization and fairness. We propose
Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled
extension to GRPO that addresses inter-group imbalance with two key
innovations. Domain-aware reward scaling counteracts frequency bias by
reweighting optimization based on domain prevalence. Difficulty-aware reward
scaling leverages prompt-level self-consistency to identify and prioritize
uncertain prompts that offer greater learning value. Together, these strategies
promote more equitable and effective policy learning across domains. Extensive
experiments across multiple LLMs and skewed training distributions show that
DISCO improves generalization, outperforms existing GRPO variants by 5% on
Qwen3 models, and sets new state-of-the-art results on multi-domain alignment
benchmarks.",http://arxiv.org/abs/2505.15074v1
MoTime: A Dataset Suite for Multimodal Time Series Forecasting,"While multimodal data sources are increasingly available from real-world
forecasting, most existing research remains on unimodal time series. In this
work, we present MoTime, a suite of multimodal time series forecasting datasets
that pair temporal signals with external modalities such as text, metadata, and
images. Covering diverse domains, MoTime supports structured evaluation of
modality utility under two scenarios: 1) the common forecasting task, where
varying-length history is available, and 2) cold-start forecasting, where no
historical data is available. Experiments show that external modalities can
improve forecasting performance in both scenarios, with particularly strong
benefits for short series in some datasets, though the impact varies depending
on data characteristics. By making datasets and findings publicly available, we
aim to support more comprehensive and realistic benchmarks in future multimodal
time series forecasting research.",http://arxiv.org/abs/2505.15072v1
ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges,"Recent progress in large language models (LLMs) has enabled substantial
advances in solving mathematical problems. However, existing benchmarks often
fail to reflect the complexity of real-world problems, which demand open-ended,
interdisciplinary reasoning and integration of computational tools. To address
this gap, we introduce ModelingBench, a novel benchmark featuring
real-world-inspired, open-ended problems from math modeling competitions across
diverse domains, ranging from urban traffic optimization to ecosystem resource
planning. These tasks require translating natural language into formal
mathematical formulations, applying appropriate tools, and producing
structured, defensible reports. ModelingBench also supports multiple valid
solutions, capturing the ambiguity and creativity of practical modeling. We
also present ModelingAgent, a multi-agent framework that coordinates tool use,
supports structured workflows, and enables iterative self-refinement to
generate well-grounded, creative solutions. To evaluate outputs, we further
propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as
domain-specialized judges assessing solutions from multiple expert
perspectives. Empirical results show that ModelingAgent substantially
outperforms strong baselines and often produces solutions indistinguishable
from those of human experts. Together, our work provides a comprehensive
framework for evaluating and advancing real-world problem-solving in
open-ended, interdisciplinary modeling challenges.",http://arxiv.org/abs/2505.15068v1
Generalization Through Growth: Hidden Dynamics Controls Depth Dependence,"Recent theory has reduced the depth dependence of generalization bounds from
exponential to polynomial and even depth-independent rates, yet these results
remain tied to specific architectures and Euclidean inputs. We present a
unified framework for arbitrary \blue{pseudo-metric} spaces in which a
depth-\(k\) network is the composition of continuous hidden maps
\(f:\mathcal{X}\to \mathcal{X}\) and an output map \(h:\mathcal{X}\to
\mathbb{R}\). The resulting bound $O(\sqrt{(\alpha + \log \beta(k))/n})$
isolates the sole depth contribution in \(\beta(k)\), the word-ball growth of
the semigroup generated by the hidden layers. By Gromov's theorem polynomial
(resp. exponential) growth corresponds to virtually nilpotent (resp. expanding)
dynamics, revealing a geometric dichotomy behind existing $O(\sqrt{k})$
(sublinear depth) and $\tilde{O}(1)$ (depth-independent) rates. We further
provide covering-number estimates showing that expanding dynamics yield an
exponential parameter saving via compositional expressivity. Our results
decouple specification from implementation, offering architecture-agnostic and
dynamical-systems-aware guarantees applicable to modern deep-learning paradigms
such as test-time inference and diffusion models.",http://arxiv.org/abs/2505.15064v1
Restricted Spectral Gap Decomposition for Simulated Tempering Targeting Mixture Distributions,"Simulated tempering is a widely used strategy for sampling from multimodal
distributions. In this paper, we consider simulated tempering combined with an
arbitrary local Markov chain Monte Carlo sampler and present a new
decomposition theorem that provides a lower bound on the restricted spectral
gap of the algorithm for sampling from mixture distributions. By working with
the restricted spectral gap, the applicability of our results is extended to
broader settings such as when the usual spectral gap is difficult to bound or
becomes degenerate. We demonstrate the application of our theoretical results
by analyzing simulated tempering combined with random walk Metropolis--Hastings
for sampling from mixtures of Gaussian distributions. We show that in
fixed-dimensional settings, the algorithm's complexity scales polynomially with
the separation between modes and logarithmically with $1/\varepsilon$, where
$\varepsilon$ is the target accuracy in total variation distance.",http://arxiv.org/abs/2505.15059v1
"MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation","Precise recognition, editing, and generation of molecules are essential
prerequisites for both chemists and AI systems tackling various chemical tasks.
We present MolLangBench, a comprehensive benchmark designed to evaluate
fundamental molecule-language interface tasks: language-prompted molecular
structure recognition, editing, and generation. To ensure high-quality,
unambiguous, and deterministic outputs, we construct the recognition tasks
using automated cheminformatics tools, and curate editing and generation tasks
through rigorous expert annotation and validation. MolLangBench supports the
evaluation of models that interface language with different molecular
representations, including linear strings, molecular images, and molecular
graphs. Evaluations of state-of-the-art models reveal significant limitations:
the strongest model (o3) achieves $79.2\%$ and $78.5\%$ accuracy on recognition
and editing tasks, which are intuitively simple for humans, and performs even
worse on the generation task, reaching only $29.0\%$ accuracy. These results
highlight the shortcomings of current AI systems in handling even preliminary
molecular recognition and manipulation tasks. We hope MolLangBench will
catalyze further research toward more effective and reliable AI systems for
chemical applications.",http://arxiv.org/abs/2505.15054v1
PiFlow: Principle-aware Scientific Discovery with Multi-Agent Collaboration,"Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate
remarkable potential for scientific discovery. Existing approaches, however,
often automate scientific discovery using predefined workflows that lack
rationality constraints. This often leads to aimless hypothesizing and a
failure to consistently link hypotheses with evidence, thereby hindering
systematic uncertainty reduction. Overcoming these limitations fundamentally
requires systematic uncertainty reduction. We introduce \texttt{PiFlow}, an
information-theoretical framework, treating automated scientific discovery as a
structured uncertainty reduction problem guided by principles (e.g., scientific
laws). In evaluations across three distinct scientific domains -- discovering
nanomaterial structures, bio-molecules, and superconductor candidates with
targeted properties -- our method significantly improves discovery efficiency,
reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property
values versus exploration steps, and enhances solution quality by 94.06\%
compared to a vanilla agent system. Overall, \texttt{PiFlow} serves as a
Plug-and-Play method, establishing a novel paradigm shift in highly efficient
automated scientific discovery, paving the way for more robust and accelerated
AI-driven research. Code is publicly available at our
\href{https://github.com/amair-lab/PiFlow}{GitHub}.",http://arxiv.org/abs/2505.15047v1
Learning-based Airflow Inertial Odometry for MAVs using Thermal Anemometers in a GPS and vision denied environment,"This work demonstrates an airflow inertial based odometry system with
multi-sensor data fusion, including thermal anemometer, IMU, ESC, and
barometer. This goal is challenging because low-cost IMUs and barometers have
significant bias, and anemometer measurements are very susceptible to
interference from spinning propellers and ground effects. We employ a GRU-based
deep neural network to estimate relative air speed from noisy and disturbed
anemometer measurements, and an observer with bias model to fuse the sensor
data and thus estimate the state of aerial vehicle. A complete flight data,
including takeoff and landing on the ground, shows that the approach is able to
decouple the downwash induced wind speed caused by propellers and the ground
effect, and accurately estimate the flight speed in a wind-free indoor
environment. IMU, and barometer bias are effectively estimated, which
significantly reduces the position integration drift, which is only 5.7m for
203s manual random flight. The open source is available on
https://github.com/SyRoCo-ISIR/Flight-Speed-Estimation-Airflow.",http://arxiv.org/abs/2505.15044v1
RLBenchNet: The Right Network for the Right Reinforcement Learning Task,"Reinforcement learning (RL) has seen significant advancements through the
application of various neural network architectures. In this study, we
systematically investigate the performance of several neural networks in RL
tasks, including Long Short-Term Memory (LSTM), Multi-Layer Perceptron (MLP),
Mamba/Mamba-2, Transformer-XL, Gated Transformer-XL, and Gated Recurrent Unit
(GRU). Through comprehensive evaluation across continuous control, discrete
decision-making, and memory-based environments, we identify
architecture-specific strengths and limitations. Our results reveal that: (1)
MLPs excel in fully observable continuous control tasks, providing an optimal
balance of performance and efficiency; (2) recurrent architectures like LSTM
and GRU offer robust performance in partially observable environments with
moderate memory requirements; (3) Mamba models achieve a 4.5x higher throughput
compared to LSTM and a 3.9x increase over GRU, all while maintaining comparable
performance; and (4) only Transformer-XL, Gated Transformer-XL, and Mamba-2
successfully solve the most challenging memory-intensive tasks, with Mamba-2
requiring 8x less memory than Transformer-XL. These findings provide insights
for researchers and practitioners, enabling more informed architecture
selection based on specific task characteristics and computational constraints.
Code is available at: https://github.com/SafeRL-Lab/RLBenchNet",http://arxiv.org/abs/2505.15040v1
RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning,"Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.",http://arxiv.org/abs/2505.15034v1
Harnessing On-Device Large Language Model: Empirical Results and Implications for AI PC,"The increasing deployment of Large Language Models (LLMs) on edge devices,
driven by model advancements and hardware improvements, offers significant
privacy benefits. However, these on-device LLMs inherently face performance
limitations due to reduced model capacity and necessary compression techniques.
To address this, we introduce a systematic methodology -- encompassing model
capability, development efficiency, and system resources -- for evaluating
on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to
14B parameters and seven post-training quantization (PTQ) methods on commodity
laptops, yields several critical insights: 1) System-level metrics exhibit
near-linear scaling with effective bits-per-weight (BPW). 2) A practical
threshold exists around $\sim$3.5 effective BPW, larger models subjected to
low-bit quantization consistently outperform smaller models utilizing higher
bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but
significant memory savings. 4) Determined by low-level implementation specifics
power consumption on CPU, where computation-intensive operations spend more
power than memory-intensive ones. These findings offer crucial insights and
practical guidelines for the efficient deployment and optimized configuration
of LLMs on resource-constrained edge devices. Our codebase is available at
https://github.com/simmonssong/LLMOnDevice.",http://arxiv.org/abs/2505.15030v2
Infinite hierarchical contrastive clustering for personal digital envirotyping,"Daily environments have profound influence on our health and behavior. Recent
work has shown that digital envirotyping, where computer vision is applied to
images of daily environments taken during ecological momentary assessment
(EMA), can be used to identify meaningful relationships between environmental
features and health outcomes of interest. To systematically study such effects
on an individual level, it is helpful to group images into distinct
environments encountered in an individual's daily life; these may then be
analyzed, further grouped into related environments with similar features, and
linked to health outcomes. Here we introduce infinite hierarchical contrastive
clustering to address this challenge. Building on the established contrastive
clustering framework, our method a) allows an arbitrary number of clusters
without requiring the full Dirichlet Process machinery by placing a
stick-breaking prior on predicted cluster probabilities; and b) encourages
distinct environments to form well-defined sub-clusters within each cluster of
related environments by incorporating a participant-specific prediction loss.
Our experiments show that our model effectively identifies distinct personal
environments and groups these environments into meaningful environment types.
We then illustrate how the resulting clusters can be linked to various health
outcomes, highlighting the potential of our approach to advance the
envirotyping paradigm.",http://arxiv.org/abs/2505.15022v1
Beyond Node Attention: Multi-Scale Harmonic Encoding for Feature-Wise Graph Message Passing,"Conventional Graph Neural Networks (GNNs) aggregate neighbor embeddings as
holistic vectors, lacking the ability to identify fine-grained,
direction-specific feature relevance. We propose MSH-GNN (Multi-Scale Harmonic
Graph Neural Network), a novel architecture that performs feature-wise adaptive
message passing through node-specific harmonic projections. For each node,
MSH-GNN dynamically projects neighbor features onto frequency-sensitive
directions determined by the target node's own representation. These
projections are further modulated using learnable sinusoidal encodings at
multiple frequencies, enabling the model to capture both smooth and oscillatory
structural patterns across scales. A frequency-aware attention pooling
mechanism is introduced to emphasize spectrally and structurally salient nodes
during readout. Theoretically, we prove that MSH-GNN approximates
shift-invariant kernels and matches the expressive power of the
1-Weisfeiler-Lehman (1-WL) test. Empirically, MSH-GNN consistently outperforms
state-of-the-art models on a wide range of graph and node classification tasks.
Furthermore, in challenging classification settings involving joint variations
in graph topology and spectral frequency, MSH-GNN excels at capturing
structural asymmetries and high-frequency modulations, enabling more accurate
graph discrimination.",http://arxiv.org/abs/2505.15015v1
Convergence of Adam in Deep ReLU Networks via Directional Complexity and Kakeya Bounds,"First-order adaptive optimization methods like Adam are the default choices
for training modern deep neural networks. Despite their empirical success, the
theoretical understanding of these methods in non-smooth settings, particularly
in Deep ReLU networks, remains limited. ReLU activations create exponentially
many region boundaries where standard smoothness assumptions break down.
\textbf{We derive the first
\(\tilde{O}\!\bigl(\sqrt{d_{\mathrm{eff}}/n}\bigr)\) generalization bound for
Adam in Deep ReLU networks and the first global-optimal convergence for Adam in
the non smooth, non convex relu landscape without a global PL or convexity
assumption.} Our analysis is based on stratified Morse theory and novel results
in Kakeya sets. We develop a multi-layer refinement framework that
progressively tightens bounds on region crossings. We prove that the number of
region crossings collapses from exponential to near-linear in the effective
dimension. Using a Kakeya based method, we give a tighter generalization bound
than PAC-Bayes approaches and showcase convergence using a mild uniform low
barrier assumption.",http://arxiv.org/abs/2505.15013v1
One-Layer Transformers are Provably Optimal for In-context Reasoning and Distributional Association Learning in Next-Token Prediction Tasks,"We study the approximation capabilities and on-convergence behaviors of
one-layer transformers on the noiseless and noisy in-context reasoning of
next-token prediction. Existing theoretical results focus on understanding the
in-context reasoning behaviors for either the first gradient step or when the
number of samples is infinite. Furthermore, no convergence rates nor
generalization abilities were known. Our work addresses these gaps by showing
that there exists a class of one-layer transformers that are provably
Bayes-optimal with both linear and ReLU attention. When being trained with
gradient descent, we show via a finite-sample analysis that the expected loss
of these transformers converges at linear rate to the Bayes risk. Moreover, we
prove that the trained models generalize to unseen samples as well as exhibit
learning behaviors that were empirically observed in previous works. Our
theoretical findings are further supported by extensive empirical validations.",http://arxiv.org/abs/2505.15009v1
Know When to Abstain: Optimal Selective Classification with Likelihood Ratios,"Selective classification enhances the reliability of predictive models by
allowing them to abstain from making uncertain predictions. In this work, we
revisit the design of optimal selection functions through the lens of the
Neyman--Pearson lemma, a classical result in statistics that characterizes the
optimal rejection rule as a likelihood ratio test. We show that this
perspective not only unifies the behavior of several post-hoc selection
baselines, but also motivates new approaches to selective classification which
we propose here. A central focus of our work is the setting of covariate shift,
where the input distribution at test time differs from that at training. This
realistic and challenging scenario remains relatively underexplored in the
context of selective classification. We evaluate our proposed methods across a
range of vision and language tasks, including both supervised learning and
vision-language models. Our experiments demonstrate that our
Neyman--Pearson-informed methods consistently outperform existing baselines,
indicating that likelihood ratio-based selection offers a robust mechanism for
improving selective classification under covariate shifts. Our code is publicly
available at https://github.com/clear-nus/sc-likelihood-ratios.",http://arxiv.org/abs/2505.15008v1
Unraveling the iterative CHAD,"Combinatory Homomorphic Automatic Differentiation (CHAD) was originally
formulated as a semantics-driven source transformation for reverse-mode AD in
total programming languages. We extend this framework to partial languages with
features such as potentially non-terminating operations, real-valued
conditionals, and iteration constructs like while-loops, while preserving
CHAD's structure-preserving semantics principle. A key contribution is the
introduction of iteration-extensive indexed categories, which allow iteration
in the base category to lift to parameterized initial algebras in the indexed
category. This enables iteration to be interpreted in the Grothendieck
construction of the target language in a principled way. The resulting fibred
iterative structure cleanly models iteration in the categorical semantics.
Consequently, the extended CHAD transformation remains the unique
structure-preserving functor (an iterative Freyd category morphism) from the
freely generated iterative Freyd category of the source language to the
Grothendieck construction of the target's syntactic semantics, mapping each
primitive operation to its derivative. We prove the correctness of this
transformation using the universal property of the source language's syntax,
showing that the transformed programs compute correct reverse-mode derivatives.
Our development also contributes to understanding iteration constructs within
dependently typed languages and categories of containers. As our primary
motivation and application, we generalize CHAD to languages with data types,
partial features, and iteration, providing the first rigorous categorical
semantics for reverse-mode CHAD in such settings and formally guaranteeing the
correctness of the source-to-source CHAD technique.",http://arxiv.org/abs/2505.15002v1
Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision,"Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs), often requiring robust multi step logical consistency. While
Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee
correctness, and improving reliability via extensive sampling is
computationally costly. This paper introduces the Energy Outcome Reward Model
(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy
Based Models (EBMs) to simplify the training of reward models by learning to
assign a scalar energy score to CoT solutions using only outcome labels,
thereby avoiding detailed annotations. It achieves this by interpreting
discriminator output logits as negative energies, effectively ranking
candidates where lower energy is assigned to solutions leading to correct final
outcomes implicitly favoring coherent reasoning. On mathematical benchmarks
(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with
Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively
leverages a given pool of candidate solutions to match or exceed the
performance of brute force sampling, thereby enhancing LLM reasoning outcome
reliability through its streamlined post hoc verification process.",http://arxiv.org/abs/2505.14999v1
Meta-Design Matters: A Self-Design Multi-Agent System,"Multi-agent systems (MAS) leveraging the impressive capabilities of Large
Language Models (LLMs) hold significant potential for tackling complex tasks.
However, most current MAS depend on manually designed agent roles and
communication protocols. These manual designs often fail to align with the
underlying LLMs' strengths and struggle to adapt to novel tasks. Recent
automatic MAS approaches attempt to mitigate these limitations but typically
necessitate a validation-set for tuning and yield static MAS designs lacking
adaptability during inference. We introduce SELF-MAS, the first
self-supervised, inference-time only framework for automatic MAS design.
SELF-MAS employs meta-level design to iteratively generate, evaluate, and
refine MAS configurations tailored to each problem instance, without requiring
a validation set. Critically, it enables dynamic agent composition and problem
decomposition through meta-feedback on solvability and completeness.
Experiments across math, graduate-level QA, and software engineering
benchmarks, using both closed-source and open-source LLM back-bones of varying
sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS
baselines, achieving a 7.44% average accuracy improvement over the next
strongest baseline while maintaining cost-efficiency. These findings underscore
the promise of meta-level self-supervised design for creating effective and
adaptive MAS.",http://arxiv.org/abs/2505.14996v1
AnyBody: A Benchmark Suite for Cross-Embodiment Manipulation,"Generalizing control policies to novel embodiments remains a fundamental
challenge in enabling scalable and transferable learning in robotics. While
prior works have explored this in locomotion, a systematic study in the context
of manipulation tasks remains limited, partly due to the lack of standardized
benchmarks. In this paper, we introduce a benchmark for learning
cross-embodiment manipulation, focusing on two foundational tasks-reach and
push-across a diverse range of morphologies. The benchmark is designed to test
generalization along three axes: interpolation (testing performance within a
robot category that shares the same link structure), extrapolation (testing on
a robot with a different link structure), and composition (testing on
combinations of link structures). On the benchmark, we evaluate the ability of
different RL policies to learn from multiple morphologies and to generalize to
novel ones. Our study aims to answer whether morphology-aware training can
outperform single-embodiment baselines, whether zero-shot generalization to
unseen morphologies is feasible, and how consistently these patterns hold
across different generalization regimes. The results highlight the current
limitations of multi-embodiment learning and provide insights into how
architectural and training design choices influence policy generalization.",http://arxiv.org/abs/2505.14986v1
Pre-validation Revisited,"Pre-validation is a way to build prediction model with two datasets of
significantly different feature dimensions. Previous work showed that the
asymptotic distribution of the resulting test statistic for the pre-validated
predictor deviates from a standard Normal, hence leads to issues in hypothesis
testing. In this paper, we revisit the pre-validation procedure and extend the
problem formulation without any independence assumption on the two feature
sets. We propose not only an analytical distribution of the test statistic for
the pre-validated predictor under certain models, but also a generic bootstrap
procedure to conduct inference. We show properties and benefits of
pre-validation in prediction, inference and error estimation by simulations and
applications, including analysis of a breast cancer study and a synthetic GWAS
example.",http://arxiv.org/abs/2505.14985v2
JARVIS: A Multi-Agent Code Assistant for High-Quality EDA Script Generation,"This paper presents JARVIS, a novel multi-agent framework that leverages
Large Language Models (LLMs) and domain expertise to generate high-quality
scripts for specialized Electronic Design Automation (EDA) tasks. By combining
a domain-specific LLM trained with synthetically generated data, a custom
compiler for structural verification, rule enforcement, code fixing
capabilities, and advanced retrieval mechanisms, our approach achieves
significant improvements over state-of-the-art domain-specific models. Our
framework addresses the challenges of data scarcity and hallucination errors in
LLMs, demonstrating the potential of LLMs in specialized engineering domains.
We evaluate our framework on multiple benchmarks and show that it outperforms
existing models in terms of accuracy and reliability. Our work sets a new
precedent for the application of LLMs in EDA and paves the way for future
innovations in this field.",http://arxiv.org/abs/2505.14978v1
Flattening Hierarchies with Policy Bootstrapping,"Offline goal-conditioned reinforcement learning (GCRL) is a promising
approach for pretraining generalist policies on large datasets of reward-free
trajectories, akin to the self-supervised objectives used to train foundation
models for computer vision and natural language processing. However, scaling
GCRL to longer horizons remains challenging due to the combination of sparse
rewards and discounting, which obscures the comparative advantages of primitive
actions with respect to distant goals. Hierarchical RL methods achieve strong
empirical results on long-horizon goal-reaching tasks, but their reliance on
modular, timescale-specific policies and subgoal generation introduces
significant additional complexity and hinders scaling to high-dimensional goal
spaces. In this work, we introduce an algorithm to train a flat
(non-hierarchical) goal-conditioned policy by bootstrapping on
subgoal-conditioned policies with advantage-weighted importance sampling. Our
approach eliminates the need for a generative model over the (sub)goal space,
which we find is key for scaling to high-dimensional control in large state
spaces. We further show that existing hierarchical and bootstrapping-based
approaches correspond to specific design choices within our derivation. Across
a comprehensive suite of state- and pixel-based locomotion and manipulation
benchmarks, our method matches or surpasses state-of-the-art offline GCRL
algorithms and scales to complex, long-horizon tasks where prior approaches
fail.",http://arxiv.org/abs/2505.14975v1
Self-Evolving Curriculum for LLM Reasoning,"Reinforcement learning (RL) has proven effective for fine-tuning large
language models (LLMs), significantly enhancing their reasoning abilities in
domains such as mathematics and code generation. A crucial factor influencing
RL fine-tuning success is the training curriculum: the order in which training
problems are presented. While random curricula serve as common baselines, they
remain suboptimal; manually designed curricula often rely heavily on
heuristics, and online filtering methods can be computationally prohibitive. To
address these limitations, we propose Self-Evolving Curriculum (SEC), an
automatic curriculum learning method that learns a curriculum policy
concurrently with the RL fine-tuning process. Our approach formulates
curriculum selection as a non-stationary Multi-Armed Bandit problem, treating
each problem category (e.g., difficulty level or problem type) as an individual
arm. We leverage the absolute advantage from policy gradient methods as a proxy
measure for immediate learning gain. At each training step, the curriculum
policy selects categories to maximize this reward signal and is updated using
the TD(0) method. Across three distinct reasoning domains: planning, inductive
reasoning, and mathematics, our experiments demonstrate that SEC significantly
improves models' reasoning capabilities, enabling better generalization to
harder, out-of-distribution test problems. Additionally, our approach achieves
better skill balance when fine-tuning simultaneously on multiple reasoning
domains. These findings highlight SEC as a promising strategy for RL
fine-tuning of LLMs.",http://arxiv.org/abs/2505.14970v1
STree: Speculative Tree Decoding for Hybrid State-Space Models,"Speculative decoding is a technique to leverage hardware concurrency to
improve the efficiency of large-scale autoregressive (AR) Transformer models by
enabling multiple steps of token generation in a single forward pass.
State-space models (SSMs) are already more efficient than AR Transformers,
since their state summarizes all past data with no need to cache or re-process
tokens in the sliding window context. However, their state can also comprise
thousands of tokens; so, speculative decoding has recently been extended to
SSMs. Existing approaches, however, do not leverage the tree-based verification
methods, since current SSMs lack the means to compute a token tree efficiently.
We propose the first scalable algorithm to perform tree-based speculative
decoding in state-space models (SSMs) and hybrid architectures of SSMs and
Transformer layers. We exploit the structure of accumulated state transition
matrices to facilitate tree-based speculative decoding with minimal overhead to
current SSM state update implementations. With the algorithm, we describe a
hardware-aware implementation that improves naive application of AR Transformer
tree-based speculative decoding methods to SSMs. Furthermore, we outperform
vanilla speculative decoding with SSMs even with a baseline drafting model and
tree structure on three different benchmarks, opening up opportunities for
further speed up with SSM and hybrid model inference. Code will be released
upon paper acceptance.",http://arxiv.org/abs/2505.14969v1
Anomaly Detection Based on Critical Paths for Deep Neural Networks,"Deep neural networks (DNNs) are notoriously hard to understand and difficult
to defend. Extracting representative paths (including the neuron activation
values and the connections between neurons) from DNNs using software
engineering approaches has recently shown to be a promising approach in
interpreting the decision making process of blackbox DNNs, as the extracted
paths are often effective in capturing essential features. With this in mind,
this work investigates a novel approach that extracts critical paths from DNNs
and subsequently applies the extracted paths for the anomaly detection task,
based on the observation that outliers and adversarial inputs do not usually
induce the same activation pattern on those paths as normal (in-distribution)
inputs.
  In our approach, we first identify critical detection paths via genetic
evolution and mutation. Since different paths in a DNN often capture different
features for the same target class, we ensemble detection results from multiple
paths by integrating random subspace sampling and a voting mechanism. Compared
with state-of-the-art methods, our experimental results suggest that our method
not only outperforms them, but it is also suitable for the detection of a broad
range of anomaly types with high accuracy.",http://arxiv.org/abs/2505.14967v1
The Achilles Heel of AI: Fundamentals of Risk-Aware Training Data for High-Consequence Models,"AI systems in high-consequence domains such as defense, intelligence, and
disaster response must detect rare, high-impact events while operating under
tight resource constraints. Traditional annotation strategies that prioritize
label volume over informational value introduce redundancy and noise, limiting
model generalization. This paper introduces smart-sizing, a training data
strategy that emphasizes label diversity, model-guided selection, and marginal
utility-based stopping. We implement this through Adaptive Label Optimization
(ALO), combining pre-labeling triage, annotator disagreement analysis, and
iterative feedback to prioritize labels that meaningfully improve model
performance. Experiments show that models trained on 20 to 40 percent of
curated data can match or exceed full-data baselines, particularly in
rare-class recall and edge-case generalization. We also demonstrate how latent
labeling errors embedded in training and validation sets can distort
evaluation, underscoring the need for embedded audit tools and
performance-aware governance. Smart-sizing reframes annotation as a
feedback-driven process aligned with mission outcomes, enabling more robust
models with fewer labels and supporting efficient AI development pipelines for
frontier models and operational systems.",http://arxiv.org/abs/2505.14964v1
Privacy Preserving Conversion Modeling in Data Clean Room,"In the realm of online advertising, accurately predicting the conversion rate
(CVR) is crucial for enhancing advertising efficiency and user satisfaction.
This paper addresses the challenge of CVR prediction while adhering to user
privacy preferences and advertiser requirements. Traditional methods face
obstacles such as the reluctance of advertisers to share sensitive conversion
data and the limitations of model training in secure environments like data
clean rooms. We propose a novel model training framework that enables
collaborative model training without sharing sample-level gradients with the
advertising platform. Our approach introduces several innovative components:
(1) utilizing batch-level aggregated gradients instead of sample-level
gradients to minimize privacy risks; (2) applying adapter-based
parameter-efficient fine-tuning and gradient compression to reduce
communication costs; and (3) employing de-biasing techniques to train the model
under label differential privacy, thereby maintaining accuracy despite
privacy-enhanced label perturbations. Our experimental results, conducted on
industrial datasets, demonstrate that our method achieves competitive ROCAUC
performance while significantly decreasing communication overhead and complying
with both advertiser privacy requirements and user privacy choices. This
framework establishes a new standard for privacy-preserving, high-performance
CVR prediction in the digital advertising landscape.",http://arxiv.org/abs/2505.14959v1
Programmatic Video Prediction Using Large Language Models,"The task of estimating the world model describing the dynamics of a real
world process assumes immense importance for anticipating and preparing for
future outcomes. For applications such as video surveillance, robotics
applications, autonomous driving, etc. this objective entails synthesizing
plausible visual futures, given a few frames of a video to set the visual
context. Towards this end, we propose ProgGen, which undertakes the task of
video frame prediction by representing the dynamics of the video using a set of
neuro-symbolic, human-interpretable set of states (one per frame) by leveraging
the inductive biases of Large (Vision) Language Models (LLM/VLM). In
particular, ProgGen utilizes LLM/VLM to synthesize programs: (i) to estimate
the states of the video, given the visual context (i.e. the frames); (ii) to
predict the states corresponding to future time steps by estimating the
transition dynamics; (iii) to render the predicted states as visual RGB-frames.
Empirical evaluations reveal that our proposed method outperforms competing
techniques at the task of video frame prediction in two challenging
environments: (i) PhyWorld (ii) Cart Pole. Additionally, ProgGen permits
counter-factual reasoning and interpretable video generation attesting to its
effectiveness and generalizability for video generation tasks.",http://arxiv.org/abs/2505.14948v1
Unlearning Algorithmic Biases over Graphs,"The growing enforcement of the right to be forgotten regulations has
propelled recent advances in certified (graph) unlearning strategies to comply
with data removal requests from deployed machine learning (ML) models.
Motivated by the well-documented bias amplification predicament inherent to
graph data, here we take a fresh look at graph unlearning and leverage it as a
bias mitigation tool. Given a pre-trained graph ML model, we develop a
training-free unlearning procedure that offers certifiable bias mitigation via
a single-step Newton update on the model weights. This way, we contribute a
computationally lightweight alternative to the prevalent training- and
optimization-based fairness enhancement approaches, with quantifiable
performance guarantees. We first develop a novel fairness-aware nodal feature
unlearning strategy along with refined certified unlearning bounds for this
setting, whose impact extends beyond the realm of graph unlearning. We then
design structural unlearning methods endowed with principled selection
mechanisms over nodes and edges informed by rigorous bias analyses. Unlearning
these judiciously selected elements can mitigate algorithmic biases with
minimal impact on downstream utility (e.g., node classification accuracy).
Experimental results over real networks corroborate the bias mitigation
efficacy of our unlearning strategies, and delineate markedly favorable
utility-complexity trade-offs relative to retraining from scratch using
augmented graph data obtained via removals.",http://arxiv.org/abs/2505.14945v1
Soft Prompts for Evaluation: Measuring Conditional Distance of Capabilities,"To help evaluate and understand the latent capabilities of language models,
this paper introduces an approach using optimized input embeddings, or 'soft
prompts,' as a metric of conditional distance between a model and a target
behavior. The technique aims to facilitate latent capability discovery as a
part of automated red teaming/evaluation suites and to provide quantitative
feedback about the accessibility of potentially concerning behaviors in a way
that may scale to powerful future models, including those which may otherwise
be capable of deceptive alignment. An evaluation framework using soft prompts
is demonstrated in natural language, chess, and pathfinding, and the technique
is extended with generalized conditional soft prompts to aid in constructing
task evaluations.",http://arxiv.org/abs/2505.14943v1
"Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning","Autonomous robots must reason about the physical consequences of their
actions to operate effectively in unstructured, real-world environments. We
present Scan, Materialize, Simulate (SMS), a unified framework that combines 3D
Gaussian Splatting for accurate scene reconstruction, visual foundation models
for semantic segmentation, vision-language models for material property
inference, and physics simulation for reliable prediction of action outcomes.
By integrating these components, SMS enables generalizable physical reasoning
and object-centric planning without the need to re-learn foundational physical
dynamics. We empirically validate SMS in a billiards-inspired manipulation task
and a challenging quadrotor landing scenario, demonstrating robust performance
on both simulated domain transfer and real-world experiments. Our results
highlight the potential of bridging differentiable rendering for scene
reconstruction, foundation models for semantic understanding, and physics-based
simulation to achieve physically grounded robot planning across diverse
settings.",http://arxiv.org/abs/2505.14938v1
Foundations of Unknown-aware Machine Learning,"Ensuring the reliability and safety of machine learning models in open-world
deployment is a central challenge in AI safety. This thesis develops both
algorithmic and theoretical foundations to address key reliability issues
arising from distributional uncertainty and unknown classes, from standard
neural networks to modern foundation models like large language models (LLMs).
  Traditional learning paradigms, such as empirical risk minimization (ERM),
assume no distribution shift between training and inference, often leading to
overconfident predictions on out-of-distribution (OOD) inputs. This thesis
introduces novel frameworks that jointly optimize for in-distribution accuracy
and reliability to unseen data. A core contribution is the development of an
unknown-aware learning framework that enables models to recognize and handle
novel inputs without labeled OOD data.
  We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to
generate informative unknowns during training. Building on this, we present
SAL, a theoretical and algorithmic framework that leverages unlabeled
in-the-wild data to enhance OOD detection under realistic deployment
conditions. These methods demonstrate that abundant unlabeled data can be
harnessed to recognize and adapt to unforeseen inputs, providing formal
reliability guarantees.
  The thesis also extends reliable learning to foundation models. We develop
HaloScope for hallucination detection in LLMs, MLLMGuard for defending against
malicious prompts in multimodal models, and data cleaning methods to denoise
human feedback used for better alignment. These tools target failure modes that
threaten the safety of large-scale models in deployment.
  Overall, these contributions promote unknown-aware learning as a new
paradigm, and we hope it can advance the reliability of AI systems with minimal
human efforts.",http://arxiv.org/abs/2505.14933v1
"Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels","Although the context length of large language models (LLMs) has increased to
millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack
approaches has proven difficult. We argue that novels provide a case study of
subtle, complicated structure and long-range semantic dependencies often over
128k tokens in length. Inspired by work on computational novel analysis, we
release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's
ability to report plot summary, storyworld configuration, and elapsed narrative
time. We find that none of seven tested frontier LLMs retain stable
understanding beyond 64k tokens. Our results suggest language model developers
must look beyond ""lost in the middle"" benchmarks when evaluating model
performance in complex long-context scenarios. To aid in further development we
release the TLDM benchmark together with reference code and data.",http://arxiv.org/abs/2505.14925v1
SecCAN: An Extended CAN Controller with Embedded Intrusion Detection,"Recent research has highlighted the vulnerability of in-vehicle network
protocols such as controller area networks (CAN) and proposed machine
learning-based intrusion detection systems (IDSs) as an effective mitigation
technique. However, their efficient integration into vehicular architecture is
non-trivial, with existing methods relying on electronic control units
(ECUs)-coupled IDS accelerators or dedicated ECUs as IDS accelerators. Here,
initiating IDS requires complete reception of a CAN message from the
controller, incurring data movement and software overheads. In this paper, we
present SecCAN, a novel CAN controller architecture that embeds IDS capability
within the datapath of the controller. This integration allows IDS to tap
messages directly from within the CAN controller as they are received from the
bus, removing overheads incurred by existing ML-based IDSs. A custom-quantised
machine-learning accelerator is developed as the IDS engine and embedded into
SecCAN's receive data path, with optimisations to overlap the IDS inference
with the protocol's reception window. We implement SecCAN on AMD XCZU7EV FPGA
to quantify its performance and benefits in hardware, using multiple attack
datasets. We show that SecCAN can completely hide the IDS latency within the
CAN reception window for all CAN packet sizes and detect multiple attacks with
state-of-the-art accuracy with zero software overheads on the ECU and low
energy overhead (73.7 uJ per message) for IDS inference. Also, SecCAN incurs
limited resource overhead compared to a standard CAN controller (< 30% LUT, <
1% FF), making it ideally suited for automotive deployment.",http://arxiv.org/abs/2505.14924v1
TxPert: Leveraging Biochemical Relationships for Out-of-Distribution Transcriptomic Perturbation Prediction,"Accurately predicting cellular responses to genetic perturbations is
essential for understanding disease mechanisms and designing effective
therapies. Yet exhaustively exploring the space of possible perturbations
(e.g., multi-gene perturbations or across tissues and cell types) is
prohibitively expensive, motivating methods that can generalize to unseen
conditions. In this work, we explore how knowledge graphs of gene-gene
relationships can improve out-of-distribution (OOD) prediction across three
challenging settings: unseen single perturbations; unseen double perturbations;
and unseen cell lines. In particular, we present: (i) TxPert, a new
state-of-the-art method that leverages multiple biological knowledge networks
to predict transcriptional responses under OOD scenarios; (ii) an in-depth
analysis demonstrating the impact of graphs, model architecture, and data on
performance; and (iii) an expanded benchmarking framework that strengthens
evaluation standards for perturbation modeling.",http://arxiv.org/abs/2505.14919v1
Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications,"This study introduces a framework for evaluating consistency in large
language model (LLM) binary text classification, addressing the lack of
established reliability assessment methods. Adapting psychometric principles,
we determine sample size requirements, develop metrics for invalid responses,
and evaluate intra- and inter-rater reliability. Our case study examines
financial news sentiment classification across 14 LLMs (including
claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and
command-r-plus), with five replicates per model on 1,350 articles. Models
demonstrated high intra-rater consistency, achieving perfect agreement on
90-98% of examples, with minimal differences between expensive and economical
models from the same families. When validated against StockNewsAPI labels,
models achieved strong performance (accuracy 0.76-0.88), with smaller models
like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger
counterparts. All models performed at chance when predicting actual market
movements, indicating task constraints rather than model limitations. Our
framework provides systematic guidance for LLM selection, sample size planning,
and reliability assessment, enabling organizations to optimize resources for
classification tasks.",http://arxiv.org/abs/2505.14918v1
When to retrain a machine learning model,"A significant challenge in maintaining real-world machine learning models is
responding to the continuous and unpredictable evolution of data. Most
practitioners are faced with the difficult question: when should I retrain or
update my machine learning model? This seemingly straightforward problem is
particularly challenging for three reasons: 1) decisions must be made based on
very limited information - we usually have access to only a few examples, 2)
the nature, extent, and impact of the distribution shift are unknown, and 3) it
involves specifying a cost ratio between retraining and poor performance, which
can be hard to characterize. Existing works address certain aspects of this
problem, but none offer a comprehensive solution. Distribution shift detection
falls short as it cannot account for the cost trade-off; the scarcity of the
data, paired with its unusual structure, makes it a poor fit for existing
offline reinforcement learning methods, and the online learning formulation
overlooks key practical considerations. To address this, we present a
principled formulation of the retraining problem and propose an
uncertainty-based method that makes decisions by continually forecasting the
evolution of model performance evaluated with a bounded metric. Our experiments
addressing classification tasks show that the method consistently outperforms
existing baselines on 7 datasets.",http://arxiv.org/abs/2505.14903v1
Multi-Channel Swin Transformer Framework for Bearing Remaining Useful Life Prediction,"Precise estimation of the Remaining Useful Life (RUL) of rolling bearings is
an important consideration to avoid unexpected failures, reduce downtime, and
promote safety and efficiency in industrial systems. Complications in
degradation trends, noise presence, and the necessity to detect faults in
advance make estimation of RUL a challenging task. This paper introduces a
novel framework that combines wavelet-based denoising method, Wavelet Packet
Decomposition (WPD), and a customized multi-channel Swin Transformer model
(MCSFormer) to address these problems. With attention mechanisms incorporated
for feature fusion, the model is designed to learn global and local degradation
patterns utilizing hierarchical representations for enhancing predictive
performance. Additionally, a customized loss function is developed as a key
distinction of this work to differentiate between early and late predictions,
prioritizing accurate early detection and minimizing the high operation risks
of late predictions. The proposed model was evaluated with the PRONOSTIA
dataset using three experiments. Intra-condition experiments demonstrated that
MCSFormer outperformed state-of-the-art models, including the Adaptive
Transformer, MDAN, and CNN-SRU, achieving 41%, 64%, and 69% lower MAE on
average across different operating conditions, respectively. In terms of
cross-condition testing, it achieved superior generalization under varying
operating conditions compared to the adapted ViT and Swin Transformer. Lastly,
the custom loss function effectively reduced late predictions, as evidenced in
a 6.3% improvement in the scoring metric while maintaining competitive overall
performance. The model's robust noise resistance, generalization capability,
and focus on safety make MCSFormer a trustworthy and effective predictive
maintenance tool in industrial applications.",http://arxiv.org/abs/2505.14897v1
Feature-Weighted MMD-CORAL for Domain Adaptation in Power Transformer Fault Diagnosis,"Ensuring the reliable operation of power transformers is critical to grid
stability. Dissolved Gas Analysis (DGA) is widely used for fault diagnosis, but
traditional methods rely on heuristic rules, which may lead to inconsistent
results. Machine learning (ML)-based approaches have improved diagnostic
accuracy; however, power transformers operate under varying conditions, and
differences in transformer type, environmental factors, and operational
settings create distribution shifts in diagnostic data. Consequently, direct
model transfer between transformers often fails, making techniques for domain
adaptation a necessity. To tackle this issue, this work proposes a
feature-weighted domain adaptation technique that combines Maximum Mean
Discrepancy (MMD) and Correlation Alignment (CORAL) with feature-specific
weighting (MCW). Kolmogorov-Smirnov (K-S) statistics are used to assign
adaptable weights, prioritizing features with larger distributional
discrepancies and thereby improving source and target domain alignment.
Experimental evaluations on datasets for power transformers demonstrate the
effectiveness of the proposed method, which achieves a 7.9% improvement over
Fine-Tuning and a 2.2% improvement over MMD-CORAL (MC). Furthermore, it
outperforms both techniques across various training sample sizes, confirming
its robustness for domain adaptation.",http://arxiv.org/abs/2505.14896v1
Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity,"Accelerating large language model (LLM) inference is critical for real-world
deployments requiring high throughput and low latency. Contextual sparsity,
where each token dynamically activates only a small subset of the model
parameters, shows promise but does not scale to large batch sizes due to union
of active neurons quickly approaching dense computation. We introduce Polar
Sparsity, highlighting a key shift in sparsity importance from MLP to Attention
layers as we scale batch size and sequence length. While MLP layers become more
compute-efficient under batching, their sparsity vanishes. In contrast,
attention becomes increasingly more expensive at scale, while their head
sparsity remains stable and batch-invariant. We develop hardware-efficient,
sparsity-aware GPU kernels for selective MLP and Attention computations,
delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2
\& 3, across various batch sizes and sequence lengths without compromising
accuracy. To our knowledge, this is the first work to demonstrate that
contextual sparsity can scale effectively to large batch sizes, delivering
substantial inference acceleration with minimal changes, making Polar Sparsity
practical for large-scale, high-throughput LLM deployment systems. Our code is
available at: https://github.com/susavlsh10/Polar-Sparsity.",http://arxiv.org/abs/2505.14884v1
An active learning framework for multi-group mean estimation,"We study a fundamental learning problem over multiple groups with unknown
data distributions, where an analyst would like to learn the mean of each
group. Moreover, we want to ensure that this data is collected in a relatively
fair manner such that the noise of the estimate of each group is reasonable. In
particular, we focus on settings where data are collected dynamically, which is
important in adaptive experimentation for online platforms or adaptive clinical
trials for healthcare. In our model, we employ an active learning framework to
sequentially collect samples with bandit feedback, observing a sample in each
period from the chosen group. After observing a sample, the analyst updates
their estimate of the mean and variance of that group and chooses the next
group accordingly. The analyst's objective is to dynamically collect samples to
minimize the collective noise of the estimators, measured by the norm of the
vector of variances of the mean estimators.
  We propose an algorithm, Variance-UCB, that sequentially selects groups
according to an upper confidence bound on the variance estimate. We provide a
general theoretical framework for providing efficient bounds on learning from
any underlying distribution where the variances can be estimated reasonably.
This framework yields upper bounds on regret that improve significantly upon
all existing bounds, as well as a collection of new results for different
objectives and distributions than those previously studied.",http://arxiv.org/abs/2505.14882v1
A self-regulated convolutional neural network for classifying variable stars,"Over the last two decades, machine learning models have been widely applied
and have proven effective in classifying variable stars, particularly with the
adoption of deep learning architectures such as convolutional neural networks,
recurrent neural networks, and transformer models. While these models have
achieved high accuracy, they require high-quality, representative data and a
large number of labelled samples for each star type to generalise well, which
can be challenging in time-domain surveys. This challenge often leads to models
learning and reinforcing biases inherent in the training data, an issue that is
not easily detectable when validation is performed on subsamples from the same
catalogue. The problem of biases in variable star data has been largely
overlooked, and a definitive solution has yet to be established. In this paper,
we propose a new approach to improve the reliability of classifiers in variable
star classification by introducing a self-regulated training process. This
process utilises synthetic samples generated by a physics-enhanced latent space
variational autoencoder, incorporating six physical parameters from Gaia Data
Release 3. Our method features a dynamic interaction between a classifier and a
generative model, where the generative model produces ad-hoc synthetic light
curves to reduce confusion during classifier training and populate
underrepresented regions in the physical parameter space. Experiments conducted
under various scenarios demonstrate that our self-regulated training approach
outperforms traditional training methods for classifying variable stars on
biased datasets, showing statistically significant improvements.",http://arxiv.org/abs/2505.14877v1
Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models,"The efficient implementation of large language models (LLMs) is crucial for
deployment on resource-constrained devices. Low-rank tensor compression
techniques, such as tensor-train (TT) networks, have been widely studied for
over-parameterized neural networks. However, their applications to compress
pre-trained large language models (LLMs) for downstream tasks (post-training)
remains challenging due to the high-rank nature of pre-trained LLMs and the
lack of access to pretraining data. In this study, we investigate low-rank
tensorized LLMs during fine-tuning and propose sparse augmented tensor networks
(Saten) to enhance their performance. The proposed Saten framework enables full
model compression. Experimental results demonstrate that Saten enhances both
accuracy and compression efficiency in tensorized language models, achieving
state-of-the-art performance.",http://arxiv.org/abs/2505.14871v1
LOBSTUR: A Local Bootstrap Framework for Tuning Unsupervised Representations in Graph Neural Networks,"Graph Neural Networks (GNNs) are increasingly used in conjunction with
unsupervised learning techniques to learn powerful node representations, but
their deployment is hindered by their high sensitivity to hyperparameter tuning
and the absence of established methodologies for selecting the optimal models.
To address these challenges, we propose LOBSTUR-GNN ({\bf Lo}cal {\bf B}oot{\bf
s}trap for {\bf T}uning {\bf U}nsupervised {\bf R}epresentations in GNNs) i), a
novel framework designed to adapt bootstrapping techniques for unsupervised
graph representation learning. LOBSTUR-GNN tackles two main challenges: (a)
adapting the bootstrap edge and feature resampling process to account for local
graph dependencies in creating alternative versions of the same graph, and (b)
establishing robust metrics for evaluating learned representations without
ground-truth labels. Using locally bootstrapped resampling and leveraging
Canonical Correlation Analysis (CCA) to assess embedding consistency, LOBSTUR
provides a principled approach for hyperparameter tuning in unsupervised GNNs.
We validate the effectiveness and efficiency of our proposed method through
extensive experiments on established academic datasets, showing an 65.9\%
improvement in the classification accuracy compared to an uninformed selection
of hyperparameters. Finally, we deploy our framework on a real-world
application, thereby demonstrating its validity and practical utility in
various settings. \footnote{The code is available at
\href{https://github.com/sowonjeong/lobstur-graph-bootstrap}{github.com/sowonjeong/lobstur-graph-bootstrap}.}",http://arxiv.org/abs/2505.14867v1
EasyMath: A 0-shot Math Benchmark for SLMs,"EasyMath is a compact benchmark for practical math reasoning in small
language models. It covers thirteen categories, from basic arithmetic and order
of operations to word problems, algebraic expressions, edge cases, and omits
specialist topics. We tested 23 models (14M to 4B parameters) using exact,
numerical, and symbolic checks on free-form answers in a zero-shot setting.
Accuracy rises with size and training, chain-of-thought adds modest gains, and
consistency improves at scale.",http://arxiv.org/abs/2505.14852v1
MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation,"We present MAATS, a Multi Agent Automated Translation System that leverages
the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal
for error detection and refinement. MAATS employs multiple specialized AI
agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,
Style, Terminology), followed by a synthesis agent that integrates the
annotations to iteratively refine translations. This design contrasts with
conventional single-agent methods that rely on self-correction.
  Evaluated across diverse language pairs and Large Language Models (LLMs),
MAATS outperforms zero-shot and single-agent baselines with statistically
significant gains in both automatic metrics and human assessments. It excels
particularly in semantic accuracy, locale adaptation, and linguistically
distant language pairs. Qualitative analysis highlights its strengths in
multi-layered error diagnosis, omission detection across perspectives, and
context-aware refinement. By aligning modular agent roles with interpretable
MQM dimensions, MAATS narrows the gap between black-box LLMs and human
translation workflows, shifting focus from surface fluency to deeper semantic
and contextual fidelity.",http://arxiv.org/abs/2505.14848v1
Subquadratic Algorithms and Hardness for Attention with Any Temperature,"Despite the popularity of the Transformer architecture, the standard
algorithm for computing Attention suffers from quadratic time complexity in
context length $n$. Alman and Song [NeurIPS 2023] showed that when the head
dimension $d = \Theta(\log n)$, subquadratic Attention is possible if and only
if the inputs have small entries bounded by $B = o(\sqrt{\log n})$ in absolute
values, under the Strong Exponential Time Hypothesis ($\mathsf{SETH}$).
Equivalently, subquadratic Attention is possible if and only if the softmax is
applied with high temperature for $d=\Theta(\log n)$. Running times of these
algorithms depend exponentially on $B$ and thus they do not lead to even a
polynomial-time algorithm outside the specific range of $B$.
  This naturally leads to the question: when can Attention be computed
efficiently without strong assumptions on temperature? Are there fast attention
algorithms that scale polylogarithmically with entry size $B$? In this work, we
resolve this question and characterize when fast Attention for arbitrary
temperatures is possible. First, for all constant $d = O(1)$, we give the first
subquadratic $\tilde{O}(n^{2 - 1/d} \cdot \mathrm{polylog}(B))$ time algorithm
for Attention with large $B$. Our result holds even for matrices with large
head dimension if they have low rank. In this regime, we also give a similar
running time for Attention gradient computation, and therefore for the full LLM
training process. Furthermore, we show that any substantial improvement on our
algorithm is unlikely. In particular, we show that even when $d =
2^{\Theta(\log^* n)}$, Attention requires $n^{2 - o(1)}$ time under
$\mathsf{SETH}$.
  Finally, in the regime where $d = \mathrm{poly}(n)$, we show that the
standard algorithm is optimal under popular fine-grained complexity
assumptions.",http://arxiv.org/abs/2505.14840v1
Deep Koopman operator framework for causal discovery in nonlinear dynamical systems,"We use a deep Koopman operator-theoretic formalism to develop a novel causal
discovery algorithm, Kausal. Causal discovery aims to identify cause-effect
mechanisms for better scientific understanding, explainable decision-making,
and more accurate modeling. Standard statistical frameworks, such as Granger
causality, lack the ability to quantify causal relationships in nonlinear
dynamics due to the presence of complex feedback mechanisms, timescale mixing,
and nonstationarity. This presents a challenge in studying many real-world
systems, such as the Earth's climate. Meanwhile, Koopman operator methods have
emerged as a promising tool for approximating nonlinear dynamics in a linear
space of observables. In Kausal, we propose to leverage this powerful idea for
causal analysis where optimal observables are inferred using deep learning.
Causal estimates are then evaluated in a reproducing kernel Hilbert space, and
defined as the distance between the marginal dynamics of the effect and the
joint dynamics of the cause-effect observables. Our numerical experiments
demonstrate Kausal's superior ability in discovering and characterizing causal
signals compared to existing approaches of prescribed observables. Lastly, we
extend our analysis to observations of El Ni\~no-Southern Oscillation
highlighting our algorithm's applicability to real-world phenomena. Our code is
available at https://github.com/juannat7/kausal.",http://arxiv.org/abs/2505.14828v1
FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain,"Supervised fine-tuning (SFT) is a standard approach to adapting large
language models (LLMs) to new domains. In this work, we improve the statistical
efficiency of SFT by selecting an informative subset of training examples.
Specifically, for a fixed budget of training examples, which determines the
computational cost of fine-tuning, we determine the most informative ones. The
key idea in our method is to select examples that maximize information gain,
measured by the Hessian of the log-likelihood of the LLM. We approximate it
efficiently by linearizing the LLM at the last layer using multinomial logistic
regression models. Our approach is computationally efficient, analyzable, and
performs well empirically. We demonstrate this on several problems, and back
our claims with both quantitative results and an LLM evaluation.",http://arxiv.org/abs/2505.14826v1
Assimilative Causal Inference,"Causal inference determines cause-and-effect relationships between variables
and has broad applications across disciplines. Traditional time-series methods
often reveal causal links only in a time-averaged sense, while ensemble-based
information transfer approaches detect the time evolution of short-term causal
relationships but are typically limited to low-dimensional systems. In this
paper, a new causal inference framework, called assimilative causal inference
(ACI), is developed. Fundamentally different from the state-of-the-art methods,
ACI uses a dynamical system and a single realization of a subset of the state
variables to identify instantaneous causal relationships and the dynamic
evolution of the associated causal influence range (CIR). Instead of
quantifying how causes influence effects as done traditionally, ACI solves an
inverse problem via Bayesian data assimilation, thus tracing causes backward
from observed effects with an implicit Bayesian hypothesis. Causality is
determined by assessing whether incorporating the information of the effect
variables reduces the uncertainty in recovering the potential cause variables.
ACI has several desirable features. First, it captures the dynamic interplay of
variables, where their roles as causes and effects can shift repeatedly over
time. Second, a mathematically justified objective criterion determines the CIR
without empirical thresholds. Third, ACI is scalable to high-dimensional
problems by leveraging computationally efficient Bayesian data assimilation
techniques. Finally, ACI applies to short time series and incomplete datasets.
Notably, ACI does not require observations of candidate causes, which is a key
advantage since potential drivers are often unknown or unmeasured. The
effectiveness of ACI is demonstrated by complex dynamical systems showcasing
intermittency and extreme events.",http://arxiv.org/abs/2505.14825v1
Sample and Computationally Efficient Continuous-Time Reinforcement Learning with General Function Approximation,"Continuous-time reinforcement learning (CTRL) provides a principled framework
for sequential decision-making in environments where interactions evolve
continuously over time. Despite its empirical success, the theoretical
understanding of CTRL remains limited, especially in settings with general
function approximation. In this work, we propose a model-based CTRL algorithm
that achieves both sample and computational efficiency. Our approach leverages
optimism-based confidence sets to establish the first sample complexity
guarantee for CTRL with general function approximation, showing that a
near-optimal policy can be learned with a suboptimality gap of
$\tilde{O}(\sqrt{d_{\mathcal{R}} + d_{\mathcal{F}}}N^{-1/2})$ using $N$
measurements, where $d_{\mathcal{R}}$ and $d_{\mathcal{F}}$ denote the
distributional Eluder dimensions of the reward and dynamic functions,
respectively, capturing the complexity of general function approximation in
reinforcement learning. Moreover, we introduce structured policy updates and an
alternative measurement strategy that significantly reduce the number of policy
updates and rollouts while maintaining competitive sample efficiency. We
implemented experiments to backup our proposed algorithms on continuous control
tasks and diffusion model fine-tuning, demonstrating comparable performance
with significantly fewer policy updates and rollouts.",http://arxiv.org/abs/2505.14821v1
Imitation Learning via Focused Satisficing,"Imitation learning often assumes that demonstrations are close to optimal
according to some fixed, but unknown, cost function. However, according to
satisficing theory, humans often choose acceptable behavior based on their
personal (and potentially dynamic) levels of aspiration, rather than achieving
(near-) optimality. For example, a lunar lander demonstration that successfully
lands without crashing might be acceptable to a novice despite being slow or
jerky. Using a margin-based objective to guide deep reinforcement learning, our
focused satisficing approach to imitation learning seeks a policy that
surpasses the demonstrator's aspiration levels -- defined over trajectories or
portions of trajectories -- on unseen demonstrations without explicitly
learning those aspirations. We show experimentally that this focuses the policy
to imitate the highest quality (portions of) demonstrations better than
existing imitation learning methods, providing much higher rates of guaranteed
acceptability to the demonstrator, and competitive true returns on a range of
environments.",http://arxiv.org/abs/2505.14820v1
Out-of-Distribution Generalization of In-Context Learning: A Low-Dimensional Subspace Perspective,"This work aims to demystify the out-of-distribution (OOD) capabilities of
in-context learning (ICL) by studying linear regression tasks parameterized
with low-rank covariance matrices. With such a parameterization, we can model
distribution shifts as a varying angle between the subspace of the training and
testing covariance matrices. We prove that a single-layer linear attention
model incurs a test risk with a non-negligible dependence on the angle,
illustrating that ICL is not robust to such distribution shifts. However, using
this framework, we also prove an interesting property of ICL: when trained on
task vectors drawn from a union of low-dimensional subspaces, ICL can
generalize to any subspace within their span, given sufficiently long prompt
lengths. This suggests that the OOD generalization ability of Transformers may
actually stem from the new task lying within the span of those encountered
during training. We empirically show that our results also hold for models such
as GPT-2, and conclude with (i) experiments on how our observations extend to
nonlinear function classes and (ii) results on how LoRA has the ability to
capture distribution shifts.",http://arxiv.org/abs/2505.14808v1
Place Cells as Position Embeddings of Multi-Time Random Walk Transition Kernels for Path Planning,"The hippocampus orchestrates spatial navigation through collective place cell
encodings that form cognitive maps. We reconceptualize the population of place
cells as position embeddings approximating multi-scale symmetric random walk
transition kernels: the inner product $\langle h(x, t), h(y, t) \rangle =
q(y|x, t)$ represents normalized transition probabilities, where $h(x, t)$ is
the embedding at location $ x $, and $q(y|x, t)$ is the normalized symmetric
transition probability over time $t$. The time parameter $\sqrt{t}$ defines a
spatial scale hierarchy, mirroring the hippocampal dorsoventral axis. $q(y|x,
t)$ defines spatial adjacency between $x$ and $y$ at scale or resolution
$\sqrt{t}$, and the pairwise adjacency relationships $(q(y|x, t), \forall x,
y)$ are reduced into individual embeddings $(h(x, t), \forall x)$ that
collectively form a map of the environment at sale $\sqrt{t}$. Our framework
employs gradient ascent on $q(y|x, t) = \langle h(x, t), h(y, t)\rangle$ with
adaptive scale selection, choosing the time scale with maximal gradient at each
step for trap-free, smooth trajectories. Efficient matrix squaring $P_{2t} =
P_t^2$ builds global representations from local transitions $P_1$ without
memorizing past trajectories, enabling hippocampal preplay-like path planning.
This produces robust navigation through complex environments, aligning with
hippocampal navigation. Experimental results show that our model captures place
cell properties -- field size distribution, adaptability, and remapping --
while achieving computational efficiency. By modeling collective transition
probabilities rather than individual place fields, we offer a biologically
plausible, scalable framework for spatial navigation.",http://arxiv.org/abs/2505.14806v2
Automated Journalistic Questions: A New Method for Extracting 5W1H in French,"The 5W1H questions -- who, what, when, where, why and how -- are commonly
used in journalism to ensure that an article describes events clearly and
systematically. Answering them is a crucial prerequisites for tasks such as
summarization, clustering, and news aggregation. In this paper, we design the
first automated extraction pipeline to get 5W1H information from French news
articles. To evaluate the performance of our algo- rithm, we also create a
corpus of 250 Quebec news articles with 5W1H answers marked by four human
annotators. Our results demonstrate that our pipeline performs as well in this
task as the large language model GPT-4o.",http://arxiv.org/abs/2505.14804v1
SurvUnc: A Meta-Model Based Uncertainty Quantification Framework for Survival Analysis,"Survival analysis, which estimates the probability of event occurrence over
time from censored data, is fundamental in numerous real-world applications,
particularly in high-stakes domains such as healthcare and risk assessment.
Despite advances in numerous survival models, quantifying the uncertainty of
predictions from these models remains underexplored and challenging. The lack
of reliable uncertainty quantification limits the interpretability and
trustworthiness of survival models, hindering their adoption in clinical
decision-making and other sensitive applications. To bridge this gap, in this
work, we introduce SurvUnc, a novel meta-model based framework for post-hoc
uncertainty quantification for survival models. SurvUnc introduces an
anchor-based learning strategy that integrates concordance knowledge into
meta-model optimization, leveraging pairwise ranking performance to estimate
uncertainty effectively. Notably, our framework is model-agnostic, ensuring
compatibility with any survival model without requiring modifications to its
architecture or access to its internal parameters. Especially, we design a
comprehensive evaluation pipeline tailored to this critical yet overlooked
problem. Through extensive experiments on four publicly available benchmarking
datasets and five representative survival models, we demonstrate the
superiority of SurvUnc across multiple evaluation scenarios, including
selective prediction, misprediction detection, and out-of-domain detection. Our
results highlight the effectiveness of SurvUnc in enhancing model
interpretability and reliability, paving the way for more trustworthy survival
predictions in real-world applications.",http://arxiv.org/abs/2505.14803v1
Text embedding models can be great data engineers,"Data engineering pipelines are essential - albeit costly - components of
predictive analytics frameworks requiring significant engineering time and
domain expertise for carrying out tasks such as data ingestion, preprocessing,
feature extraction, and feature engineering. In this paper, we propose ADEPT,
an automated data engineering pipeline via text embeddings. At the core of the
ADEPT framework is a simple yet powerful idea that the entropy of embeddings
corresponding to textually dense raw format representation of time series can
be intuitively viewed as equivalent (or in many cases superior) to that of
numerically dense vector representations obtained by data engineering
pipelines. Consequently, ADEPT uses a two step approach that (i) leverages text
embeddings to represent the diverse data sources, and (ii) constructs a
variational information bottleneck criteria to mitigate entropy variance in
text embeddings of time series data. ADEPT provides an end-to-end automated
implementation of predictive models that offers superior predictive performance
despite issues such as missing data, ill-formed records, improper or corrupted
data formats and irregular timestamps. Through exhaustive experiments, we show
that the ADEPT outperforms the best existing benchmarks in a diverse set of
datasets from large-scale applications across healthcare, finance, science and
industrial internet of things. Our results show that ADEPT can potentially
leapfrog many conventional data pipeline steps thereby paving the way for
efficient and scalable automation pathways for diverse data science
applications.",http://arxiv.org/abs/2505.14802v1
KO: Kinetics-inspired Neural Optimizer with PDE Simulation Approaches,"The design of optimization algorithms for neural networks remains a critical
challenge, with most existing methods relying on heuristic adaptations of
gradient-based approaches. This paper introduces KO (Kinetics-inspired
Optimizer), a novel neural optimizer inspired by kinetic theory and partial
differential equation (PDE) simulations. We reimagine the training dynamics of
network parameters as the evolution of a particle system governed by kinetic
principles, where parameter updates are simulated via a numerical scheme for
the Boltzmann transport equation (BTE) that models stochastic particle
collisions. This physics-driven approach inherently promotes parameter
diversity during optimization, mitigating the phenomenon of parameter
condensation, i.e. collapse of network parameters into low-dimensional
subspaces, through mechanisms analogous to thermal diffusion in physical
systems. We analyze this property, establishing both a mathematical proof and a
physical interpretation. Extensive experiments on image classification
(CIFAR-10/100, ImageNet) and text classification (IMDB, Snips) tasks
demonstrate that KO consistently outperforms baseline optimizers (e.g., Adam,
SGD), achieving accuracy improvements while computation cost remains
comparable.",http://arxiv.org/abs/2505.14777v1
Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training,"Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)
have achieved impressive reasoning capabilities by selectively activating
experts to facilitate structured cognitive processes. Despite notable advances,
existing reasoning models often suffer from cognitive inefficiencies like
overthinking and underthinking. To address these limitations, we introduce a
novel inference-time steering methodology called Reinforcing Cognitive Experts
(RICE), designed to improve reasoning performance without additional training
or complex heuristics. Leveraging normalized Pointwise Mutual Information
(nPMI), we systematically identify specialized experts, termed ''cognitive
experts'' that orchestrate meta-level reasoning operations characterized by
tokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs
(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning
benchmarks demonstrate noticeable and consistent improvements in reasoning
accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our
lightweight approach substantially outperforms prevalent reasoning-steering
techniques, such as prompt design and decoding constraints, while preserving
the model's general instruction-following skills. These results highlight
reinforcing cognitive experts as a promising, practical, and interpretable
direction to enhance cognitive efficiency within advanced reasoning models.",http://arxiv.org/abs/2505.14681v1
"UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models","Lifelong learning enables large language models (LLMs) to adapt to evolving
information by continually updating their internal knowledge. An ideal system
should support efficient, wide-ranging updates while preserving existing
capabilities and ensuring reliable deployment. Model editing stands out as a
promising solution for this goal, offering a focused and efficient way to
revise a model's internal knowledge. Although recent paradigms have made
notable progress, they often struggle to meet the demands of practical lifelong
adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally
new editing solution that is training-, subject- and memory-free, making it
particularly well-suited for ultra-scalable, real-world lifelong model editing.
ULTRAEDIT performs editing through a self-contained process that relies solely
on lightweight linear algebra operations to compute parameter shifts, enabling
fast and consistent parameter modifications with minimal overhead. To improve
scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization
strategy that continuously updates feature statistics across turns, allowing it
to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT
achieves editing speeds over 7x faster than the previous state-of-the-art
method-which was also the fastest known approach-while consuming less than 1/3
the VRAM, making it the only method currently capable of editing a 7B LLM on a
24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest
dataset in the field to date, with over 2M editing pairs-and demonstrate that
our method supports up to 1M edits while maintaining high accuracy.
Comprehensive experiments on four datasets and six models show that ULTRAEDIT
consistently achieves superior performance across diverse model editing
scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.",http://arxiv.org/abs/2505.14679v1
Quantum Optimization via Gradient-Based Hamiltonian Descent,"With rapid advancements in machine learning, first-order algorithms have
emerged as the backbone of modern optimization techniques, owing to their
computational efficiency and low memory requirements. Recently, the connection
between accelerated gradient methods and damped heavy-ball motion, particularly
within the framework of Hamiltonian dynamics, has inspired the development of
innovative quantum algorithms for continuous optimization. One such algorithm,
Quantum Hamiltonian Descent (QHD), leverages quantum tunneling to escape saddle
points and local minima, facilitating the discovery of global solutions in
complex optimization landscapes. However, QHD faces several challenges,
including slower convergence rates compared to classical gradient methods and
limited robustness in highly non-convex problems due to the non-local nature of
quantum states. Furthermore, the original QHD formulation primarily relies on
function value information, which limits its effectiveness. Inspired by
insights from high-resolution differential equations that have elucidated the
acceleration mechanisms in classical methods, we propose an enhancement to QHD
by incorporating gradient information, leading to what we call gradient-based
QHD. Gradient-based QHD achieves faster convergence and significantly increases
the likelihood of identifying global solutions. Numerical simulations on
challenging problem instances demonstrate that gradient-based QHD outperforms
existing quantum and classical methods by at least an order of magnitude.",http://arxiv.org/abs/2505.14670v1
Quartet: Native FP4 Training Can Be Optimal for Large Language Models,"The rapid advancement of large language models (LLMs) has been paralleled by
unprecedented increases in computational demands, with training costs for
state-of-the-art models doubling every few months. Training models directly in
low-precision arithmetic offers a solution, by improving both computational
throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell
architecture facilitates extremely low-precision operations, specifically FP4
variants, promising substantial efficiency gains. Yet, current algorithms for
training LLMs in FP4 precision face significant accuracy degradation and often
rely on mixed-precision fallbacks. In this paper, we systematically investigate
hardware-supported FP4 training and introduce Quartet, a new approach enabling
accurate, end-to-end FP4 training with all the major computations (in e.g.
linear layers) being performed in low precision. Through extensive evaluations
on Llama-type models, we reveal a new low-precision scaling law that quantifies
performance trade-offs across varying bit-widths and allows us to identify a
""near-optimal"" low-precision training technique in terms of
accuracy-vs-computation, called Quartet. We implement Quartet using optimized
CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve
state-of-the-art accuracy for FP4 precision, successfully training
billion-scale models. Our method demonstrates that fully FP4-based training is
a competitive alternative to standard-precision and FP8 training. Our code is
available at https://github.com/IST-DASLab/Quartet.",http://arxiv.org/abs/2505.14669v1
AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings,"Cross-modal embeddings form the foundation for multi-modal models. However,
visualization methods for interpreting cross-modal embeddings have been
primarily confined to traditional dimensionality reduction (DR) techniques like
PCA and t-SNE. These DR methods primarily focus on feature distributions within
a single modality, whilst failing to incorporate metrics (e.g., CLIPScore)
across multiple modalities.This paper introduces AKRMap, a new DR technique
designed to visualize cross-modal embeddings metric with enhanced accuracy by
learning kernel regression of the metric landscape in the projection space.
Specifically, AKRMap constructs a supervised projection network guided by a
post-projection kernel regression loss, and employs adaptive generalized
kernels that can be jointly optimized with the projection. This approach
enables AKRMap to efficiently generate visualizations that capture complex
metric distributions, while also supporting interactive features such as zoom
and overlay for deeper exploration. Quantitative experiments demonstrate that
AKRMap outperforms existing DR methods in generating more accurate and
trustworthy visualizations. We further showcase the effectiveness of AKRMap in
visualizing and comparing cross-modal embeddings for text-to-image models. Code
and demo are available at https://github.com/yilinye/AKRMap.",http://arxiv.org/abs/2505.14664v1
This Time is Different: An Observability Perspective on Time Series Foundation Models,"We introduce Toto, a time series forecasting foundation model with 151
million parameters. Toto uses a modern decoder-only architecture coupled with
architectural innovations designed to account for specific challenges found in
multivariate observability time series data. Toto's pre-training corpus is a
mixture of observability data, open datasets, and synthetic data, and is
4-10$\times$ larger than those of leading time series foundation models.
Additionally, we introduce BOOM, a large-scale benchmark consisting of 350
million observations across 2,807 real-world time series. For both Toto and
BOOM, we source observability data exclusively from Datadog's own telemetry and
internal observability metrics. Extensive evaluations demonstrate that Toto
achieves state-of-the-art performance on both BOOM and on established general
purpose time series forecasting benchmarks. Toto's model weights, inference
code, and evaluation scripts, as well as BOOM's data and evaluation code, are
all available as open source under the Apache 2.0 License available at
https://huggingface.co/Datadog/Toto-Open-Base-1.0 and
https://github.com/DataDog/toto.",http://arxiv.org/abs/2505.14766v1
Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless Networks,"As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.",http://arxiv.org/abs/2505.14659v1
Deep Learning-Based Forecasting of Boarding Patient Counts to Address ED Overcrowding,"This study develops deep learning models to forecast the number of patients
in the emergency department (ED) boarding phase six hours in advance, aiming to
support proactive operational decision-making using only non-clinical,
operational, and contextual features. Data were collected from five sources: ED
tracking systems, inpatient census records, weather reports, federal holiday
calendars, and local event schedules. After feature engineering, the data were
aggregated at an hourly level, cleaned, and merged into a unified dataset for
model training. Several time series deep learning models, including ResNetPlus,
TSTPlus, TSiTPlus (from the tsai library), and N-BEATSx, were trained using
Optuna and grid search for hyperparameter tuning. The average ED boarding count
was 28.7, with a standard deviation of 11.2. N-BEATSx achieved the best
performance, with a mean absolute error of 2.10, mean squared error of 7.08,
root mean squared error of 2.66, and a coefficient of determination of 0.95.
The model maintained stable accuracy even during periods of extremely high
boarding counts, defined as values exceeding one, two, or three standard
deviations above the mean. Results show that accurate six-hour-ahead forecasts
are achievable without using patient-level clinical data. While strong
performance was observed even with a basic feature set, the inclusion of
additional features improved prediction stability under extreme conditions.
This framework offers a practical and generalizable approach for hospital
systems to anticipate boarding levels and help mitigate ED overcrowding.",http://arxiv.org/abs/2505.14765v1
Sequential QCQP for Bilevel Optimization with Line Search,"Bilevel optimization involves a hierarchical structure where one problem is
nested within another, leading to complex interdependencies between levels. We
propose a single-loop, tuning-free algorithm that guarantees anytime
feasibility, i.e., approximate satisfaction of the lower-level optimality
condition, while ensuring descent of the upper-level objective. At each
iteration, a convex quadratically-constrained quadratic program (QCQP) with a
closed-form solution yields the search direction, followed by a backtracking
line search inspired by control barrier functions to ensure safe, uniformly
positive step sizes. The resulting method is scalable, requires no
hyperparameter tuning, and converges under mild local regularity assumptions.
We establish an O(1/k) ergodic convergence rate and demonstrate the algorithm's
effectiveness on representative bilevel tasks.",http://arxiv.org/abs/2505.14647v1
Early Diagnosis of Atrial Fibrillation Recurrence: A Large Tabular Model Approach with Structured and Unstructured Clinical Data,"BACKGROUND: Atrial fibrillation (AF), the most common arrhythmia, is linked
to high morbidity and mortality. In a fast-evolving AF rhythm control treatment
era, predicting AF recurrence after its onset may be crucial to achieve the
optimal therapeutic approach, yet traditional scores like CHADS2-VASc, HATCH,
and APPLE show limited predictive accuracy. Moreover, early diagnosis studies
often rely on codified electronic health record (EHR) data, which may contain
errors and missing information.
  OBJECTIVE: This study aims to predict AF recurrence between one month and two
years after onset by evaluating traditional clinical scores, ML models, and our
LTM approach. Moreover, another objective is to develop a methodology for
integrating structured and unstructured data to enhance tabular dataset
quality.
  METHODS: A tabular dataset was generated by combining structured clinical
data with free-text discharge reports processed through natural language
processing techniques, reducing errors and annotation effort. A total of 1,508
patients with documented AF onset were identified, and models were evaluated on
a manually annotated test set. The proposed approach includes a LTM compared
against traditional clinical scores and ML models.
  RESULTS: The proposed LTM approach achieved the highest predictive
performance, surpassing both traditional clinical scores and ML models.
Additionally, the gender and age bias analyses revealed demographic
disparities.
  CONCLUSION: The integration of structured data and free-text sources resulted
in a high-quality dataset. The findings emphasize the limitations of
traditional clinical scores in predicting AF recurrence and highlight the
potential of ML-based approaches, particularly our LTM model.",http://arxiv.org/abs/2505.14643v1
Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference,"Deep neural networks have achieved state-of-the-art results in a wide range
of applications, from natural language processing and computer vision to speech
recognition. However, as tasks become increasingly complex, model sizes
continue to grow, posing challenges in latency and memory efficiency. To meet
these constraints, post-training quantization has emerged as a promising
solution. In this paper, we propose a novel hardware-efficient quantization and
inference scheme that exploits hardware advantages with minimal accuracy
degradation. Specifically, we introduce a W4A8 scheme, where weights are
quantized and stored using 4-bit integer precision, and inference computations
are performed using 8-bit floating-point arithmetic, demonstrating significant
speedups and improved memory utilization compared to 16-bit operations,
applicable on various modern accelerators. To mitigate accuracy loss, we
develop a novel quantization algorithm, dubbed Dual Precision Quantization
(DPQ), that leverages the unique structure of our scheme without introducing
additional inference overhead. Experimental results demonstrate improved
performance (i.e., increased throughput) while maintaining tolerable accuracy
degradation relative to the full-precision model.",http://arxiv.org/abs/2505.14638v1
Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep Learning,"We present the first theoretical framework that connects predictive coding
(PC), a biologically inspired local learning rule, with the minimum description
length (MDL) principle in deep networks. We prove that layerwise PC performs
block-coordinate descent on the MDL two-part code objective, thereby jointly
minimizing empirical risk and model complexity. Using Hoeffding's inequality
and a prefix-code prior, we derive a novel generalization bound of the form
$R(\theta) \le \hat{R}(\theta) + \frac{L(\theta)}{N}$, capturing the tradeoff
between fit and compression. We further prove that each PC sweep monotonically
decreases the empirical two-part codelength, yielding tighter high-probability
risk bounds than unconstrained gradient descent. Finally, we show that repeated
PC updates converge to a block-coordinate stationary point, providing an
approximate MDL-optimal solution. To our knowledge, this is the first result
offering formal generalization and convergence guarantees for PC-trained deep
models, positioning PC as a theoretically grounded and biologically plausible
alternative to backpropagation.",http://arxiv.org/abs/2505.14635v1
Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas,"Detecting AI risks becomes more challenging as stronger models emerge and
find novel methods such as Alignment Faking to circumvent these detection
attempts. Inspired by how risky behaviors in humans (i.e., illegal activities
that may hurt others) are sometimes guided by strongly-held values, we believe
that identifying values within AI models can be an early warning system for
AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal
AI models' priorities on a range of AI value classes. Then, we collect
AIRiskDilemmas, a diverse collection of dilemmas that pit values against one
another in scenarios relevant to AI safety risks such as Power Seeking. By
measuring an AI model's value prioritization using its aggregate choices, we
obtain a self-consistent set of predicted value priorities that uncover
potential risks. We show that values in LitmusValues (including seemingly
innocuous ones like Care) can predict for both seen risky behaviors in
AIRiskDilemmas and unseen risky behaviors in HarmBench.",http://arxiv.org/abs/2505.14633v1
KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models,"Recent advances in large language models (LLMs) and the abundance of food
data have resulted in studies to improve food understanding using LLMs. Despite
several recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there
has been limited research on integrating food related KGs with LLMs. We
introduce KERL, a unified system that leverages food KGs and LLMs to provide
personalized food recommendations and generates recipes with associated
micro-nutritional information. Given a natural language question, KERL extracts
entities, retrieves subgraphs from the KG, which are then fed into the LLM as
context to select the recipes that satisfy the constraints. Next, our system
generates the cooking steps and nutritional information for each recipe. To
evaluate our approach, we also develop a benchmark dataset by curating recipe
related questions, combined with constraints and personal preferences. Through
extensive experiments, we show that our proposed KG-augmented LLM significantly
outperforms existing approaches, offering a complete and coherent solution for
food recommendation, recipe generation, and nutritional analysis. Our code and
benchmark datasets are publicly available at
https://github.com/mohbattharani/KERL.",http://arxiv.org/abs/2505.14629v1
TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning,"Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.",http://arxiv.org/abs/2505.14625v2
3D Reconstruction from Sketches,"We consider the problem of reconstructing a 3D scene from multiple sketches.
We propose a pipeline which involves (1) stitching together multiple sketches
through use of correspondence points, (2) converting the stitched sketch into a
realistic image using a CycleGAN, and (3) estimating that image's depth-map
using a pre-trained convolutional neural network based architecture called
MegaDepth. Our contribution includes constructing a dataset of image-sketch
pairs, the images for which are from the Zurich Building Database, and sketches
have been generated by us. We use this dataset to train a CycleGAN for our
pipeline's second step. We end up with a stitching process that does not
generalize well to real drawings, but the rest of the pipeline that creates a
3D reconstruction from a single sketch performs quite well on a wide variety of
drawings.",http://arxiv.org/abs/2505.14621v1
Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs,"Huawei Cloud users leverage LoRA (Low-Rank Adaptation) as an efficient and
scalable method to fine-tune and customize large language models (LLMs) for
application-specific needs. However, tasks that require complex reasoning or
deep contextual understanding are often hindered by biases or interference from
the base model when using typical decoding methods like greedy or beam search.
These biases can lead to generic or task-agnostic responses from the base model
instead of leveraging the LoRA-specific adaptations. In this paper, we
introduce Contrastive LoRA Decoding (CoLD), a novel decoding framework designed
to maximize the use of task-specific knowledge in LoRA-adapted models,
resulting in better downstream performance. CoLD uses contrastive decoding by
scoring candidate tokens based on the divergence between the probability
distributions of a LoRA-adapted expert model and the corresponding base model.
This approach prioritizes tokens that better align with the LoRA's learned
representations, enhancing performance for specialized tasks. While effective,
a naive implementation of CoLD is computationally expensive because each
decoding step requires evaluating multiple token candidates across both models.
To address this, we developed an optimized kernel for Huawei's Ascend NPU. CoLD
achieves up to a 5.54% increase in task accuracy while reducing end-to-end
latency by 28% compared to greedy decoding. This work provides practical and
efficient decoding strategies for fine-tuned LLMs in resource-constrained
environments and has broad implications for applied data science in both cloud
and on-premises settings.",http://arxiv.org/abs/2505.14620v1
SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas,"We introduce SATBench, a benchmark for evaluating the logical reasoning
capabilities of large language models (LLMs) through logical puzzles derived
from Boolean satisfiability (SAT) problems. Unlike prior work that focuses on
inference rule-based reasoning, which often involves deducing conclusions from
a set of premises, our approach leverages the search-based nature of SAT
problems, where the objective is to find a solution that fulfills a specified
set of logical constraints. Each instance in SATBench is generated from a SAT
formula, then translated into a story context and conditions using LLMs. The
generation process is fully automated and allows for adjustable difficulty by
varying the number of clauses. All 2100 puzzles are validated through both
LLM-assisted and solver-based consistency checks, with human validation on a
subset. Experimental results show that even the strongest model, o4-mini,
achieves only 65.0% accuracy on hard UNSAT problems, close to the random
baseline of 50%. SATBench exposes fundamental limitations in the search-based
logical reasoning abilities of current LLMs and provides a scalable testbed for
future research in logical reasoning.",http://arxiv.org/abs/2505.14615v1
"Virtual Cells: Predict, Explain, Discover","Drug discovery is fundamentally a process of inferring the effects of
treatments on patients, and would therefore benefit immensely from
computational models that can reliably simulate patient responses, enabling
researchers to generate and test large numbers of therapeutic hypotheses safely
and economically before initiating costly clinical trials. Even a more specific
model that predicts the functional response of cells to a wide range of
perturbations would be tremendously valuable for discovering safe and effective
treatments that successfully translate to the clinic. Creating such virtual
cells has long been a goal of the computational research community that
unfortunately remains unachieved given the daunting complexity and scale of
cellular biology. Nevertheless, recent advances in AI, computing power, lab
automation, and high-throughput cellular profiling provide new opportunities
for reaching this goal. In this perspective, we present a vision for developing
and evaluating virtual cells that builds on our experience at Recursion. We
argue that in order to be a useful tool to discover novel biology, virtual
cells must accurately predict the functional response of a cell to
perturbations and explain how the predicted response is a consequence of
modifications to key biomolecular interactions. We then introduce key
principles for designing therapeutically-relevant virtual cells, describe a
lab-in-the-loop approach for generating novel insights with them, and advocate
for biologically-grounded benchmarks to guide virtual cell development.
Finally, we make the case that our approach to virtual cells provides a useful
framework for building other models at higher levels of organization, including
virtual patients. We hope that these directions prove useful to the research
community in developing virtual models optimized for positive impact on drug
discovery outcomes.",http://arxiv.org/abs/2505.14613v1
MMD-Newton Method for Multi-objective Optimization,"Maximum mean discrepancy (MMD) has been widely employed to measure the
distance between probability distributions. In this paper, we propose using MMD
to solve continuous multi-objective optimization problems (MOPs). For solving
MOPs, a common approach is to minimize the distance (e.g., Hausdorff) between a
finite approximate set of the Pareto front and a reference set. Viewing these
two sets as empirical measures, we propose using MMD to measure the distance
between them. To minimize the MMD value, we provide the analytical expression
of its gradient and Hessian matrix w.r.t. the search variables, and use them to
devise a novel set-oriented, MMD-based Newton (MMDN) method. Also, we analyze
the theoretical properties of MMD's gradient and Hessian, including the
first-order stationary condition and the eigenspectrum of the Hessian, which
are important for verifying the correctness of MMDN. To solve complicated
problems, we propose hybridizing MMDN with multiobjective evolutionary
algorithms (MOEAs), where we first execute an EA for several iterations to get
close to the global Pareto front and then warm-start MMDN with the result of
the MOEA to efficiently refine the approximation. We empirically test the
hybrid algorithm on 11 widely used benchmark problems, and the results show the
hybrid (MMDN + MOEA) can achieve a much better optimization accuracy than EA
alone with the same computation budget.",http://arxiv.org/abs/2505.14610v1
Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It),"Despite considerable progress in the development of machine-text detectors,
it has been suggested that the problem is inherently hard, and therefore, that
stakeholders should proceed under the assumption that machine-generated text
cannot be reliably detected as such. We examine a recent such claim by Nicks et
al. (2024) regarding the ease with which language models can be optimized to
degrade the performance of machine-text detectors, including detectors not
specifically optimized against. We identify a feature space$\unicode{x2013}$the
stylistic feature space$\unicode{x2013}$that is robust to such optimization,
and show that it may be used to reliably detect samples from language models
optimized to prevent detection. Furthermore, we show that even when models are
explicitly optimized against stylistic detectors, detection performance remains
surprisingly unaffected. We then seek to understand if stylistic detectors are
inherently more robust. To study this question, we explore a new paraphrasing
approach that simultaneously aims to close the gap between human writing and
machine writing in stylistic feature space while avoiding detection using
traditional features. We show that when only a single sample is available for
detection, this attack is universally effective across all detectors
considered, including those that use writing style. However, as the number of
samples available for detection grows, the human and machine distributions
become distinguishable. This observation encourages us to introduce AURA, a
metric that estimates the overlap between human and machine-generated
distributions by analyzing how detector performance improves as more samples
become available. Overall, our findings underscore previous recommendations to
avoid reliance on machine-text detection.",http://arxiv.org/abs/2505.14608v1
Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic Potentials,"Recent advances in neural network interatomic potentials have emerged as a
promising research direction. However, popular deep learning models often lack
auxiliary constraints grounded in physical laws, which could accelerate
training and improve fidelity through physics-based regularization. In this
work, we introduce $\Phi$-Module, a universal plugin module that enforces
Poisson's equation within the message-passing framework to learn electrostatic
interactions in a self-supervised manner. Specifically, each atom-wise
representation is encouraged to satisfy a discretized Poisson's equation,
making it possible to acquire a potential $\boldsymbol{\phi}$ and a
corresponding charge density $\boldsymbol{\rho}$ linked to the learnable
Laplacian eigenbasis coefficients of a given molecular graph. We then derive an
electrostatic energy term, crucial for improved total energy predictions. This
approach integrates seamlessly into any existing neural potential with
insignificant computational overhead. Experiments on the OE62 and MD22
benchmarks confirm that models combined with $\Phi$-Module achieve robust
improvements over baseline counterparts. For OE62 error reduction ranges from
4.5\% to 17.8\%, and for MD22, baseline equipped with $\Phi$-Module achieves
best results on 5 out of 14 cases. Our results underscore how embedding a
first-principles constraint in neural interatomic potentials can significantly
improve performance while remaining hyperparameter-friendly, memory-efficient
and lightweight in training. Code will be available at
\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.",http://arxiv.org/abs/2505.14606v1
Towards a Foundation Model for Communication Systems,"Artificial Intelligence (AI) has demonstrated unprecedented performance
across various domains, and its application to communication systems is an
active area of research. While current methods focus on task-specific
solutions, the broader trend in AI is shifting toward large general models
capable of supporting multiple applications. In this work, we take a step
toward a foundation model for communication data--a transformer-based,
multi-modal model designed to operate directly on communication data. We
propose methodologies to address key challenges, including tokenization,
positional embedding, multimodality, variable feature sizes, and normalization.
Furthermore, we empirically demonstrate that such a model can successfully
estimate multiple features, including transmission rank, selected precoder,
Doppler spread, and delay profile.",http://arxiv.org/abs/2505.14603v1
CSTS: A Benchmark for the Discovery of Correlation Structures in Time Series Clustering,"Time series clustering promises to uncover hidden structural patterns in data
with applications across healthcare, finance, industrial systems, and other
critical domains. However, without validated ground truth information,
researchers cannot objectively assess clustering quality or determine whether
poor results stem from absent structures in the data, algorithmic limitations,
or inappropriate validation methods, raising the question whether clustering is
""more art than science"" (Guyon et al., 2009). To address these challenges, we
introduce CSTS (Correlation Structures in Time Series), a synthetic benchmark
for evaluating the discovery of correlation structures in multivariate time
series data. CSTS provides a clean benchmark that enables researchers to
isolate and identify specific causes of clustering failures by differentiating
between correlation structure deterioration and limitations of clustering
algorithms and validation methods. Our contributions are: (1) a comprehensive
benchmark for correlation structure discovery with distinct correlation
structures, systematically varied data conditions, established performance
thresholds, and recommended evaluation protocols; (2) empirical validation of
correlation structure preservation showing moderate distortion from
downsampling and minimal effects from distribution shifts and sparsification;
and (3) an extensible data generation framework enabling structure-first
clustering evaluation. A case study demonstrates CSTS's practical utility by
identifying an algorithm's previously undocumented sensitivity to non-normal
distributions, illustrating how the benchmark enables precise diagnosis of
methodological limitations. CSTS advances rigorous evaluation standards for
correlation-based time series clustering.",http://arxiv.org/abs/2505.14596v1
Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers,"Reduced-order modeling (ROM) of time-dependent and parameterized differential
equations aims to accelerate the simulation of complex high-dimensional systems
by learning a compact latent manifold representation that captures the
characteristics of the solution fields and their time-dependent dynamics.
Although high-fidelity numerical solvers generate the training datasets, they
have thus far been excluded from the training process, causing the learned
latent dynamics to drift away from the discretized governing physics. This
mismatch often limits generalization and forecasting capabilities. In this
work, we propose Physics-informed ROM ($\Phi$-ROM) by incorporating
differentiable PDE solvers into the training procedure. Specifically, the
latent space dynamics and its dependence on PDE parameters are shaped directly
by the governing physics encoded in the solver, ensuring a strong
correspondence between the full and reduced systems. Our model outperforms
state-of-the-art data-driven ROMs and other physics-informed strategies by
accurately generalizing to new dynamics arising from unseen parameters,
enabling long-term forecasting beyond the training horizon, maintaining
continuity in both time and space, and reducing the data cost. Furthermore,
$\Phi$-ROM learns to recover and forecast the solution fields even when trained
or evaluated with sparse and irregular observations of the fields, providing a
flexible framework for field reconstruction and data assimilation. We
demonstrate the framework's robustness across different PDE solvers and
highlight its broad applicability by providing an open-source JAX
implementation readily extensible to other PDE systems and differentiable
solvers.",http://arxiv.org/abs/2505.14595v1
Adaptive Pruning of Deep Neural Networks for Resource-Aware Embedded Intrusion Detection on the Edge,"Artificial neural network pruning is a method in which artificial neural
network sizes can be reduced while attempting to preserve the predicting
capabilities of the network. This is done to make the model smaller or faster
during inference time. In this work we analyze the ability of a selection of
artificial neural network pruning methods to generalize to a new cybersecurity
dataset utilizing a simpler network type than was designed for. We analyze each
method using a variety of pruning degrees to best understand how each algorithm
responds to the new environment. This has allowed us to determine the most well
fit pruning method of those we searched for the task. Unexpectedly, we have
found that many of them do not generalize to the problem well, leaving only a
few algorithms working to an acceptable degree.",http://arxiv.org/abs/2505.14592v1
High-Dimensional Analysis of Bootstrap Ensemble Classifiers,"Bootstrap methods have long been a cornerstone of ensemble learning in
machine learning. This paper presents a theoretical analysis of bootstrap
techniques applied to the Least Square Support Vector Machine (LSSVM) ensemble
in the context of large and growing sample sizes and feature dimensionalities.
Leveraging tools from Random Matrix Theory, we investigate the performance of
this classifier that aggregates decision functions from multiple weak
classifiers, each trained on different subsets of the data. We provide insights
into the use of bootstrap methods in high-dimensional settings, enhancing our
understanding of their impact. Based on these findings, we propose strategies
to select the number of subsets and the regularization parameter that maximize
the performance of the LSSVM. Empirical experiments on synthetic and real-world
datasets validate our theoretical results.",http://arxiv.org/abs/2505.14587v1
Instance Segmentation for Point Sets,"Recently proposed neural network architectures like PointNet [QSMG16] and
PointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point
sets. The feature representations of shapes learned by these two networks
enabled training classifiers for Semantic Segmentation, and more recently for
Instance Segmentation via the Similarity Group Proposal Network (SGPN)
[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,
pertains to use of memory intensive similarity matrices which occupy memory
quadratic in the number of points. In this report, we attempt to tackle this
issue through use of two sampling based methods, which compute Instance
Segmentation on a sub-sampled Point Set, and then extrapolate labels to the
complete set using the nearest neigbhour approach. While both approaches
perform equally well on large sub-samples, the random-based strategy gives the
most improvements in terms of speed and memory usage.",http://arxiv.org/abs/2505.14583v1
LEANCODE: Understanding Models Better for Code Simplification of Pre-trained Large Language Models,"Large Language Models for code often entail significant computational
complexity, which grows significantly with the length of the input code
sequence. We propose LeanCode for code simplification to reduce training and
prediction time, leveraging code contexts in utilizing attention scores to
represent the tokens' importance. We advocate for the selective removal of
tokens based on the average context-aware attention scores rather than average
scores across all inputs. LeanCode uses the attention scores of `CLS' tokens
within the encoder for classification tasks, such as code search. It also
employs the encoder-decoder attention scores to determine token significance
for sequence-to-sequence tasks like code summarization. Our evaluation shows
LeanCode's superiority over the SOTAs DietCode and Slimcode, with improvements
of 60% and 16% for code search, and 29% and 27% for code summarization,
respectively.",http://arxiv.org/abs/2505.14759v2
Performance Optimization of Energy-Harvesting Underlay Cognitive Radio Networks Using Reinforcement Learning,"In this paper, a reinforcement learning technique is employed to maximize the
performance of a cognitive radio network (CRN). In the presence of primary
users (PUs), it is presumed that two secondary users (SUs) access the licensed
band within underlay mode. In addition, the SU transmitter is assumed to be an
energy-constrained device that requires harvesting energy in order to transmit
signals to their intended destination. Therefore, we propose that there are two
main sources of energy; the interference of PUs' transmissions and ambient
radio frequency (RF) sources. The SU will select whether to gather energy from
PUs or only from ambient sources based on a predetermined threshold. The
process of energy harvesting from the PUs' messages is accomplished via the
time switching approach. In addition, based on a deep Q-network (DQN) approach,
the SU transmitter determines whether to collect energy or transmit messages
during each time slot as well as selects the suitable transmission power in
order to maximize its average data rate. Our approach outperforms a baseline
strategy and converges, as shown by our findings.",http://arxiv.org/abs/2505.14581v1
Inference with correlated priors using sisters cells,"A common view of sensory processing is as probabilistic inference of latent
causes from receptor activations. Standard approaches often assume these causes
are a priori independent, yet real-world generative factors are typically
correlated. Representing such structured priors in neural systems poses
architectural challenges, particularly when direct interactions between units
representing latent causes are biologically implausible or computationally
expensive. Inspired by the architecture of the olfactory bulb, we propose a
novel circuit motif that enables inference with correlated priors without
requiring direct interactions among latent cause units. The key insight lies in
using sister cells: neurons receiving shared receptor input but connected
differently to local interneurons. The required interactions among latent units
are implemented indirectly through their connections to the sister cells, such
that correlated connectivity implies anti-correlation in the prior and vice
versa. We use geometric arguments to construct connectivity that implements a
given prior and to bound the number of causes for which such priors can be
constructed. Using simulations, we demonstrate the efficacy of such priors for
inference in noisy environments and compare the inference dynamics to those
experimentally observed. Finally, we show how, under certain assumptions on
latent representations, the prior used can be inferred from sister cell
activations. While biologically grounded in the olfactory system, our mechanism
generalises to other natural and artificial sensory systems and may inform the
design of architectures for efficient inference under correlated latent
structure.",http://arxiv.org/abs/2505.14579v1
Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images,"The International Society of Ultrasound advocates Intrapartum Ultrasound (US)
Imaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression
through changes in fetal head position. Two reliable ultrasound-derived
parameters that are used to predict outcomes of instrumental vaginal delivery
are the angle of progression (AoP) and head-symphysis distance (HSD). In this
work, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we
propose an automated fetal biometry measurement pipeline to reduce intra- and
inter-observer variability and improve measurement reliability. Our pipeline
consists of three key tasks: (i) classification of standard planes (SP) from US
videos, (ii) segmentation of fetal head and pubic symphysis from the detected
SPs, and (iii) computation of the AoP and HSD from the segmented regions. We
perform sparse sampling to mitigate class imbalances and reduce spurious
correlations in task (i), and utilize ensemble-based deep learning methods for
task (i) and (ii) to enhance generalizability under different US acquisition
settings. Finally, to promote robustness in task iii) with respect to the
structural fidelity of measurements, we retain the largest connected components
and apply ellipse fitting to the segmentations. Our solution achieved ACC:
0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,
$\Delta_{AoP}$: 8.90 and $\Delta_{HSD}$: 14.35 across an unseen hold-out set of
4 patients and 224 US frames. The results from the proposed automated pipeline
can improve the understanding of labour arrest causes and guide the development
of clinical risk stratification tools for efficient and effective prenatal
care.",http://arxiv.org/abs/2505.14572v1
Agent Context Protocols Enhance Collective Inference,"AI agents have become increasingly adept at complex tasks such as coding,
reasoning, and multimodal understanding. However, building generalist systems
requires moving beyond individual agents to collective inference -- a paradigm
where multi-agent systems with diverse, task-specialized agents complement one
another through structured communication and collaboration. Today, coordination
is usually handled with imprecise, ad-hoc natural language, which limits
complex interaction and hinders interoperability with domain-specific agents.
We introduce Agent context protocols (ACPs): a domain- and agent-agnostic
family of structured protocols for agent-agent communication, coordination, and
error handling. ACPs combine (i) persistent execution blueprints -- explicit
dependency graphs that store intermediate agent outputs -- with (ii)
standardized message schemas, enabling robust and fault-tolerant multi-agent
collective inference. ACP-powered generalist systems reach state-of-the-art
performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance
and best-in-class multimodal technical reports, outperforming commercial AI
systems in human evaluation. ACPs are highly modular and extensible, allowing
practitioners to build top-tier generalist agents quickly.",http://arxiv.org/abs/2505.14569v1
KIPPO: Koopman-Inspired Proximal Policy Optimization,"Reinforcement Learning (RL) has made significant strides in various domains,
and policy gradient methods like Proximal Policy Optimization (PPO) have gained
popularity due to their balance in performance, training stability, and
computational efficiency. These methods directly optimize policies through
gradient-based updates. However, developing effective control policies for
environments with complex and non-linear dynamics remains a challenge. High
variance in gradient estimates and non-convex optimization landscapes often
lead to unstable learning trajectories. Koopman Operator Theory has emerged as
a powerful framework for studying non-linear systems through an
infinite-dimensional linear operator that acts on a higher-dimensional space of
measurement functions. In contrast with their non-linear counterparts, linear
systems are simpler, more predictable, and easier to analyze. In this paper, we
present Koopman-Inspired Proximal Policy Optimization (KIPPO), which learns an
approximately linear latent-space representation of the underlying system's
dynamics while retaining essential features for effective policy learning. This
is achieved through a Koopman-approximation auxiliary network that can be added
to the baseline policy optimization algorithms without altering the
architecture of the core policy or value function. Extensive experimental
results demonstrate consistent improvements over the PPO baseline with 6-60%
increased performance while reducing variability by up to 91% when evaluated on
various continuous control tasks.",http://arxiv.org/abs/2505.14566v1
Bellman operator convergence enhancements in reinforcement learning algorithms,"This paper reviews the topological groundwork for the study of reinforcement
learning (RL) by focusing on the structure of state, action, and policy spaces.
We begin by recalling key mathematical concepts such as complete metric spaces,
which form the foundation for expressing RL problems. By leveraging the Banach
contraction principle, we illustrate how the Banach fixed-point theorem
explains the convergence of RL algorithms and how Bellman operators, expressed
as operators on Banach spaces, ensure this convergence. The work serves as a
bridge between theoretical mathematics and practical algorithm design, offering
new approaches to enhance the efficiency of RL. In particular, we investigate
alternative formulations of Bellman operators and demonstrate their impact on
improving convergence rates and performance in standard RL environments such as
MountainCar, CartPole, and Acrobot. Our findings highlight how a deeper
mathematical understanding of RL can lead to more effective algorithms for
decision-making problems.",http://arxiv.org/abs/2505.14564v1
SSPS: Self-Supervised Positive Sampling for Robust Self-Supervised Speaker Verification,"Self-Supervised Learning (SSL) has led to considerable progress in Speaker
Verification (SV). The standard framework uses same-utterance positive sampling
and data-augmentation to generate anchor-positive pairs of the same speaker.
This is a major limitation, as this strategy primarily encodes channel
information from the recording condition, shared by the anchor and positive. We
propose a new positive sampling technique to address this bottleneck:
Self-Supervised Positive Sampling (SSPS). For a given anchor, SSPS aims to find
an appropriate positive, i.e., of the same speaker identity but a different
recording condition, in the latent space using clustering assignments and a
memory queue of positive embeddings. SSPS improves SV performance for both
SimCLR and DINO, reaching 2.57% and 2.53% EER, outperforming SOTA SSL methods
on VoxCeleb1-O. In particular, SimCLR-SSPS achieves a 58% EER reduction by
lowering intra-speaker variance, providing comparable performance to DINO-SSPS.",http://arxiv.org/abs/2505.14561v1
Physics-Guided Learning of Meteorological Dynamics for Weather Downscaling and Forecasting,"Weather forecasting is essential but remains computationally intensive and
physically incomplete in traditional numerical weather prediction (NWP)
methods. Deep learning (DL) models offer efficiency and accuracy but often
ignore physical laws, limiting interpretability and generalization. We propose
PhyDL-NWP, a physics-guided deep learning framework that integrates physical
equations with latent force parameterization into data-driven models. It
predicts weather variables from arbitrary spatiotemporal coordinates, computes
physical terms via automatic differentiation, and uses a physics-informed loss
to align predictions with governing dynamics. PhyDL-NWP enables resolution-free
downscaling by modeling weather as a continuous function and fine-tunes
pre-trained models with minimal overhead, achieving up to 170x faster inference
with only 55K parameters. Experiments show that PhyDL-NWP improves both
forecasting performance and physical consistency.",http://arxiv.org/abs/2505.14555v1
Pivot Language for Low-Resource Machine Translation,"Certain pairs of languages suffer from lack of a parallel corpus which is
large in size and diverse in domain. One of the ways this is overcome is via
use of a pivot language. In this paper we use Hindi as a pivot language to
translate Nepali into English. We describe what makes Hindi a good candidate
for the pivot. We discuss ways in which a pivot language can be used, and use
two such approaches - the Transfer Method (fully supervised) and
Backtranslation (semi-supervised) - to translate Nepali into English. Using the
former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which
improves the baseline fully supervised score reported by (Guzman et al., 2019)
by 6.6 points. While we are slightly below the semi-supervised baseline score
of 15.1, we discuss what may have caused this under-performance, and suggest
scope for future work.",http://arxiv.org/abs/2505.14553v2
KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation,"Recent advancements in large language models (LLMs) underscore the need for
more comprehensive evaluation methods to accurately assess their reasoning
capabilities. Existing benchmarks are often domain-specific and thus cannot
fully capture an LLM's general reasoning potential. To address this limitation,
we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic
evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over
fifty games in either textual or visual formats and supports interactive,
multi-turn assessments with reinforcement learning scenarios. Using KORGym, we
conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent
reasoning patterns within model families and demonstrating the superior
performance of closed-source models. Further analysis examines the effects of
modality, reasoning strategies, reinforcement learning techniques, and response
length on model performance. We expect KORGym to become a valuable resource for
advancing LLM reasoning research and developing evaluation methodologies suited
to complex, interactive environments.",http://arxiv.org/abs/2505.14552v2
Time to Embed: Unlocking Foundation Models for Time Series with Channel Descriptions,"Traditional time series models are task-specific and often depend on
dataset-specific training and extensive feature engineering. While
Transformer-based architectures have improved scalability, foundation models,
commonplace in text, vision, and audio, remain under-explored for time series
and are largely restricted to forecasting. We introduce $\textbf{CHARM}$, a
foundation embedding model for multivariate time series that learns shared,
transferable, and domain-aware representations. To address the unique
difficulties of time series foundation learning, $\textbf{CHARM}$ incorporates
architectural innovations that integrate channel-level textual descriptions
while remaining invariant to channel order. The model is trained using a Joint
Embedding Predictive Architecture (JEPA), with novel augmentation schemes and a
loss function designed to improve interpretability and training stability. Our
$7$M-parameter model achieves state-of-the-art performance across diverse
downstream tasks, setting a new benchmark for time series representation
learning.",http://arxiv.org/abs/2505.14543v1
Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion for imbalanced Multi-modal Learning,"Multimodal spiking neural networks (SNNs) hold significant potential for
energy-efficient sensory processing but face critical challenges in modality
imbalance and temporal misalignment. Current approaches suffer from
uncoordinated convergence speeds across modalities and static fusion mechanisms
that ignore time-varying cross-modal interactions. We propose the temporal
attention-guided adaptive fusion framework for multimodal SNNs with two
synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion
(TAAF) module that dynamically assigns importance scores to fused spiking
features at each timestep, enabling hierarchical integration of temporally
heterogeneous spike-based features; 2) The temporal adaptive balanced fusion
loss that modulates learning rates per modality based on the above attention
scores, preventing dominant modalities from monopolizing optimization. The
proposed framework implements adaptive fusion, especially in the temporal
dimension, and alleviates the modality imbalance during multimodal learning,
mimicking cortical multisensory integration principles. Evaluations on CREMA-D,
AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%,
70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system
resolves temporal misalignment through learnable time-warping operations and
faster modality convergence coordination than baseline SNNs. This work
establishes a new paradigm for temporally coherent multimodal learning in
neuromorphic systems, bridging the gap between biological sensory processing
and efficient machine intelligence.",http://arxiv.org/abs/2505.14535v1
$\texttt{LLINBO}$: Trustworthy LLM-in-the-Loop Bayesian Optimization,"Bayesian optimization (BO) is a sequential decision-making tool widely used
for optimizing expensive black-box functions. Recently, Large Language Models
(LLMs) have shown remarkable adaptability in low-data regimes, making them
promising tools for black-box optimization by leveraging contextual knowledge
to propose high-quality query points. However, relying solely on LLMs as
optimization agents introduces risks due to their lack of explicit surrogate
modeling and calibrated uncertainty, as well as their inherently opaque
internal mechanisms. This structural opacity makes it difficult to characterize
or control the exploration-exploitation trade-off, ultimately undermining
theoretical tractability and reliability. To address this, we propose LLINBO:
LLM-in-the-Loop BO, a hybrid framework for BO that combines LLMs with
statistical surrogate experts (e.g., Gaussian Processes (GP)). The core
philosophy is to leverage contextual reasoning strengths of LLMs for early
exploration, while relying on principled statistical models to guide efficient
exploitation. Specifically, we introduce three mechanisms that enable this
collaboration and establish their theoretical guarantees. We end the paper with
a real-life proof-of-concept in the context of 3D printing. The code to
reproduce the results can be found at
https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO.",http://arxiv.org/abs/2505.14756v1
Lessons from Defending Gemini Against Indirect Prompt Injections,"Gemini is increasingly used to perform tasks on behalf of users, where
function-calling and tool-use capabilities enable the model to access user
data. Some tools, however, require access to untrusted data introducing risk.
Adversaries can embed malicious instructions in untrusted data which cause the
model to deviate from the user's expectations and mishandle their data or
permissions. In this report, we set out Google DeepMind's approach to
evaluating the adversarial robustness of Gemini models and describe the main
lessons learned from the process. We test how Gemini performs against a
sophisticated adversary through an adversarial evaluation framework, which
deploys a suite of adaptive attack techniques to run continuously against past,
current, and future versions of Gemini. We describe how these ongoing
evaluations directly help make Gemini more resilient against manipulation.",http://arxiv.org/abs/2505.14534v1
Energy-Efficient Deep Reinforcement Learning with Spiking Transformers,"Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.",http://arxiv.org/abs/2505.14533v1
SifterNet: A Generalized and Model-Agnostic Trigger Purification Approach,"Aiming at resisting backdoor attacks in convolution neural networks and
vision Transformer-based large model, this paper proposes a generalized and
model-agnostic trigger-purification approach resorting to the classic Ising
model. To date, existing trigger detection/removal studies usually require to
know the detailed knowledge of target model in advance, access to a large
number of clean samples or even model-retraining authorization, which brings
the huge inconvenience for practical applications, especially inaccessible to
target model. An ideal countermeasure ought to eliminate the implanted trigger
without regarding whatever the target models are. To this end, a lightweight
and black-box defense approach SifterNet is proposed through leveraging the
memorization-association functionality of Hopfield network, by which the
triggers of input samples can be effectively purified in a proper manner. The
main novelty of our proposed approach lies in the introduction of ideology of
Ising model. Extensive experiments also validate the effectiveness of our
approach in terms of proper trigger purification and high accuracy achievement,
and compared to the state-of-the-art baselines under several commonly-used
datasets, our SiferNet has a significant superior performance.",http://arxiv.org/abs/2505.14531v1
Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs,"We show that large language models (LLMs) exhibit an $\textit{internal
chain-of-thought}$: they sequentially decompose and execute composite tasks
layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned
at different network depths, and (ii) these subtasks are executed sequentially
across layers. On a benchmark of 15 two-step composite tasks, we employ
layer-from context-masking and propose a novel cross-task patching method,
confirming (i). To examine claim (ii), we apply LogitLens to decode hidden
states, revealing a consistent layerwise execution pattern. We further
replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing
the same stepwise dynamics. Together, our results enhance LLMs transparency by
showing their capacity to internally plan and execute subtasks (or
instructions), opening avenues for fine-grained, instruction-level activation
steering.",http://arxiv.org/abs/2505.14530v1
A simple estimator of the correlation kernel matrix of a determinantal point process,"The Determinantal Point Process (DPP) is a parameterized model for
multivariate binary variables, characterized by a correlation kernel matrix.
This paper proposes a closed form estimator of this kernel, which is
particularly easy to implement and can also be used as a starting value of
learning algorithms for maximum likelihood estimation. We prove the consistency
and asymptotic normality of our estimator, as well as its large deviation
properties.",http://arxiv.org/abs/2505.14529v1
Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities,"Wind hazards such as tornadoes and straight-line winds frequently affect
vulnerable communities in the Great Plains of the United States, where limited
infrastructure and sparse data coverage hinder effective emergency response.
Existing forecasting systems focus primarily on meteorological elements and
often fail to capture community-specific vulnerabilities, limiting their
utility for localized risk assessment and resilience planning. To address this
gap, we propose an interpretable dual-stream learning framework that integrates
structured numerical weather data with unstructured textual event narratives.
Our architecture combines a Random Forest and RoBERTa-based transformer through
a late fusion mechanism, enabling robust and context-aware wind hazard
prediction. The system is tailored for underserved tribal communities and
supports block-level risk assessment. Experimental results show significant
performance gains over traditional baselines. Furthermore, gradient-based
sensitivity and ablation studies provide insight into the model's
decision-making process, enhancing transparency and operational trust. The
findings demonstrate both predictive effectiveness and practical value in
supporting emergency preparedness and advancing community resilience.",http://arxiv.org/abs/2505.14522v1
Steering Deep Non-Linear Spatially Selective Filters for Weakly Guided Extraction of Moving Speakers in Dynamic Scenarios,"Recent speaker extraction methods using deep non-linear spatial filtering
perform exceptionally well when the target direction is known and stationary.
However, spatially dynamic scenarios are considerably more challenging due to
time-varying spatial features and arising ambiguities, e.g. when moving
speakers cross. While in a static scenario it may be easy for a user to point
to the target's direction, manually tracking a moving speaker is impractical.
Instead of relying on accurate time-dependent directional cues, which we refer
to as strong guidance, in this paper we propose a weakly guided extraction
method solely depending on the target's initial position to cope with spatial
dynamic scenarios. By incorporating our own deep tracking algorithm and
developing a joint training strategy on a synthetic dataset, we demonstrate the
proficiency of our approach in resolving spatial ambiguities and even
outperform a mismatched, but strongly guided extraction method.",http://arxiv.org/abs/2505.14517v1
Latent Flow Transformer,"Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.",http://arxiv.org/abs/2505.14513v1
Just One Layer Norm Guarantees Stable Extrapolation,"In spite of their prevalence, the behaviour of Neural Networks when
extrapolating far from the training distribution remains poorly understood,
with existing results limited to specific cases. In this work, we prove general
results -- the first of their kind -- by applying Neural Tangent Kernel (NTK)
theory to analyse infinitely-wide neural networks trained until convergence and
prove that the inclusion of just one Layer Norm (LN) fundamentally alters the
induced NTK, transforming it into a bounded-variance kernel. As a result, the
output of an infinitely wide network with at least one LN remains bounded, even
on inputs far from the training data. In contrast, we show that a broad class
of networks without LN can produce pathologically large outputs for certain
inputs. We support these theoretical findings with empirical experiments on
finite-width networks, demonstrating that while standard NNs often exhibit
uncontrolled growth outside the training domain, a single LN layer effectively
mitigates this instability. Finally, we explore real-world implications of this
extrapolatory stability, including applications to predicting residue sizes in
proteins larger than those seen during training and estimating age from facial
images of underrepresented ethnicities absent from the training set.",http://arxiv.org/abs/2505.14512v1
BACON: A fully explainable AI model with graded logic for decision making problems,"As machine learning models and autonomous agents are increasingly deployed in
high-stakes, real-world domains such as healthcare, security, finance, and
robotics, the need for transparent and trustworthy explanations has become
critical. To ensure end-to-end transparency of AI decisions, we need models
that are not only accurate but also fully explainable and human-tunable. We
introduce BACON, a novel framework for automatically training explainable AI
models for decision making problems using graded logic. BACON achieves high
predictive accuracy while offering full structural transparency and precise,
logic-based symbolic explanations, enabling effective human-AI collaboration
and expert-guided refinement. We evaluate BACON with a diverse set of
scenarios: classic Boolean approximation, Iris flower classification, house
purchasing decisions and breast cancer diagnosis. In each case, BACON provides
high-performance models while producing compact, human-verifiable decision
logic. These results demonstrate BACON's potential as a practical and
principled approach for delivering crisp, trustworthy explainable AI.",http://arxiv.org/abs/2505.14510v2
Federated prediction for scalable and privacy-preserved knowledge-based planning in radiotherapy,"Background: Deep learning has potential to improve the efficiency and
consistency of radiation therapy planning, but clinical adoption is hindered by
the limited model generalizability due to data scarcity and heterogeneity among
institutions. Although aggregating data from different institutions could
alleviate this problem, data sharing is a practical challenge due to concerns
about patient data privacy and other technical obstacles. Purpose: This work
aims to address this dilemma by developing FedKBP+, a comprehensive federated
learning (FL) platform for predictive tasks in real-world applications in
radiotherapy treatment planning. Methods: We implemented a unified
communication stack based on Google Remote Procedure Call (gRPC) to support
communication between participants whether located on the same workstation or
distributed across multiple workstations. In addition to supporting the
centralized FL strategies commonly available in existing open-source
frameworks, FedKBP+ also provides a fully decentralized FL model where
participants directly exchange model weights to each other through Peer-to-Peer
communication. We evaluated FedKBP+ on three predictive tasks using
scale-attention network (SA-Net) as the predictive model. Conclusions: Our
results demonstrate that FedKBP+ is highly effective, efficient and robust,
showing great potential as a federated learning platform for radiation therapy.",http://arxiv.org/abs/2505.14507v1
Learning to Integrate Diffusion ODEs by Averaging the Derivatives,"To accelerate diffusion model inference, numerical solvers perform poorly at
extremely small steps, while distillation techniques often introduce complexity
and instability. This work presents an intermediate strategy, balancing
performance and cost, by learning ODE integration using loss functions derived
from the derivative-integral relationship, inspired by Monte Carlo integration
and Picard iteration. From a geometric perspective, the losses operate by
gradually extending the tangent to the secant, thus are named as secant losses.
The secant losses can rapidly convert (via fine-tuning or distillation) a
pretrained diffusion model into its secant version. In our experiments, the
secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the
secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID
of $1.96$ on ImageNet-$256\times256$. Code will be available.",http://arxiv.org/abs/2505.14502v1
Personalised Insulin Adjustment with Reinforcement Learning: An In-Silico Validation for People with Diabetes on Intensive Insulin Treatment,"Despite recent advances in insulin preparations and technology, adjusting
insulin remains an ongoing challenge for the majority of people with type 1
diabetes (T1D) and longstanding type 2 diabetes (T2D). In this study, we
propose the Adaptive Basal-Bolus Advisor (ABBA), a personalised insulin
treatment recommendation approach based on reinforcement learning for
individuals with T1D and T2D, performing self-monitoring blood glucose
measurements and multiple daily insulin injection therapy. We developed and
evaluated the ability of ABBA to achieve better time-in-range (TIR) for
individuals with T1D and T2D, compared to a standard basal-bolus advisor (BBA).
The in-silico test was performed using an FDA-accepted population, including
101 simulated adults with T1D and 101 with T2D. An in-silico evaluation shows
that ABBA significantly improved TIR and significantly reduced both times
below- and above-range, compared to BBA. ABBA's performance continued to
improve over two months, whereas BBA exhibited only modest changes. This
personalised method for adjusting insulin has the potential to further optimise
glycaemic control and support people with T1D and T2D in their daily
self-management. Our results warrant ABBA to be trialed for the first time in
humans.",http://arxiv.org/abs/2505.14477v1
Enhancing Interpretability of Sparse Latent Representations with Class Information,"Variational Autoencoders (VAEs) are powerful generative models for learning
latent representations. Standard VAEs generate dispersed and unstructured
latent spaces by utilizing all dimensions, which limits their interpretability,
especially in high-dimensional spaces. To address this challenge, Variational
Sparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting
in sparse latent representations for each input. These sparse representations,
characterized by a limited number of active dimensions, are inherently more
interpretable. Despite this advantage, VSC falls short in providing structured
interpretations across samples within the same class. Intuitively, samples from
the same class are expected to share similar attributes while allowing for
variations in those attributes. This expectation should manifest as consistent
patterns of active dimensions in their latent representations, but VSC does not
enforce such consistency.
  In this paper, we propose a novel approach to enhance the latent space
interpretability by ensuring that the active dimensions in the latent space are
consistent across samples within the same class. To achieve this, we introduce
a new loss function that encourages samples from the same class to share
similar active dimensions. This alignment creates a more structured and
interpretable latent space, where each shared dimension corresponds to a
high-level concept, or ""factor."" Unlike existing disentanglement-based methods
that primarily focus on global factors shared across all classes, our method
captures both global and class-specific factors, thereby enhancing the utility
and interpretability of latent representations.",http://arxiv.org/abs/2505.14476v1
PAST: Phonetic-Acoustic Speech Tokenizer,"We present PAST, a novel end-to-end framework that jointly models phonetic
information alongside signal reconstruction, eliminating the need for external
pretrained models. Unlike previous approaches that rely on pretrained
self-supervised models, PAST employs supervised phonetic data, directly
integrating domain knowledge into the tokenization process via auxiliary tasks.
Additionally, we introduce a streamable, causal variant of PAST, enabling
real-time speech applications. Results demonstrate that PAST surpasses existing
evaluated baseline tokenizers across common evaluation metrics, including
phonetic representation and speech reconstruction. Notably, PAST also achieves
superior performance when serving as a speech representation for speech
language models, further highlighting its effectiveness as a foundation for
spoken language generation. To foster further research, we release the full
implementation. For code, model checkpoints, and samples see:
https://pages.cs.huji.ac.il/adiyoss-lab/PAST",http://arxiv.org/abs/2505.14470v1
ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs,"Serverless computing has grown rapidly for serving Large Language Model (LLM)
inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid
scaling. However, our analysis reveals that current serverless can effectively
serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to
three key limitations: 1) massive parameter redundancy among functions where
99% of weights are unnecessarily duplicated, 2) costly artifact loading latency
beyond LLM loading, and 3) magnified resource contention when serving multiple
LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased
Time-To-First-Token (TTFT), and high monetary costs.
  We propose ServerlessLoRA, a novel serverless inference system designed for
faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM
sharing across isolated LoRA functions to reduce redundancy. We design a
pre-loading method that pre-loads comprehensive LoRA artifacts to minimize
cold-start latency. Furthermore, ServerlessLoRA employs contention aware
batching and offloading to mitigate GPU resource conflicts during bursty
workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA
reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to
state-of-the-art LLM inference solutions.",http://arxiv.org/abs/2505.14468v1
FlowTSE: Target Speaker Extraction with Flow Matching,"Target speaker extraction (TSE) aims to isolate a specific speaker's speech
from a mixture using speaker enrollment as a reference. While most existing
approaches are discriminative, recent generative methods for TSE achieve strong
results. However, generative methods for TSE remain underexplored, with most
existing approaches relying on complex pipelines and pretrained components,
leading to computational overhead. In this work, we present FlowTSE, a simple
yet effective TSE approach based on conditional flow matching. Our model
receives an enrollment audio sample and a mixed speech signal, both represented
as mel-spectrograms, with the objective of extracting the target speaker's
clean speech. Furthermore, for tasks where phase reconstruction is crucial, we
propose a novel vocoder conditioned on the complex STFT of the mixed signal,
enabling improved phase estimation. Experimental results on standard TSE
benchmarks show that FlowTSE matches or outperforms strong baselines.",http://arxiv.org/abs/2505.14465v1
Adverseness vs. Equilibrium: Exploring Graph Adversarial Resilience through Dynamic Equilibrium,"Adversarial attacks to graph analytics are gaining increased attention. To
date, two lines of countermeasures have been proposed to resist various graph
adversarial attacks from the perspectives of either graph per se or graph
neural networks. Nevertheless, a fundamental question lies in whether there
exists an intrinsic adversarial resilience state within a graph regime and how
to find out such a critical state if exists. This paper contributes to tackle
the above research questions from three unique perspectives: i) we regard the
process of adversarial learning on graph as a complex multi-object dynamic
system, and model the behavior of adversarial attack; ii) we propose a
generalized theoretical framework to show the existence of critical adversarial
resilience state; and iii) we develop a condensed one-dimensional function to
capture the dynamic variation of graph regime under perturbations, and pinpoint
the critical state through solving the equilibrium point of dynamic system.
Multi-facet experiments are conducted to show our proposed approach can
significantly outperform the state-of-the-art defense methods under five
commonly-used real-world datasets and three representative attacks.",http://arxiv.org/abs/2505.14463v1
Interpretable Reinforcement Learning for Load Balancing using Kolmogorov-Arnold Networks,"Reinforcement learning (RL) has been increasingly applied to network control
problems, such as load balancing. However, existing RL approaches often suffer
from lack of interpretability and difficulty in extracting controller
equations. In this paper, we propose the use of Kolmogorov-Arnold Networks
(KAN) for interpretable RL in network control. We employ a PPO agent with a
1-layer actor KAN model and an MLP Critic network to learn load balancing
policies that maximise throughput utility, minimize loss as well as delay. Our
approach allows us to extract controller equations from the learned neural
networks, providing insights into the decision-making process. We evaluate our
approach using different reward functions demonstrating its effectiveness in
improving network performance while providing interpretable policies.",http://arxiv.org/abs/2505.14459v1
Model-Independent Machine Learning Approach for Nanometric Axial Localization and Tracking,"Accurately tracking particles and determining their position along the
optical axis is a major challenge in optical microscopy, especially when
extremely high precision is needed. In this study, we introduce a deep learning
approach using convolutional neural networks (CNNs) that can determine axial
positions from dual-focal plane images without relying on predefined models.
Our method achieves an axial localization accuracy of 40 nanometers - six times
better than traditional single-focal plane techniques. The model's simple
design and strong performance make it suitable for a wide range of uses,
including dark matter detection, proton therapy for cancer, and radiation
protection in space. It also shows promise in fields like biological imaging,
materials science, and environmental monitoring. This work highlights how
machine learning can turn complex image data into reliable, precise
information, offering a flexible and powerful tool for many scientific
applications.",http://arxiv.org/abs/2505.14754v1
RefiDiff: Refinement-Aware Diffusion for Efficient Missing Data Imputation,"Missing values in high-dimensional, mixed-type datasets pose significant
challenges for data imputation, particularly under Missing Not At Random (MNAR)
mechanisms. Existing methods struggle to integrate local and global data
characteristics, limiting performance in MNAR and high-dimensional settings. We
propose an innovative framework, RefiDiff, combining local machine learning
predictions with a novel Mamba-based denoising network capturing
interrelationships among distant features and samples. Our approach leverages
pre-refinement for initial warm-up imputations and post-refinement to polish
results, enhancing stability and accuracy. By encoding mixed-type data into
unified tokens, RefiDiff enables robust imputation without architectural or
hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods
across missing-value settings, excelling in MNAR with a 4x faster training time
than SOTA DDPM-based approaches. Extensive evaluations on nine real-world
datasets demonstrate its robustness, scalability, and effectiveness in handling
complex missingness patterns.",http://arxiv.org/abs/2505.14451v1
Interpretable Neural System Dynamics: Combining Deep Learning with System Dynamics Modeling to Support Critical Applications,"The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.",http://arxiv.org/abs/2505.14428v1
Explaining Neural Networks with Reasons,"We propose a new interpretability method for neural networks, which is based
on a novel mathematico-philosophical theory of reasons. Our method computes a
vector for each neuron, called its reasons vector. We then can compute how
strongly this reasons vector speaks for various propositions, e.g., the
proposition that the input image depicts digit 2 or that the input prompt has a
negative sentiment. This yields an interpretation of neurons, and groups
thereof, that combines a logical and a Bayesian perspective, and accounts for
polysemanticity (i.e., that a single neuron can figure in multiple concepts).
We show, both theoretically and empirically, that this method is: (1) grounded
in a philosophically established notion of explanation, (2) uniform, i.e.,
applies to the common neural network architectures and modalities, (3)
scalable, since computing reason vectors only involves forward-passes in the
neural network, (4) faithful, i.e., intervening on a neuron based on its reason
vector leads to expected changes in model output, (5) correct in that the
model's reasons structure matches that of the data source, (6) trainable, i.e.,
neural networks can be trained to improve their reason strengths, (7) useful,
i.e., it delivers on the needs for interpretability by increasing, e.g.,
robustness and fairness.",http://arxiv.org/abs/2505.14424v1
A system identification approach to clustering vector autoregressive time series,"Clustering of time series based on their underlying dynamics is keeping
attracting researchers due to its impacts on assisting complex system
modelling. Most current time series clustering methods handle only scalar time
series, treat them as white noise, or rely on domain knowledge for high-quality
feature construction, where the autocorrelation pattern/feature is mostly
ignored. Instead of relying on heuristic feature/metric construction, the
system identification approach allows treating vector time series clustering by
explicitly considering their underlying autoregressive dynamics. We first
derive a clustering algorithm based on a mixture autoregressive model.
Unfortunately it turns out to have significant computational problems. We then
derive a `small-noise' limiting version of the algorithm, which we call k-LMVAR
(Limiting Mixture Vector AutoRegression), that is computationally manageable.
We develop an associated BIC criterion for choosing the number of clusters and
model order. The algorithm performs very well in comparative simulations and
also scales well computationally.",http://arxiv.org/abs/2505.14421v1
SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection,"Predicting earnings surprises through the analysis of earnings conference
call transcripts has attracted increasing attention from the financial research
community. Conference calls serve as critical communication channels between
company executives, analysts, and shareholders, offering valuable
forward-looking information. However, these transcripts present significant
analytical challenges, typically containing over 5,000 words with substantial
redundancy and industry-specific terminology that creates obstacles for
language models. In this work, we propose the Sparse Autoencoder for Financial
Representation Enhancement (SAE-FiRE) framework to address these limitations by
extracting key information while eliminating redundancy. SAE-FiRE employs
Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out
noises, and focusing specifically on capturing nuanced financial signals that
have predictive power for earnings surprises. Experimental results indicate
that the proposed method can significantly outperform comparing baselines.",http://arxiv.org/abs/2505.14420v1
Towards Non-Euclidean Foundation Models: Advancing AI Beyond Euclidean Frameworks,"In the era of foundation models and Large Language Models (LLMs), Euclidean
space is the de facto geometric setting of our machine learning architectures.
However, recent literature has demonstrated that this choice comes with
fundamental limitations. To that end, non-Euclidean learning is quickly gaining
traction, particularly in web-related applications where complex relationships
and structures are prevalent. Non-Euclidean spaces, such as hyperbolic,
spherical, and mixed-curvature spaces, have been shown to provide more
efficient and effective representations for data with intrinsic geometric
properties, including web-related data like social network topology,
query-document relationships, and user-item interactions. Integrating
foundation models with non-Euclidean geometries has great potential to enhance
their ability to capture and model the underlying structures, leading to better
performance in search, recommendations, and content understanding. This
workshop focuses on the intersection of Non-Euclidean Foundation Models and
Geometric Learning (NEGEL), exploring its potential benefits, including the
potential benefits for advancing web-related technologies, challenges, and
future directions. Workshop page:
[https://hyperboliclearning.github.io/events/www2025workshop](https://hyperboliclearning.github.io/events/www2025workshop)",http://arxiv.org/abs/2505.14417v1
Table Foundation Models: on knowledge pre-training for tabular learning,"Table foundation models bring high hopes to data science: pre-trained on
tabular data to embark knowledge or priors, they should facilitate downstream
tasks on tables. One specific challenge is that of data semantics: numerical
entries take their meaning from context, e.g., column name. Pre-trained neural
networks that jointly model column names and table entries have recently
boosted prediction accuracy. While these models outline the promises of world
knowledge to interpret table values, they lack the convenience of popular
foundation models in text or vision. Indeed, they must be fine-tuned to bring
benefits, come with sizeable computation costs, and cannot easily be reused or
combined with other architectures. Here we introduce TARTE, a foundation model
that transforms tables to knowledge-enhanced vector representations using the
string to capture semantics. Pre-trained on large relational data, TARTE yields
representations that facilitate subsequent learning with little additional
cost. These representations can be fine-tuned or combined with other learners,
giving models that push the state-of-the-art prediction performance and improve
the prediction/computation performance trade-off. Specialized to a task or a
domain, TARTE gives domain-specific representations that facilitate further
learning. Our study demonstrates an effective approach to knowledge
pre-training for tabular learning.",http://arxiv.org/abs/2505.14415v1
Byte Pair Encoding for Efficient Time Series Forecasting,"Existing time series tokenization methods predominantly encode a constant
number of samples into individual tokens. This inflexible approach can generate
excessive tokens for even simple patterns like extended constant values,
resulting in substantial computational overhead. Inspired by the success of
byte pair encoding, we propose the first pattern-centric tokenization scheme
for time series analysis. Based on a discrete vocabulary of frequent motifs,
our method merges samples with underlying patterns into tokens, compressing
time series adaptively. Exploiting our finite set of motifs and the continuous
properties of time series, we further introduce conditional decoding as a
lightweight yet powerful post-hoc optimization method, which requires no
gradient computation and adds no computational overhead. On recent time series
foundation models, our motif-based tokenization improves forecasting
performance by 36% and boosts efficiency by 1990% on average. Conditional
decoding further reduces MSE by up to 44%. In an extensive analysis, we
demonstrate the adaptiveness of our tokenization to diverse temporal patterns,
its generalization to unseen data, and its meaningful token representations
capturing distinct time series properties, including statistical moments and
trends.",http://arxiv.org/abs/2505.14411v1
Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach,"Autonomous systems that rely on Machine Learning (ML) utilize online fault
tolerance mechanisms, such as runtime monitors, to detect ML prediction errors
and maintain safety during operation. However, the lack of human-interpretable
explanations for these errors can hinder the creation of strong assurances
about the system's safety and reliability. This paper introduces a novel
fuzzy-based monitor tailored for ML perception components. It provides
human-interpretable explanations about how different operating conditions
affect the reliability of perception components and also functions as a runtime
safety monitor. We evaluated our proposed monitor using naturalistic driving
datasets as part of an automated driving case study. The interpretability of
the monitor was evaluated and we identified a set of operating conditions in
which the perception component performs reliably. Additionally, we created an
assurance case that links unit-level evidence of \textit{correct} ML operation
to system-level \textit{safety}. The benchmarking demonstrated that our monitor
achieved a better increase in safety (i.e., absence of hazardous situations)
while maintaining availability (i.e., ability to perform the mission) compared
to state-of-the-art runtime ML monitors in the evaluated dataset.",http://arxiv.org/abs/2505.14407v1
Unearthing Gems from Stones: Policy Optimization with Negative Sample Augmentation for LLM Reasoning,"Recent advances in reasoning language models have witnessed a paradigm shift
from short to long CoT pattern. Given the substantial computational cost of
rollouts in long CoT models, maximizing the utility of fixed training datasets
becomes crucial. Our analysis reveals that negative responses contain valuable
components such as self-reflection and error-correction steps, yet primary
existing methods either completely discard negative samples (RFT) or apply
equal penalization across all tokens (RL), failing to leverage these potential
learning signals. In light of this, we propose Behavior Constrained Policy
Gradient with Negative Sample Augmentation (BCPG-NSA), a fine-grained offline
RL framework that encompasses three stages: 1) sample segmentation, 2)
consensus-based step correctness assessment combining LLM and PRM judgers, and
3) policy optimization with NSA designed to effectively mine positive steps
within negative samples. Experimental results show that BCPG-NSA outperforms
baselines on several challenging math/coding reasoning benchmarks using the
same training dataset, achieving improved sample efficiency and demonstrating
robustness and scalability when extended to multiple iterations.",http://arxiv.org/abs/2505.14403v1
Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation,"While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (KV) caches, encoding the full
reasoning context of prior tasks while storing KV caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the KV values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing KV caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and KV cache techniques.",http://arxiv.org/abs/2505.14398v1
Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds,"Causal world models are systems that can answer counterfactual questions
about an environment of interest, i.e. predict how it would have evolved if an
arbitrary subset of events had been realized differently. It requires
understanding the underlying causes behind chains of events and conducting
causal inference for arbitrary unseen distributions. So far, this task eludes
foundation models, notably large language models (LLMs), which do not have
demonstrated causal reasoning capabilities beyond the memorization of existing
causal relationships. Furthermore, evaluating counterfactuals in real-world
applications is challenging since only the factual world is observed, limiting
evaluation to synthetic datasets. We address these problems by explicitly
extracting and modeling causal relationships and propose the Causal
Cartographer framework. First, we introduce a graph retrieval-augmented
generation agent tasked to retrieve causal relationships from data. This
approach allows us to construct a large network of real-world causal
relationships that can serve as a repository of causal knowledge and build
real-world counterfactuals. In addition, we create a counterfactual reasoning
agent constrained by causal relationships to perform reliable step-by-step
causal inference. We show that our approach can extract causal knowledge and
improve the robustness of LLMs for causal reasoning tasks while reducing
inference costs and spurious correlations.",http://arxiv.org/abs/2505.14396v1
Algorithmic Hiring and Diversity: Reducing Human-Algorithm Similarity for Better Outcomes,"Algorithmic tools are increasingly used in hiring to improve fairness and
diversity, often by enforcing constraints such as gender-balanced candidate
shortlists. However, we show theoretically and empirically that enforcing equal
representation at the shortlist stage does not necessarily translate into more
diverse final hires, even when there is no gender bias in the hiring stage. We
identify a crucial factor influencing this outcome: the correlation between the
algorithm's screening criteria and the human hiring manager's evaluation
criteria -- higher correlation leads to lower diversity in final hires. Using a
large-scale empirical analysis of nearly 800,000 job applications across
multiple technology firms, we find that enforcing equal shortlists yields
limited improvements in hire diversity when the algorithmic screening closely
mirrors the hiring manager's preferences. We propose a complementary
algorithmic approach designed explicitly to diversify shortlists by selecting
candidates likely to be overlooked by managers, yet still competitive according
to their evaluation criteria. Empirical simulations show that this approach
significantly enhances gender diversity in final hires without substantially
compromising hire quality. These findings highlight the importance of
algorithmic design choices in achieving organizational diversity goals and
provide actionable guidance for practitioners implementing fairness-oriented
hiring algorithms.",http://arxiv.org/abs/2505.14388v1
Layer-wise Quantization for Quantized Optimistic Dual Averaging,"Modern deep neural networks exhibit heterogeneity across numerous layers of
various types such as residuals, multi-head attention, etc., due to varying
structures (dimensions, activation functions, etc.), distinct representation
characteristics, which impact predictions. We develop a general layer-wise
quantization framework with tight variance and code-length bounds, adapting to
the heterogeneities over the course of training. We then apply a new layer-wise
quantization technique within distributed variational inequalities (VIs),
proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with
adaptive learning rates, which achieves competitive convergence rates for
monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup
over the baselines in end-to-end training time for training Wasserstein GAN on
$12+$ GPUs.",http://arxiv.org/abs/2505.14371v1
Vid2World: Crafting Video Diffusion Models to Interactive World Models,"World models, which predict transitions based on history observation and
action sequences, have shown great promise in improving data efficiency for
sequential decision making. However, existing world models often require
extensive domain-specific training and still produce low-fidelity, coarse
predictions, limiting their applicability in complex environments. In contrast,
video diffusion models trained on large, internet-scale datasets have
demonstrated impressive capabilities in generating high-quality videos that
capture diverse real-world dynamics. In this work, we present Vid2World, a
general approach for leveraging and transferring pre-trained video diffusion
models into interactive world models. To bridge the gap, Vid2World performs
casualization of a pre-trained video diffusion model by crafting its
architecture and training objective to enable autoregressive generation.
Furthermore, it introduces a causal action guidance mechanism to enhance action
controllability in the resulting interactive world model. Extensive experiments
in robot manipulation and game simulation domains show that our method offers a
scalable and effective approach for repurposing highly capable video diffusion
models to interactive world models.",http://arxiv.org/abs/2505.14357v1
WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications,"Large Language Models (LLMs) have achieved impressive results across a broad
array of tasks, yet their capacity for complex, domain-specific mathematical
reasoning-particularly in wireless communications-remains underexplored. In
this work, we introduce WirelessMathBench, a novel benchmark specifically
designed to evaluate LLMs on mathematical modeling challenges to wireless
communications engineering. Our benchmark consists of 587 meticulously curated
questions sourced from 40 state-of-the-art research papers, encompassing a
diverse spectrum of tasks ranging from basic multiple-choice questions to
complex equation completion tasks, including both partial and full completions,
all of which rigorously adhere to physical and dimensional constraints. Through
extensive experimentation with leading LLMs, we observe that while many models
excel in basic recall tasks, their performance degrades significantly when
reconstructing partially or fully obscured equations, exposing fundamental
limitations in current LLMs. Even DeepSeek-R1, the best performer on our
benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83%
success rate in full equation completion. By publicly releasing
WirelessMathBench along with the evaluation toolkit, we aim to advance the
development of more robust, domain-aware LLMs for wireless system analysis and
broader engineering applications.",http://arxiv.org/abs/2505.14354v1
